{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from load_data import DATA_DIR, TRANSFORMS, P53_CLASS_NAMES, \\\n",
    "    convert_presence_probs_to_status_probs\n",
    "from resnet import ResNetModel, ResNetModelDoubleBinary\n",
    "# from resnet_patch import ResNetModelDoubleBinary as ResNetModelDoubleBinaryPatch\n",
    "from pl_clam import CLAM_MB, CLAM_db\n",
    "\n",
    "P53_CLASS_CODES = [\"WT\", \"OE\", \"NM\", \"DC\"]\n",
    "\n",
    "BOLERO_DIR = os.path.join(DATA_DIR, '..', 'BOLERO')\n",
    "PATHXL_DIR = os.path.join(DATA_DIR, '..', 'p53_consensus_study')\n",
    "\n",
    "BASE_DIR = {\n",
    "    'test': DATA_DIR,\n",
    "    'bolero': BOLERO_DIR,\n",
    "    'pathxl': PATHXL_DIR\n",
    "}\n",
    "\n",
    "RESULTS_DIR = os.path.join(DATA_DIR, '..', '..', 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "VIS_DIR = os.path.join(DATA_DIR, '..', '..', 'visualizations')\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '..', '..', 'models')\n",
    "\n",
    "bag_latent_paths = {\n",
    "    \"test\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs256_retccl__backup.pt\"),\n",
    "    \"bolero\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "    \"pathxl\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "color_dict = {\n",
    "    \"r\": '#DA4C4C', # Red\n",
    "    \"o\": '#E57439', # Orange\n",
    "    \"y\": '#EDB732', # Yellow\n",
    "    \"g\": '#479A5F', # Green\n",
    "    \"lb\": '#5BC5DB', # Light blue\n",
    "    \"b\": '#5387DD', # Blue\n",
    "    \"p\": '#7D54B2', # Purple\n",
    "    \"pi\": '#E87B9F', # Pink\n",
    "#  '#229487', # Dark green/Turquoise\n",
    "#  '#C565C7', # Lilac\n",
    "    \"r_p\": '#E89393', # Pale red\n",
    "    \"o_p\": '#EFAB88', # Pale orange\n",
    "    \"y_p\": '#F4D384', # Pale yellow\n",
    "    \"g_p\": '#90C29F', # Pale green\n",
    "    \"lb_p\":'#9CDCE9', # Pale light blue\n",
    "    \"b_p\": '#98B7EA', # Pale blue\n",
    "    \"p_p\": \"#B198D0\", # Pale purple\n",
    "}\n",
    "colors = list(color_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb: full biopsy, db: double binary, gs: grid spacing\n",
    "model_kwargs = {\n",
    "    \"CLAM\":     {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db\":  {\"model_class\": CLAM_db, \"gs\": 256},\n",
    "    \"CLAM_m\":   {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db_m\":{\"model_class\": CLAM_db, \"gs\": 256},\n",
    "}\n",
    "for name in [\"fb_db\", \"fb\"]:\n",
    "    for spacing in [2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "        if \"db\" not in name and spacing == 2: # Skipped this one because it's too slow\n",
    "            continue\n",
    "        model_name = f\"{name}_spacing{spacing}\"\n",
    "        model_kwargs[model_name] = {\"spacing\": spacing}\n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"fb_db\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModelDoubleBinary\n",
    "    elif \"fb\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModel\n",
    "    checkpoint_dir = os.path.join(MODELS_DIR, model_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model_kwargs[model_name][\"checkpoint_paths\"] = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n",
    "\n",
    "def load_model(model_class, checkpoint_path):\n",
    "    if \"CLAM\" in model_class.__name__: # For some reason pl can't load these models with load_from_checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        # model = model_class(**checkpoint[\"model_kwargs\"])\n",
    "        model = model_class()\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        model = model_class.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAM: 5 checkpoints\n",
      "CLAM_db: 5 checkpoints\n",
      "CLAM_m: 5 checkpoints\n",
      "CLAM_db_m: 5 checkpoints\n",
      "fb_db_spacing2: 5 checkpoints\n",
      "fb_db_spacing4: 5 checkpoints\n",
      "fb_db_spacing8: 4 checkpoints\n",
      "fb_db_spacing16: 5 checkpoints\n",
      "fb_db_spacing32: 5 checkpoints\n",
      "fb_db_spacing64: 5 checkpoints\n",
      "fb_db_spacing128: 5 checkpoints\n",
      "fb_db_spacing256: 5 checkpoints\n",
      "fb_spacing4: 5 checkpoints\n",
      "fb_spacing8: 5 checkpoints\n",
      "fb_spacing16: 5 checkpoints\n",
      "fb_spacing32: 5 checkpoints\n",
      "fb_spacing64: 5 checkpoints\n",
      "fb_spacing128: 5 checkpoints\n",
      "fb_spacing256: 5 checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Print how many checkpoints we have for each model\n",
    "for model_name in model_kwargs:\n",
    "    print(f\"{model_name}: {len(model_kwargs[model_name]['checkpoint_paths'])} checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_name, data_name):\n",
    "    img_path = os.path.join(BASE_DIR[data_name], 'biopsies', f\"{img_name}.png\")\n",
    "    img = plt.imread(img_path) # (H, W, C) float32\n",
    "    return img\n",
    "    \n",
    "def load_patch_latents(img_name, data_name, bag_latents):\n",
    "    if data_name == \"test\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "    elif data_name == \"bolero\":\n",
    "        slide_name, biopsy_name = tuple(img_name.split(\"_\"))\n",
    "        slide_latents = bag_latents[int(slide_name)] # (n_biopsies, N, 2048)\n",
    "        return slide_latents[int(biopsy_name)+1] # (N, 2048)\n",
    "    elif data_name == \"pathxl\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "\n",
    "\n",
    "def call_constant_size(model, img_name, data_name, size, **kwargs):\n",
    "    img = load_img(img_name, data_name)\n",
    "    img = torch.nn.functional.interpolate(torch.tensor(img).permute(2, 0, 1).unsqueeze(0), size=(size, size), mode='bilinear')\n",
    "    img = TRANSFORMS['normalize'](img)\n",
    "    with torch.no_grad():\n",
    "        return model(img.to(device)).cpu().detach().numpy()\n",
    "\n",
    "def call_constant_spacing(model, img_name, data_name, spacing, **kwargs):\n",
    "    img = load_img(img_name, data_name)\n",
    "    h = img.shape[0] // spacing\n",
    "    w = img.shape[1] // spacing\n",
    "    img = torch.nn.functional.interpolate(torch.tensor(img).permute(2, 0, 1).unsqueeze(0), size=(h, w), mode='bilinear')\n",
    "    img = TRANSFORMS['normalize'](img)\n",
    "    with torch.no_grad():\n",
    "        return model(img.to(device)).cpu().detach().numpy()\n",
    "\n",
    "def call_CLAM(model, img_name, data_name, bag_latents, **kwargs):\n",
    "    patch_latents = load_patch_latents(img_name, data_name, bag_latents).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits, Y_prob, Y_hat, A_raw, results_dict = model(patch_latents.to(device))\n",
    "        return Y_prob.cpu().detach().numpy(), A_raw.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"size\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"call\"] = call_constant_size\n",
    "    elif \"spacing\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"call\"] = call_constant_spacing\n",
    "    elif \"CLAM\" in model_name:\n",
    "        model_kwargs[model_name][\"call\"] = call_CLAM\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown call type for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_names(data_name='test'):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        return get_img_names(\"test\") + get_img_names(\"pathxl\")\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv'))[\"id\"].tolist()\n",
    "    return [img_path.split('.')[0] for img_path in os.listdir(os.path.join(BASE_DIR[data_name], 'biopsies'))]\n",
    "\n",
    "def get_results(model, mdl_kwargs, data_name='test', bag_latents=None):\n",
    "    img_names = get_img_names(data_name)\n",
    "    results = {}\n",
    "    for img_name in tqdm(img_names):\n",
    "        results[img_name] = mdl_kwargs[\"call\"](model, img_name, data_name, bag_latents=bag_latents, **mdl_kwargs)\n",
    "    return results\n",
    "\n",
    "def save_results(results, model_name, data_name, checkpoint_name):\n",
    "    results_dir = os.path.join(VIS_DIR, model_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "    torch.save(results, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RUNNING TEST\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "\n",
      "RUNNING BOLERO\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "\n",
      "RUNNING PATHXL\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_db\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done CLAM_db_m\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing2\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing4\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing8\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing16\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing32\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing64\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing128\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_db_spacing256\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing4\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing8\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing16\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing32\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing64\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing128\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n",
      "Already done fb_spacing256\n"
     ]
    }
   ],
   "source": [
    "for data_name in [\"test\", \"bolero\", \"pathxl\"]:\n",
    "    print(f\"\\nRUNNING {data_name.upper()}\")\n",
    "    bag_latents = torch.load(bag_latent_paths[data_name], map_location=device)\n",
    "    for model_name, mdl_kwargs in model_kwargs.items():\n",
    "        for i, checkpoint_path in enumerate(mdl_kwargs[\"checkpoint_paths\"][:]):\n",
    "            checkpoint_name = os.path.basename(checkpoint_path).replace(\".ckpt\", \"\")\n",
    "            results_path = os.path.join(VIS_DIR, model_name, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "\n",
    "            if os.path.exists(results_path):\n",
    "                print(f\"Already done {model_name}\")\n",
    "                continue\n",
    "            print(f\"Running {model_name} {i}\")\n",
    "            model = load_model(mdl_kwargs[\"model_class\"], checkpoint_path)\n",
    "            results = get_results(model, mdl_kwargs, data_name=data_name, bag_latents=bag_latents)\n",
    "            save_results(results, model_name, data_name=data_name, checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Metrics\n",
    "Load results and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fb_db   has a dict like idx: shape (1,2) with the two mutation probabilities\n",
    "fb      has a dict like idx: shape (1,4) with the four class probabilities\n",
    "CLAM_db has a dict like idx: tuple of: (\n",
    "            shape (1,2) with the two mutation probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "    )\n",
    "CLAM    has a dict like idx: tuple of: (\n",
    "            shape (1,4) with the four class probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "        )\n",
    "\"\"\"\n",
    "model_type_result_keys = {\n",
    "    \"fb_db\": [\"presence_probs\"],\n",
    "    \"fb\": [\"status_probs\"],\n",
    "    \"CLAM_db\": [\"presence_probs\", \"A_raw\"],\n",
    "    \"CLAM\": [\"status_probs\", \"A_raw\"],\n",
    "}\n",
    "def get_result_keys(model_name):\n",
    "    for key in model_type_result_keys: # The order is important\n",
    "        if key in model_name:\n",
    "            return model_type_result_keys[key]\n",
    "\n",
    "def load_results(model_name, data_name):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        results = defaultdict(lambda: defaultdict(dict))\n",
    "        for data_name in [\"test\", \"pathxl\"]:\n",
    "            results_data = load_results(model_name, data_name)\n",
    "            for key in results_data:\n",
    "                results[key].update(results_data[key])\n",
    "        return results\n",
    "\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    for i, checkpoint_name in enumerate([f for f in os.listdir(results_dir) if f.startswith(data_name)]):\n",
    "        results_path = os.path.join(results_dir, checkpoint_name)\n",
    "        result_content = torch.load(results_path)\n",
    "        checkpoint_name = checkpoint_name.replace(f\"{data_name}_\", \"\").replace(\".pt\", \"\")\n",
    "        if \"CLAM_db\" in model_name:\n",
    "            for img_name, (presence_probs, A_raw) in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"CLAM\" in model_name:\n",
    "            for img_name, (status_probs, A_raw) in result_content.items():\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"fb_db\" in model_name:\n",
    "            for img_name, presence_probs in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "        elif \"fb\" in model_name:\n",
    "            for img_name, status_probs in result_content.items():\n",
    "                # results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = torch.nn.functional.softmax(torch.tensor(status_probs), dim=1).numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    return results\n",
    "\n",
    "def get_labels(data_name):\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv')).set_index(\"id\").to_dict(orient='dict')['label']\n",
    "    elif data_name == \"bolero\":\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[data_name], 'P53_BOLERO_T.csv'))\n",
    "        labels = labels.sort_values(by=\"Case ID\")\n",
    "        labels = labels.reset_index(drop=True)\n",
    "        # Map GS to {1:0, 2:1, 3:2, 4:4} where 4 is unknown\n",
    "        labels[\"GS\"] = labels[\"GS\"].map({1:0, 2:1, 3:2, 4:4})\n",
    "        # Only keep GS column\n",
    "        labels = labels[[\"GS\"]].to_dict(orient='dict')[\"GS\"]\n",
    "        return labels\n",
    "    elif data_name.startswith(\"pathxl\"):\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[\"pathxl\"], 'labels.csv'))\n",
    "        # idx is id column and biopsy_nr column separated by _\n",
    "        labels[\"idx\"] = labels[\"id\"].astype(str) + \"_\" + labels[\"biopsy_nr\"].astype(str)\n",
    "        labels = labels.set_index(\"idx\")\n",
    "        # Sort by id primarily and biopsy_nr secondarily\n",
    "        labels = labels.sort_values(by=[\"id\", \"biopsy_nr\"])\n",
    "        # Map label\n",
    "        mapping = {\"WT\":0, \"Overexpression\":1, \"Null\":2, \"Double clones\":3}\n",
    "        labels[\"label\"] = labels[\"label\"].map(mapping)\n",
    "        if data_name == \"pathxl\": # Filter out any concordance % < 75\n",
    "            labels = labels[labels[\"concordance %\"] >= 75]\n",
    "        elif data_name == \"pathxl-100\":\n",
    "            labels = labels[labels[\"concordance %\"] == 100]\n",
    "        labels = labels[[\"label\"]].to_dict(orient='dict')[\"label\"]\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl\"))\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl-100\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl-100\"))\n",
    "        return labels\n",
    "    \n",
    "# load_results(\"fb_spacing4\", \"test\")[\"status_probs\"]\n",
    "# get_labels(\"test+pathxl-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../CLAM/results/test_s1/clam_outputs.json\", \"r\") as f:\n",
    "    clam_outputs = json.load(f)\n",
    "\n",
    "non_empty_patch_indices_by_biopsy = torch.load(os.path.join(DATA_DIR, \"non_empty_patch_indices_gs256_relaxed.pt\"))\n",
    "\n",
    "test_idx = 19\n",
    "output = clam_outputs[test_idx]\n",
    "label = output['labels']\n",
    "pred = output['preds']\n",
    "class_probs = output['class_probs']\n",
    "patch_attention = output['patch_attention']\n",
    "\n",
    "idx = test_dataset.labels[test_idx][0]\n",
    "img = plt.imread(os.path.join(DATA_DIR, \"biopsies\", f\"{idx}.png\")) # shape (h, w, 3)\n",
    "patch_size = 256\n",
    "# _, non_empty_indices = process_image(torch.tensor(img).permute(2,0,1), patch_size)\n",
    "non_empty_indices = non_empty_patch_indices_by_biopsy[idx]\n",
    "patch_rows = max(round(img.shape[0] / patch_size), 1)\n",
    "patch_cols = max(round(img.shape[1] / patch_size), 1)\n",
    "# The patch attention is a 1D array and corresponds to the non-empty indices\n",
    "all_patch_attention = torch.zeros(patch_rows * patch_cols)\n",
    "all_patch_attention[non_empty_indices] = torch.tensor(patch_attention)\n",
    "all_patch_attention = all_patch_attention.reshape(patch_rows, patch_cols)\n",
    "heatmap = cv2.resize(all_patch_attention.numpy(), (img.shape[1], img.shape[0]))\n",
    "print(heatmap.min(), heatmap.max())\n",
    "# heatmap = torch.nn.functional.sigmoid(torch.tensor(heatmap-heatmap.min())/10).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title(f\"Label: {P53_CLASS_NAMES[label]}, Pred: {P53_CLASS_NAMES[pred]}\")\n",
    "\n",
    "if label == 2:\n",
    "    mask = plt.imread(os.path.join(DATA_DIR, \"masks\", f\"{idx}.png\"))\n",
    "    # Draw black contour on the img\n",
    "    contours, _ = cv2.findContours((mask > 0).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(img, contours, -1, (0, 0, 0), 5)\n",
    "\n",
    "ax[1].imshow(img)\n",
    "ax[1].imshow(heatmap, alpha=0.3, cmap='jet'\n",
    "            #  , vmin=5, vmax=10\n",
    "             )\n",
    "ax[1].set_title(f\"Label: {P53_CLASS_NAMES[label]}, Pred: {P53_CLASS_NAMES[pred]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Biopsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM for non-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient-based class activation maps (Grad-CAM)\n",
    "#\n",
    "# Grad-CAM is a technique to visualize the regions of the image that are important for the model's\n",
    "# prediction. It does this by computing the gradients of the output class with respect to the\n",
    "# feature maps of the last convolutional layer of the model. The gradients are then used to compute\n",
    "# a weighted sum of the feature maps, where the weights are the gradients. The resulting heatmap\n",
    "# is then overlaid on the original image to visualize the important regions.\n",
    "#\n",
    "# The code below is adapted from the PyTorch Grad-CAM tutorial:\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#grad-cam\n",
    "def get_last_conv_layer(model):\n",
    "    for name, layer in reversed(list(model.named_modules())):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            return name, layer\n",
    "    raise ValueError(\"No convolutional layer found in the model\")\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer, target_layer_name):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.model.eval()\n",
    "        self.feature_grad = None\n",
    "        self.feature_map = None\n",
    "        self.hook_feature_map()\n",
    "        self.hook_feature_grad()\n",
    "\n",
    "    def hook_feature_map(self):\n",
    "        def hook_fn(module, input, output):\n",
    "            self.feature_map = output\n",
    "        self.target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    def hook_feature_grad(self):\n",
    "        def hook_fn(module, grad_input, grad_output):\n",
    "            self.feature_grad = grad_output[0]\n",
    "        self.target_layer.register_backward_hook(hook_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, output, target_class):\n",
    "        self.model.zero_grad()\n",
    "        one_hot_output = torch.zeros((1, output.size()[-1]), dtype=torch.float32, device=output.device)\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "    def __call__(self, x, target_class, img_size):\n",
    "        output = self.forward(x)\n",
    "        self.backward(output, target_class)\n",
    "        if self.feature_grad is None or self.feature_map is None:\n",
    "            raise ValueError(\"Feature gradients or feature maps are not set. Check hooks.\")\n",
    "        \n",
    "        weights = torch.mean(self.feature_grad, dim=(2, 3)).squeeze()\n",
    "        cam = torch.tensordot(weights, self.feature_map.squeeze(0), dims=([0], [0]))\n",
    "        cam = torch.nn.functional.relu(cam)\n",
    "        \n",
    "        # Avoid NaNs in normalization\n",
    "        if torch.isnan(cam).any():\n",
    "            raise ValueError(\"CAM contains NaN values before normalization.\")\n",
    "        \n",
    "        cam = torch.nn.functional.interpolate(cam.unsqueeze(0).unsqueeze(0), size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
    "        cam = cam.squeeze(0).squeeze(0)\n",
    "        \n",
    "        cam_min, cam_max = cam.min(), cam.max()\n",
    "        cam = (cam - cam_min)\n",
    "        if cam_max == cam_min:\n",
    "            print(\"CAM has uniform values. Check model and target class.\")\n",
    "        else:\n",
    "            cam /= (cam_max - cam_min)\n",
    "        \n",
    "        return cam\n",
    "\n",
    "def get_grad_cam(model, img, target_class, img_size):\n",
    "    model.eval()\n",
    "    last_conv_layer_name, last_conv_layer = get_last_conv_layer(model)\n",
    "    grad_cam = GradCam(model, last_conv_layer, last_conv_layer_name)\n",
    "    return grad_cam(img, target_class, img_size)\n",
    "\n",
    "    \n",
    "# Visualize the Grad-CAM heatmap for the predicted class\n",
    "# Doubleclone idx: 56, 46, 53, 54\n",
    "idx = np.random.randint(len(test_dataset))\n",
    "idx = 19\n",
    "print(\"Image index: \", idx)\n",
    "img, label = test_dataset[idx]\n",
    "img_size = img.size(-1)\n",
    "show_img = img.permute(1, 2, 0)\n",
    "img = img.unsqueeze(0).to(device)\n",
    "outputs = model(img)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print(\"Predicted: \", predicted.item())\n",
    "grad_cam_class1 = get_grad_cam(model, img, 1, img_size).cpu().detach().numpy().squeeze()\n",
    "grad_cam_class2 = get_grad_cam(model, img, 2, img_size).cpu().detach().numpy().squeeze()\n",
    "\n",
    "# Normalize the img\n",
    "show_img = (show_img - show_img.min()) / (show_img.max() - show_img.min())\n",
    "\n",
    "# Plot the original image and the Grad-CAM heatmap, and the original image with the heatmap overlayed\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30, 30))\n",
    "ax[0].imshow(show_img)\n",
    "ax[0].set_title(\"Original image\\nLabel: {}\\nPredicted: {}\".format(P53_CLASS_NAMES[label], P53_CLASS_NAMES[predicted.item()]))\n",
    "ax[1].imshow(show_img)\n",
    "ax[1].imshow(grad_cam_class1, vmin=0, vmax=1, alpha=0.5, cmap='jet')\n",
    "ax[1].set_title(\"Grad-CAM heatmap\\nOverexpression\")\n",
    "ax[2].imshow(show_img)\n",
    "ax[2].imshow(grad_cam_class2, vmin=0, vmax=1, alpha=0.5, cmap='jet')\n",
    "ax[2].set_title(\"Grad-CAM heatmap\\nNull mutation\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM for db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Class Activation Mapping (CAM) For Double Binary Model\n",
    "#\n",
    "# Keep in mind that in this case, we have a separate heatmap for the first and second binary classifier head\n",
    "# The first heatmap will show the regions of the image that are important for the Overexpression prediction\n",
    "# The second heatmap will show the regions of the image that are important for the Nullmutation prediction\n",
    "\n",
    "# Visualize the CAM heatmap for the predicted class\n",
    "idx = np.random.randint(len(test_dataset))\n",
    "# idx = 19\n",
    "print(\"Image index: \", idx)\n",
    "img, label = test_dataset[idx]\n",
    "img_size = img.size(-1)\n",
    "show_img = img.permute(1, 2, 0)\n",
    "img = img.unsqueeze(0).to(device)\n",
    "outputs = model_db(img).cpu().detach().numpy().squeeze()\n",
    "prediction = int(outputs[0] > 0.5) + 2 * int(outputs[1] > 0.5)\n",
    "# In this case, the output is a tensor of shape (1, 2) with the independent predictions for each binary classifier\n",
    "# Therefore we don't apply the argmax function to get the predicted class\n",
    "print(\"Predicted: \", P53_CLASS_NAMES[prediction])\n",
    "grad_cam_class1 = get_grad_cam(model_db, img, 0, img_size).cpu().detach().numpy().squeeze()\n",
    "grad_cam_class2 = get_grad_cam(model_db, img, 1, img_size).cpu().detach().numpy().squeeze()\n",
    "cam_class1 = get_cam(model_db, img, 0).cpu().detach().numpy().squeeze()\n",
    "cam_class2 = get_cam(model_db, img, 1).cpu().detach().numpy().squeeze()\n",
    "\n",
    "# Normalize the img\n",
    "show_img = (show_img - show_img.min()) / (show_img.max() - show_img.min())\n",
    "\n",
    "# Plot the original image and the grad-CAM and CAM heatmap\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
    "ax[0, 0].imshow(show_img)\n",
    "ax[0, 0].set_title(\"Original image\\nLabel: {}\\nPredicted: {}\".format(P53_CLASS_NAMES[label], P53_CLASS_NAMES[prediction]))\n",
    "ax[0, 1].imshow(show_img)\n",
    "ax[0, 1].imshow(grad_cam_class1, vmin=0, vmax=1, alpha=0.5, cmap='jet')\n",
    "ax[0, 1].set_title(f\"Grad-CAM heatmap\\nOverexpression {outputs[0]:.2f}\")\n",
    "ax[0, 2].imshow(show_img)\n",
    "ax[0, 2].imshow(grad_cam_class2, vmin=0, vmax=1, alpha=0.5, cmap='jet')\n",
    "ax[0, 2].set_title(f\"Grad-CAM heatmap\\nNullmutation {outputs[1]:.2f}\")\n",
    "ax[1, 1].imshow(show_img)\n",
    "ax[1, 1].imshow(cam_class1, cmap='jet', vmin=0, vmax=1, alpha=0.5)\n",
    "ax[1, 1].set_title(f\"CAM heatmap\\nOverexpression {outputs[0]:.2f}\")\n",
    "ax[1, 2].imshow(show_img)\n",
    "ax[1, 2].imshow(cam_class2, cmap='jet', vmin=0, vmax=1, alpha=0.5)\n",
    "ax[1, 2].set_title(f\"CAM heatmap\\nNullmutation {outputs[1]:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_heatmap(idx):\n",
    "    img = plt.imread(os.path.join(DATA_DIR, 'biopsies', f'{idx}.png')) # Shape: (H, W, 3)\n",
    "    img_size = 1024\n",
    "    img = torch.tensor(img).permute(2, 0, 1).to(device) # Shape: (3, H, W)\n",
    "    # Resize image\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=(img_size, img_size), mode='bilinear', align_corners=False)\n",
    "    img = test_transform(img) # Shape: (1, 3, H, W)\n",
    "\n",
    "    # Do the same as above but this time with overlap between the patches, to make the heatmap smoother\n",
    "    # So for example with patch size 256, but step size 64, we will have 16 times more patches\n",
    "    patch_size = 128\n",
    "    step_size = 64\n",
    "\n",
    "    # Pad the image to accommodate the step size when sliding the patches\n",
    "    pad = (patch_size - step_size)\n",
    "    img = torch.nn.functional.pad(img, (pad, 0, pad, 0)) # the order is left, right, top, bottom\n",
    "    WT_img = torch.zeros(3, img_size, img_size)\n",
    "    img_size = img.shape[-1]\n",
    "    # Fill with mean color of the image\n",
    "    pixels = img.squeeze().cpu()\n",
    "    WT_img[0] = pixels[0].mean()\n",
    "    WT_img[1] = pixels[1].mean()\n",
    "    WT_img[2] = pixels[2].mean()\n",
    "    WT_img = torch.nn.functional.pad(WT_img, (pad, 0, pad, 0))\n",
    "\n",
    "    patches_added = WT_img.unsqueeze(0).clone() # Shape: (1, 3, img_size, img_size)\n",
    "    steps = img_size // step_size\n",
    "    middle = img_size // 2 - patch_size // 2\n",
    "    patches_added = patches_added.repeat(steps, steps, 1, 1, 1)\n",
    "    for i in range(0, img_size, step_size):\n",
    "        for j in range(0, img_size, step_size):\n",
    "            # Place patch in the middle of patches_added\n",
    "            patch = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "            w, h = patch.shape[-2], patch.shape[-1] # This is necessary because the patch can be smaller than patch_size at the edges\n",
    "            patches_added[i // step_size, j // step_size, :, middle:middle+w, middle:middle+h] = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "    # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "    patches_added = patches_added.view(-1, 3, img_size, img_size)\n",
    "\n",
    "    # Crop to the original image size\n",
    "    patches_added = patches_added[:, :, pad:, pad:]\n",
    "\n",
    "    # Get the model output for each image\n",
    "    diff = torch.zeros(patches_added.shape[0], 2)\n",
    "    # for i in tqdm(range(patches_added.shape[0]), desc=\"Generating Heatmap\"): # necessary for CUDA memory\n",
    "    for i in range(patches_added.shape[0]): # necessary for CUDA memory\n",
    "        current_img = patches_added[i].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            diff[i] = model_db(current_img).cpu().detach().squeeze()\n",
    "    # Normalize the difference\n",
    "    diff[:, 0] = (diff[:, 0] - diff[:, 0].min())\n",
    "    diff[:, 1] = (diff[:, 1] - diff[:, 1].min())\n",
    "\n",
    "    # Make heatmap as grid of patch outputs\n",
    "    overexpression_heatmap = np.zeros((steps, steps))\n",
    "    nullmutation_heatmap = np.zeros((steps, steps))\n",
    "\n",
    "    # Make a mask to keep track of the number of patches that overlap in each pixel\n",
    "    mask = np.zeros((steps, steps))\n",
    "\n",
    "    steps_per_patch = patch_size // step_size\n",
    "    for i in range(diff.shape[0]):\n",
    "        row = i // steps\n",
    "        col = i % steps\n",
    "        overexpression_heatmap[row:row+steps_per_patch, col:col+steps_per_patch] += diff[i, 0].item()\n",
    "        nullmutation_heatmap[row:row+steps_per_patch, col:col+steps_per_patch] += diff[i, 1].item()\n",
    "        mask[row:row+steps_per_patch, col:col+steps_per_patch] += 1\n",
    "\n",
    "    # Normalize the img by dividing by the mask\n",
    "    overexpression_heatmap /= mask\n",
    "    nullmutation_heatmap /= mask\n",
    "\n",
    "    # Crop to the original image size\n",
    "    pad_steps = pad // step_size\n",
    "    overexpression_heatmap = overexpression_heatmap[pad_steps:, pad_steps:]\n",
    "    nullmutation_heatmap = nullmutation_heatmap[pad_steps:, pad_steps:]\n",
    "\n",
    "    return overexpression_heatmap, nullmutation_heatmap\n",
    "\n",
    "\n",
    "def plot_heatmap(img, overexpression_heatmap, nullmutation_heatmap, mask=None):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(\"Original image\")\n",
    "    # Overlay the heatmap on the image\n",
    "    ax[1].imshow(img)\n",
    "    # Rescale the heatmap to the image size\n",
    "    overexpression_heatmap = cv2.resize(overexpression_heatmap, (img.shape[1], img.shape[0]), \n",
    "                                        # interpolation=cv2.INTER_NEAREST\n",
    "                                        )\n",
    "    ax[1].imshow(overexpression_heatmap, alpha=0.3, cmap='jet', vmin=0, \n",
    "                 vmax=1\n",
    "                 )\n",
    "    ax[1].set_title(\"Overexpression heatmap\")\n",
    "\n",
    "    # If mask is provided, draw its contours on the null mutation heatmap\n",
    "    if mask is not None:\n",
    "        contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(img, contours, -1, (0,0,0), 5)\n",
    "\n",
    "    ax[2].imshow(img)\n",
    "    nullmutation_heatmap = cv2.resize(nullmutation_heatmap, (img.shape[1], img.shape[0]), \n",
    "                                    #   interpolation=cv2.INTER_NEAREST\n",
    "                                      )\n",
    "    ax[2].imshow(nullmutation_heatmap, alpha=0.3, cmap='jet', vmin=0, \n",
    "                 vmax=1\n",
    "                 )\n",
    "    ax[2].set_title(\"Null mutation heatmap\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how large ResNet18s receptive field is for different image sizes\n",
    "from resnet import ResNetModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dummy input image\n",
    "img_size = 1024\n",
    "img = torch.zeros(1, 3, img_size, img_size)\n",
    "img[:, :, img_size//2, img_size//2] = 1\n",
    "img[:, :, img_size//4, img_size//4] = 1\n",
    "# img[:, :, img_size//4:3*img_size//4, img_size//4:3*img_size//4] = 1\n",
    "\n",
    "plt.imshow(img.squeeze().numpy().transpose(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "# Load the model\n",
    "model = models.resnet18()\n",
    "# Set all parameters to 1\n",
    "# for param in model.parameters():\n",
    "#     param.data.fill_(1)\n",
    "# Cut off before the pooling layer\n",
    "# model = nn.Sequential(*list(list(model.children())[0].children())[:-2])\n",
    "model = nn.Sequential(*list(model.children())[:-2])\n",
    "# display(model)\n",
    "model.eval()\n",
    "\n",
    "# Get the output of the model\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "\n",
    "# Plot the output\n",
    "output = output.squeeze().numpy().transpose(1, 2, 0).max(axis=2)\n",
    "# display(output)\n",
    "plt.imshow(output)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
