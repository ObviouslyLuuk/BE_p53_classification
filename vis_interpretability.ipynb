{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from collections import defaultdict\n",
    "\n",
    "from load_data import DATA_DIR, TRANSFORMS, P53_CLASS_NAMES, \\\n",
    "    convert_presence_probs_to_status_probs\n",
    "from resnet import ResNetModel, ResNetModelDoubleBinary\n",
    "from pl_clam import CLAM_MB, CLAM_db\n",
    "\n",
    "P53_CLASS_CODES = [\"WT\", \"OE\", \"NM\", \"DC\"]\n",
    "\n",
    "BOLERO_DIR = os.path.join(DATA_DIR, '..', 'BOLERO')\n",
    "\n",
    "BASE_DIR = {\n",
    "    'test': DATA_DIR,\n",
    "    'bolero': BOLERO_DIR,\n",
    "}\n",
    "\n",
    "RESULTS_DIR = os.path.join(DATA_DIR, '..', '..', 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "VIS_DIR = os.path.join(DATA_DIR, '..', '..', 'visualizations')\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '..', '..', 'models')\n",
    "\n",
    "bag_latent_paths = {\n",
    "    \"test\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "    \"bolero\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "color_dict = {\n",
    "    \"r\": '#DA4C4C', # Red\n",
    "    \"o\": '#E57439', # Orange\n",
    "    \"y\": '#EDB732', # Yellow\n",
    "    \"g\": '#479A5F', # Green\n",
    "    \"lb\": '#5BC5DB', # Light blue\n",
    "    \"b\": '#5387DD', # Blue\n",
    "    \"p\": '#7D54B2', # Purple\n",
    "    \"pi\": '#E87B9F', # Pink\n",
    "#  '#229487', # Dark green/Turquoise\n",
    "#  '#C565C7', # Lilac\n",
    "    \"r_p\": '#E89393', # Pale red\n",
    "    \"o_p\": '#EFAB88', # Pale orange\n",
    "    \"y_p\": '#F4D384', # Pale yellow\n",
    "    \"g_p\": '#90C29F', # Pale green\n",
    "    \"lb_p\":'#9CDCE9', # Pale light blue\n",
    "    \"b_p\": '#98B7EA', # Pale blue\n",
    "    \"p_p\": \"#B198D0\", # Pale purple\n",
    "}\n",
    "colors = list(color_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb: full biopsy, db: double binary, gs: grid spacing\n",
    "model_kwargs = {\n",
    "    \"CLAM\":     {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db\":  {\"model_class\": CLAM_db, \"gs\": 256},\n",
    "    \"CLAM_m\":   {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db_m\":{\"model_class\": CLAM_db, \"gs\": 256},\n",
    "}\n",
    "for name in [\"fb_db\", \"fb\"]:\n",
    "    for spacing in [2, 4, \n",
    "                    # 8, 16, 32, 64, 128, 256\n",
    "                    ]:\n",
    "        if \"db\" not in name and spacing == 2: # Skipped this one because it's too slow\n",
    "            continue\n",
    "        model_name = f\"{name}_spacing{spacing}\"\n",
    "        model_kwargs[model_name] = {\"spacing\": spacing}\n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"fb_db\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModelDoubleBinary\n",
    "    elif \"fb\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModel\n",
    "    checkpoint_dir = os.path.join(MODELS_DIR, model_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model_kwargs[model_name][\"checkpoint_paths\"] = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n",
    "\n",
    "def load_model(model_class, checkpoint_path):\n",
    "    if \"CLAM\" in model_class.__name__: # For some reason pl can't load these models with load_from_checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        # model = model_class(**checkpoint[\"model_kwargs\"])\n",
    "        model = model_class()\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        model = model_class.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many checkpoints we have for each model\n",
    "for model_name in model_kwargs:\n",
    "    print(f\"{model_name}: {len(model_kwargs[model_name]['checkpoint_paths'])} checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_dict = {\n",
    "    \"CLAM\": 4,\n",
    "    \"CLAM_db\": 0,\n",
    "    \"CLAM_m\": 3,\n",
    "    \"CLAM_db_m\": 2,\n",
    "    \"fb_spacing4\": 4,\n",
    "    \"fb_db_spacing4\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_name, data_name):\n",
    "    \"\"\"\n",
    "    Returns (H, W, C) float32 tensor\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(BASE_DIR[data_name], 'biopsies', f\"{img_name}.png\")\n",
    "    img = plt.imread(img_path) # (H, W, C) float32\n",
    "    return img\n",
    "    \n",
    "def load_patch_latents(img_name, data_name, bag_latents):\n",
    "    \"\"\"\n",
    "    Returns (N, 2048) tensor\n",
    "    \"\"\"\n",
    "    if data_name == \"test\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "    elif data_name == \"bolero\":\n",
    "        slide_name, biopsy_name = tuple(img_name.split(\"_\"))\n",
    "        slide_latents = bag_latents[int(slide_name)] # (n_biopsies, N, 2048)\n",
    "        return slide_latents[int(biopsy_name)+1] # (N, 2048)\n",
    "    elif data_name == \"pathxl\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "    \n",
    "def get_img_const_spacing(img_name, data_name, spacing):\n",
    "    \"\"\"\n",
    "    Returns (1, C, H, W) tensor with constant spacing (spacing x spacing pixels per grid cell)\n",
    "    \"\"\"\n",
    "    img = load_img(img_name, data_name)\n",
    "    h = img.shape[0] // spacing\n",
    "    w = img.shape[1] // spacing\n",
    "    img = torch.nn.functional.interpolate(torch.tensor(img).permute(2, 0, 1).unsqueeze(0), size=(h, w), mode='bilinear')\n",
    "    img = TRANSFORMS['normalize'](img)\n",
    "    return img\n",
    "\n",
    "def call_constant_spacing(model, img_name, data_name, spacing, **kwargs):\n",
    "    img = get_img_const_spacing(img_name, data_name, spacing)\n",
    "    with torch.no_grad():\n",
    "        return model(img.to(device)).cpu().detach().numpy()\n",
    "\n",
    "def call_CLAM(model, img_name, data_name, bag_latents, **kwargs):\n",
    "    patch_latents = load_patch_latents(img_name, data_name, bag_latents).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits, Y_prob, Y_hat, A_raw, results_dict = model(patch_latents.to(device))\n",
    "        return Y_prob.cpu().detach().numpy(), A_raw.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"spacing\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"call\"] = call_constant_spacing\n",
    "    elif \"CLAM\" in model_name:\n",
    "        model_kwargs[model_name][\"call\"] = call_CLAM\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown call type for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion(model, img, patch_size=128, step_size=64, mode=\"inclusion\", print_progress=True):\n",
    "    # Make sure the image is divisible by the patch size\n",
    "    h, w = img.shape[-2], img.shape[-1]\n",
    "    h_padded, w_padded = int(np.ceil(h/patch_size))*patch_size, int(np.ceil(w/patch_size))*patch_size\n",
    "    pad_h, pad_w = h_padded - h, w_padded - w\n",
    "\n",
    "    pixels = img.squeeze().cpu()\n",
    "    img = torch.nn.functional.pad(img, (pad_w, 0, pad_h, 0)) # the order is left, right, top, bottom\n",
    "    h_padded, w_padded = img.shape[-2], img.shape[-1]\n",
    "\n",
    "    if mode == \"inclusion\":\n",
    "        # dummy_img = torch.zeros_like(img).squeeze()\n",
    "        dummy_img = torch.zeros(3, 1024, 1024)\n",
    "        # # Fill with mean color of the image\n",
    "        # dummy_img[0] = pixels[0].mean()\n",
    "        # dummy_img[1] = pixels[1].mean()\n",
    "        # dummy_img[2] = pixels[2].mean()\n",
    "\n",
    "        # Place patches in the middle of the dummy image\n",
    "        patches_changed = dummy_img.unsqueeze(0).clone() # Shape: (1, 3, img_h, img_w)\n",
    "        h_steps = h_padded // step_size\n",
    "        w_steps = w_padded // step_size\n",
    "        patches_changed = patches_changed.repeat(h_steps, w_steps, 1, 1, 1)\n",
    "        for i in range(0, h_padded, step_size):\n",
    "            for j in range(0, w_padded, step_size):\n",
    "                # Place patch in the middle of patches_changed\n",
    "                h_middle = dummy_img.shape[-2] // 2 - patch_size // 2\n",
    "                w_middle = dummy_img.shape[-1] // 2 - patch_size // 2\n",
    "                patch = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "                ph, pw = patch.shape[-2], patch.shape[-1]\n",
    "                patches_changed[i // step_size, j // step_size, :, h_middle:h_middle+ph, w_middle:w_middle+pw] = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "        # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "        patches_changed = patches_changed.view(-1, 3, dummy_img.shape[-2], dummy_img.shape[-1])\n",
    "    elif mode == \"patch\":\n",
    "        # Just have patches by themselves\n",
    "        h_steps = h_padded // step_size\n",
    "        w_steps = w_padded // step_size\n",
    "        patches_changed = torch.zeros(h_steps, w_steps, 3, patch_size, patch_size)\n",
    "        for i in range(0, h_padded, step_size):\n",
    "            for j in range(0, w_padded, step_size):\n",
    "                # Place patch in the middle of patches_changed\n",
    "                patch = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "                ph, pw = patch.shape[-2], patch.shape[-1]\n",
    "                h_middle = patch_size // 2 - ph // 2\n",
    "                w_middle = patch_size // 2 - pw // 2\n",
    "                patches_changed[i // step_size, j // step_size, :, h_middle:h_middle+ph, w_middle:w_middle+pw] = patch\n",
    "        # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "        patches_changed = patches_changed.view(-1, 3, patch_size, patch_size)\n",
    "    elif mode == \"occlusion\":\n",
    "        with torch.no_grad():\n",
    "            full_prediction = model(img.to(device)).cpu().detach().squeeze()\n",
    "            if len(full_prediction) == 4:\n",
    "                full_prediction = torch.nn.functional.softmax(full_prediction, dim=0)\n",
    "                full_prediction = full_prediction[[1,2]]\n",
    "        # Copy the image to the dummy image except for the patch\n",
    "        h_steps = h_padded // step_size\n",
    "        w_steps = w_padded // step_size\n",
    "        patches_changed = img.repeat(h_steps, w_steps, 1, 1, 1)\n",
    "        for i in range(0, h_padded, step_size):\n",
    "            for j in range(0, w_padded, step_size):\n",
    "                patches_changed[i // step_size, j // step_size, :, i:i+patch_size, j:j+patch_size] = 0\n",
    "        # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "        patches_changed = patches_changed.view(-1, 3, h_padded, w_padded)\n",
    "\n",
    "    # Get the model output for each image\n",
    "    diff = torch.zeros(patches_changed.shape[0], 2)\n",
    "    iterator = range(patches_changed.shape[0])\n",
    "    if print_progress:\n",
    "        iterator = tqdm(iterator)\n",
    "    for i in iterator: # necessary for CUDA memory\n",
    "        current_img = patches_changed[i].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(current_img).cpu().detach().squeeze()\n",
    "            if len(preds) == 4:\n",
    "                preds = torch.nn.functional.softmax(preds, dim=0)\n",
    "                preds = preds[[1,2]]\n",
    "            diff[i] = preds\n",
    "    if mode != \"occlusion\":\n",
    "        # Threshold the difference at the minimum value\n",
    "        diff[:, 0] = (diff[:, 0] - diff[:, 0].min())\n",
    "        diff[:, 1] = (diff[:, 1] - diff[:, 1].min())\n",
    "    else:\n",
    "        # # Subtract the full prediction\n",
    "        diff[:, 0] = full_prediction[0] - diff[:, 0]\n",
    "        diff[:, 1] = full_prediction[1] - diff[:, 1]\n",
    "        # Center at 0.5\n",
    "        diff[:, 0] = diff[:, 0]*2 + 0.5\n",
    "        diff[:, 1] = diff[:, 1]*2 + 0.5\n",
    "\n",
    "    # Make heatmap as grid of patch outputs\n",
    "    overexpression_heatmap = np.zeros((h_steps, w_steps))\n",
    "    nullmutation_heatmap = np.zeros((h_steps, w_steps))\n",
    "\n",
    "    # Make a mask to keep track of the number of patches that overlap in each pixel\n",
    "    mask = np.zeros((h_steps, w_steps))\n",
    "\n",
    "    steps_per_patch = patch_size // step_size\n",
    "    for i in range(diff.shape[0]):\n",
    "        row = i // w_steps\n",
    "        col = i % w_steps\n",
    "        overexpression_heatmap[row:row+steps_per_patch, col:col+steps_per_patch] += diff[i, 0].item()\n",
    "        nullmutation_heatmap  [row:row+steps_per_patch, col:col+steps_per_patch] += diff[i, 1].item()\n",
    "        mask                  [row:row+steps_per_patch, col:col+steps_per_patch] += 1\n",
    "\n",
    "    # Crop to the original image size\n",
    "    pad_steps_h = pad_h // step_size\n",
    "    pad_steps_w = pad_w // step_size\n",
    "    overexpression_heatmap = overexpression_heatmap[pad_steps_h:, pad_steps_w:]\n",
    "    nullmutation_heatmap = nullmutation_heatmap[pad_steps_h:, pad_steps_w:]\n",
    "    mask = mask[pad_steps_h:, pad_steps_w:]\n",
    "\n",
    "    # Normalize the img by dividing by the mask\n",
    "    overexpression_heatmap /= mask\n",
    "    nullmutation_heatmap /= mask\n",
    "\n",
    "    return overexpression_heatmap, nullmutation_heatmap\n",
    "\n",
    "def vis_constant_spacing(model, img_name, data_name, spacing, **kwargs):\n",
    "    img = get_img_const_spacing(img_name, data_name, spacing) # (1, 3, H, W)\n",
    "    return list(occlusion(model, img, **kwargs))\n",
    "\n",
    "\n",
    "def load_attention_maps(model_name, data_name):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        results = defaultdict(lambda: defaultdict(dict))\n",
    "        for data_name in [\"test\", \"pathxl\"]:\n",
    "            results_data = load_attention_maps(model_name, data_name)\n",
    "            for key in results_data:\n",
    "                results[key].update(results_data[key])\n",
    "        return results\n",
    "\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    checkpoint_name = [f for f in os.listdir(results_dir) if f.startswith(data_name)][best_checkpoint_idx]\n",
    "    results_path = os.path.join(results_dir, checkpoint_name)\n",
    "    result_content = torch.load(results_path)\n",
    "    if \"CLAM_db\" in model_name:\n",
    "        for img_name, (presence_probs, A_raw) in result_content.items():\n",
    "            results[\"presence_probs\"][img_name] = presence_probs\n",
    "            results[\"status_probs\"][img_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "            results[\"A_raw\"][img_name] = A_raw\n",
    "    elif \"CLAM\" in model_name:\n",
    "        for img_name, (status_probs, A_raw) in result_content.items():\n",
    "            results[\"status_probs\"][img_name] = status_probs\n",
    "            results[\"A_raw\"][img_name] = A_raw\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    return results\n",
    "\n",
    "def vis_CLAM(model, img_name, data_name, attention_maps, non_empty_patch_indices, biopsy_dims):\n",
    "    prediction = np.round(attention_maps[\"status_probs\"][img_name],2)\n",
    "    pred_argmax = np.argmax(prediction)\n",
    "    print(\"Prediction:\", prediction)\n",
    "    # Rank the class predictions, like prediction=[0.1, 0.6, 0.2, 0.1] -> [1, 2, 0, 3]\n",
    "    pred_rank = torch.argsort(-torch.tensor(prediction))[0]\n",
    "    multipliers = [1,1,1,1]\n",
    "    for i in range(len(pred_rank)):\n",
    "        branch = pred_rank[i]\n",
    "        multipliers[branch] = 1/(i+1)\n",
    "    multipliers = multipliers[1:] # Skip the WT class\n",
    "\n",
    "    attention_map = attention_maps[\"A_raw\"][img_name] # (K, N)\n",
    "    K = attention_map.shape[0]\n",
    "    non_empty_indices = non_empty_patch_indices[img_name] # (N,)\n",
    "    img_h, img_w = biopsy_dims[str(img_name)]\n",
    "\n",
    "    patch_size = 256\n",
    "    patch_rows = max(round(img_h / patch_size), 1)\n",
    "    patch_cols = max(round(img_w / patch_size), 1)\n",
    "    # The patch attention is a 1D array and corresponds to the non-empty indices\n",
    "    all_patch_attention = torch.zeros(2, patch_rows * patch_cols)\n",
    "    if K == 4:\n",
    "        # for i in range(3):\n",
    "        #     all_patch_attention[i].fill_(attention_map[1:][i].min())\n",
    "        # all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)[1:] # Skip the WT class\n",
    "\n",
    "        all_patch_attention[0].fill_(attention_map[pred_argmax].min())\n",
    "        all_patch_attention[0, non_empty_indices] = torch.tensor(attention_map)[pred_argmax]\n",
    "        all_patch_attention = all_patch_attention[:1]\n",
    "\n",
    "        # all_patch_attention.fill_(attention_map.min())\n",
    "        # all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)[1:3]\n",
    "\n",
    "        # all_patch_attention = torch.zeros(4, patch_rows * patch_cols)\n",
    "        # all_patch_attention.fill_(attention_map.min())\n",
    "        # all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)\n",
    "    elif K == 2:\n",
    "        all_patch_attention.fill_(attention_map.min())\n",
    "        all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)\n",
    "        # Normalize the attention values to [0, 1]\n",
    "        # all_patch_attention = (all_patch_attention - all_patch_attention.min()) / (all_patch_attention.max() - all_patch_attention.min())\n",
    "        # Convert attention scores to percentiles\n",
    "        # all_patch_attention[pred_argmax] = torch.tensor(rankdata(all_patch_attention[pred_argmax], method='ordinal') / len(all_patch_attention[pred_argmax]))\n",
    "\n",
    "    for i in range(len(all_patch_attention)):\n",
    "        # all_patch_attention[i] = (all_patch_attention[i] - all_patch_attention[i].min()) / (all_patch_attention[i].max() - all_patch_attention[i].min()) * multipliers[i]\n",
    "        all_patch_attention[i] = (all_patch_attention[i] - all_patch_attention[i].min()) / (all_patch_attention.max() - all_patch_attention[i].min())\n",
    "\n",
    "    heatmaps = [a.numpy() for a in all_patch_attention.reshape(-1, patch_rows, patch_cols)]\n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def load_mask(img_name, data_name):\n",
    "    mask_path = os.path.join(BASE_DIR[data_name], 'masks', f\"{img_name}.png\")\n",
    "    if not os.path.exists(mask_path):\n",
    "        return None\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return mask\n",
    "\n",
    "def plot_heatmap(heatmaps, img_name, data_name, downsample=0.25, in_one=False):\n",
    "    img = load_img(img_name, data_name) # (H, W, C) float32\n",
    "    img = cv2.resize(img, (0,0), fx=downsample, fy=downsample)  \n",
    "    for i, heatmap in enumerate(heatmaps):\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) # For upsampling, this uses linear interpolation\n",
    "        heatmaps[i] = heatmap\n",
    "    mask = load_mask(img_name, data_name)\n",
    "    if mask is not None:\n",
    "        mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "    heatmap_colors = ['Oranges', 'Blues'] if len(heatmaps) == 2 else ['Oranges', 'Blues', 'Purples']\n",
    "    if not in_one:\n",
    "        # Plot the img and then the heatmaps overlayed one by one to the side\n",
    "        fig, ax = plt.subplots(1, len(heatmaps)+1, figsize=(20, 10))\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].axis('off')\n",
    "        masks = [None, mask] if len(heatmaps) == 2 else [None, None, mask, mask]\n",
    "        for i, heatmap in enumerate(heatmaps):\n",
    "            show_img = img.copy()\n",
    "            if masks[i] is not None:\n",
    "                contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                cv2.drawContours(show_img, contours[0], -1, (0, 0, 0), 2)\n",
    "            ax[i+1].imshow(show_img)\n",
    "            ax[i+1].imshow(heatmap, alpha=0.3, cmap='jet', vmin=0, vmax=1)\n",
    "            ax[i+1].axis('off')\n",
    "    else:\n",
    "        # Plot the img and then the heatmaps overlayed on top of each other\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        # Add the mask\n",
    "        if mask is not None:\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img, contours[0], -1, (0, 0, 0), 2)\n",
    "            ax.imshow(img, alpha=0.3)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        for i, h in enumerate(heatmaps):\n",
    "            heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "            ax.imshow(h, alpha=0.3, cmap=heatmap_colors[i], vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"spacing\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"vis\"] = vis_constant_spacing\n",
    "    elif \"CLAM\" in model_name:\n",
    "        model_kwargs[model_name][\"vis\"] = vis_CLAM\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown call type for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_conv_layer(model):\n",
    "    for name, layer in reversed(list(model.named_modules())):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            return name, layer\n",
    "    raise ValueError(\"No convolutional layer found in the model\")\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer, target_layer_name):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.model.eval()\n",
    "        self.feature_grad = None\n",
    "        self.feature_map = None\n",
    "        self.hook_feature_map()\n",
    "        self.hook_feature_grad()\n",
    "\n",
    "    def hook_feature_map(self):\n",
    "        def hook_fn(module, input, output):\n",
    "            self.feature_map = output\n",
    "        self.target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    def hook_feature_grad(self):\n",
    "        def hook_fn(module, grad_input, grad_output):\n",
    "            self.feature_grad = grad_output[0]\n",
    "        self.target_layer.register_backward_hook(hook_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, output, target_class):\n",
    "        self.model.zero_grad()\n",
    "        one_hot_output = torch.zeros((1, output.size()[-1]), dtype=torch.float32, device=output.device)\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        cams = []\n",
    "        output = self.forward(x)\n",
    "        n_outputs = output.size()[-1]\n",
    "        for target_class in range(n_outputs):\n",
    "            self.backward(output, target_class)\n",
    "            if self.feature_grad is None or self.feature_map is None:\n",
    "                raise ValueError(\"Feature gradients or feature maps are not set. Check hooks.\")\n",
    "            \n",
    "            weights = torch.mean(self.feature_grad, dim=(2, 3)).squeeze()\n",
    "            cam = torch.tensordot(weights, self.feature_map.squeeze(0), dims=([0], [0]))\n",
    "            # cam = torch.nn.functional.relu(cam)\n",
    "            \n",
    "            # Avoid NaNs in normalization\n",
    "            if torch.isnan(cam).any():\n",
    "                raise ValueError(\"CAM contains NaN values before normalization.\")\n",
    "            cam = cam.detach().cpu().numpy()\n",
    "\n",
    "            cam_min, cam_max = cam.min(), cam.max()\n",
    "            if cam_max == cam_min:\n",
    "                # print(\"CAM has uniform values. Check model and target class.\")\n",
    "                pass\n",
    "            else:\n",
    "                cam /= (cam_max - cam_min)\n",
    "            cam += 0.5\n",
    "            cams.append(cam)\n",
    "        return output, cams\n",
    "\n",
    "def get_grad_cam(model, img_name, data_name):\n",
    "    img = get_img_const_spacing(img_name, data_name, 4)\n",
    "    last_conv_layer_name, last_conv_layer = get_last_conv_layer(model)\n",
    "    grad_cam = GradCam(model, last_conv_layer, last_conv_layer_name)\n",
    "    output, cams = grad_cam(img.to(device))\n",
    "    cams = cams if len(cams) == 2 else cams[1:3]\n",
    "    return output, cams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_names(data_name='test'):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        return get_img_names(\"test\") + get_img_names(\"pathxl\")\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv'))[\"id\"].tolist()\n",
    "    return [img_path.split('.')[0] for img_path in os.listdir(os.path.join(BASE_DIR[data_name], 'biopsies'))]\n",
    "\n",
    "def get_results(model, mdl_kwargs, data_name='test', bag_latents=None):\n",
    "    img_names = get_img_names(data_name)\n",
    "    results = {}\n",
    "    for img_name in tqdm(img_names):\n",
    "        results[img_name] = mdl_kwargs[\"call\"](model, img_name, data_name, bag_latents=bag_latents, **mdl_kwargs)\n",
    "    return results\n",
    "\n",
    "def save_results(results, model_name, data_name, checkpoint_name):\n",
    "    results_dir = os.path.join(VIS_DIR, model_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "    torch.save(results, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run visualizations for fb models\n",
    "# model_heatmaps = { # data_name: {model_name: {img_name: {heatmap_type: heatmaps}}}\n",
    "#     \"test\": {\n",
    "#         \"fb_spacing4\": defaultdict(dict),\n",
    "#         \"fb_db_spacing4\": defaultdict(dict),\n",
    "#     },\n",
    "# }\n",
    "vis_save_path = os.path.join(VIS_DIR, \"fb_models.pt\")\n",
    "model_heatmaps = torch.load(vis_save_path)\n",
    "\n",
    "data_name = \"test\"\n",
    "for model_name in [\"fb_spacing4\", \"fb_db_spacing4\"]:\n",
    "    print(model_name)\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    model = load_model(model_kwargs[model_name][\"model_class\"], model_kwargs[model_name][\"checkpoint_paths\"][best_checkpoint_idx])\n",
    "    img_names = get_img_names(data_name)\n",
    "    for img_name in tqdm(img_names):\n",
    "        for heatmap_type in [\"inclusion\", \"patch\", \"occlusion\", \"gradcam\"]:\n",
    "            # Skip if we already have the results\n",
    "            if img_name in model_heatmaps[data_name][model_name] and heatmap_type in model_heatmaps[data_name][model_name][img_name]:\n",
    "                continue\n",
    "\n",
    "            if heatmap_type == \"gradcam\":\n",
    "                output, heatmaps = get_grad_cam(model, img_name, data_name)\n",
    "            else:\n",
    "                heatmaps = vis_constant_spacing(model, img_name, data_name, model_kwargs[model_name][\"spacing\"], \n",
    "                                                        patch_size=128, step_size=64, mode=heatmap_type, print_progress=False)\n",
    "            model_heatmaps[data_name][model_name][img_name][heatmap_type] = heatmaps\n",
    "\n",
    "vis_save_path = os.path.join(VIS_DIR, \"fb_models.pt\")\n",
    "torch.save(model_heatmaps, vis_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Metrics\n",
    "Load results and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fb_db   has a dict like idx: shape (1,2) with the two mutation probabilities\n",
    "fb      has a dict like idx: shape (1,4) with the four class probabilities\n",
    "CLAM_db has a dict like idx: tuple of: (\n",
    "            shape (1,2) with the two mutation probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "    )\n",
    "CLAM    has a dict like idx: tuple of: (\n",
    "            shape (1,4) with the four class probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "        )\n",
    "\"\"\"\n",
    "model_type_result_keys = {\n",
    "    \"fb_db\": [\"presence_probs\"],\n",
    "    \"fb\": [\"status_probs\"],\n",
    "    \"CLAM_db\": [\"presence_probs\", \"A_raw\"],\n",
    "    \"CLAM\": [\"status_probs\", \"A_raw\"],\n",
    "}\n",
    "def get_result_keys(model_name):\n",
    "    for key in model_type_result_keys: # The order is important\n",
    "        if key in model_name:\n",
    "            return model_type_result_keys[key]\n",
    "\n",
    "def load_results(model_name, data_name):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        results = defaultdict(lambda: defaultdict(dict))\n",
    "        for data_name in [\"test\", \"pathxl\"]:\n",
    "            results_data = load_results(model_name, data_name)\n",
    "            for key in results_data:\n",
    "                results[key].update(results_data[key])\n",
    "        return results\n",
    "\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    for i, checkpoint_name in enumerate([f for f in os.listdir(results_dir) if f.startswith(data_name)]):\n",
    "        results_path = os.path.join(results_dir, checkpoint_name)\n",
    "        result_content = torch.load(results_path)\n",
    "        checkpoint_name = checkpoint_name.replace(f\"{data_name}_\", \"\").replace(\".pt\", \"\")\n",
    "        if \"CLAM_db\" in model_name:\n",
    "            for img_name, (presence_probs, A_raw) in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"CLAM\" in model_name:\n",
    "            for img_name, (status_probs, A_raw) in result_content.items():\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"fb_db\" in model_name:\n",
    "            for img_name, presence_probs in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "        elif \"fb\" in model_name:\n",
    "            for img_name, status_probs in result_content.items():\n",
    "                # results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = torch.nn.functional.softmax(torch.tensor(status_probs), dim=1).numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    return results\n",
    "\n",
    "def get_labels(data_name):\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv')).set_index(\"id\").to_dict(orient='dict')['label']\n",
    "    elif data_name == \"bolero\":\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[data_name], 'P53_BOLERO_T.csv'))\n",
    "        labels = labels.sort_values(by=\"Case ID\")\n",
    "        labels = labels.reset_index(drop=True)\n",
    "        # Map GS to {1:0, 2:1, 3:2, 4:4} where 4 is unknown\n",
    "        labels[\"GS\"] = labels[\"GS\"].map({1:0, 2:1, 3:2, 4:4})\n",
    "        # Only keep GS column\n",
    "        labels = labels[[\"GS\"]].to_dict(orient='dict')[\"GS\"]\n",
    "        return labels\n",
    "    elif data_name.startswith(\"pathxl\"):\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[\"pathxl\"], 'labels.csv'))\n",
    "        # idx is id column and biopsy_nr column separated by _\n",
    "        labels[\"idx\"] = labels[\"id\"].astype(str) + \"_\" + labels[\"biopsy_nr\"].astype(str)\n",
    "        labels = labels.set_index(\"idx\")\n",
    "        # Sort by id primarily and biopsy_nr secondarily\n",
    "        labels = labels.sort_values(by=[\"id\", \"biopsy_nr\"])\n",
    "        # Map label\n",
    "        mapping = {\"WT\":0, \"Overexpression\":1, \"Null\":2, \"Double clones\":3}\n",
    "        labels[\"label\"] = labels[\"label\"].map(mapping)\n",
    "        if data_name == \"pathxl\": # Filter out any concordance % < 75\n",
    "            labels = labels[labels[\"concordance %\"] >= 75]\n",
    "        elif data_name == \"pathxl-100\":\n",
    "            labels = labels[labels[\"concordance %\"] == 100]\n",
    "        labels = labels[[\"label\"]].to_dict(orient='dict')[\"label\"]\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl\"))\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl-100\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl-100\"))\n",
    "        return labels\n",
    "    \n",
    "# load_results(\"fb_spacing4\", \"test\")[\"status_probs\"]\n",
    "# get_labels(\"test+pathxl-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Biopsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load biopsy dims\n",
    "with open(os.path.join(BASE_DIR[\"test\"], \"biopsy_dims.json\"), \"r\") as f:\n",
    "    biopsy_dims = json.load(f)\n",
    "\n",
    "img_names = get_img_names(\"test\")\n",
    "print(img_names[:5])\n",
    "\n",
    "# Get largest biopsies in test set\n",
    "biopsy_dims = {str(k): biopsy_dims[str(k)] for k in img_names}\n",
    "sorted_biopsies = sorted(biopsy_dims.items(), key=lambda x: min(x[1]), reverse=True)\n",
    "sorted_biopsies[:10]\n",
    "display(sorted_biopsies[:10])\n",
    "\n",
    "# Show little thumbnails of these biopsies downsampled by 8x\n",
    "n = 4\n",
    "fig, ax = plt.subplots(n, 5, figsize=(20, 5*n))\n",
    "labels = get_labels(\"test\")\n",
    "for i, (img_name, dims) in enumerate(sorted_biopsies[:n*5]):\n",
    "    img = get_img_const_spacing(img_name, \"test\", 8)\n",
    "    ax[i//5, i%5].imshow(img.squeeze().permute(1, 2, 0))\n",
    "    ax[i//5, i%5].axis('off')\n",
    "    label = labels[img_name]\n",
    "    ax[i//5, i%5].set_title(img_name + f\"\\n({label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_mapping = {\n",
    "    \"fb_spacing4\": \"Full-Biopsy Multiclass\",\n",
    "    \"fb_db_spacing4\": \"Full-Biopsy Double-Binary\",\n",
    "    \"CLAM\": \"CLAM\",\n",
    "    \"CLAM_db\": \"CLAM Double-Binary\",\n",
    "    \"CLAM_m\": \"CLAM +DC\",\n",
    "    \"CLAM_db_m\": \"CLAM Double-Binary +DC\",\n",
    "}\n",
    "model_name_mapping = {\n",
    "    \"fb_spacing4\": \"FB\",\n",
    "    \"fb_db_spacing4\": \"FBdb\",\n",
    "    \"CLAM\": \"CLAM\",\n",
    "    \"CLAM_db\": \"CLAMdb\",\n",
    "    \"CLAM_m\": \"CLAM+DC\",\n",
    "    \"CLAM_db_m\": \"CLAMdb+DC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_dict = {\n",
    "    # # 1416: \"right\",\n",
    "    # 1133: \"top\",\n",
    "    # # 240: \"bottom\",\n",
    "    # 192: \"right\",\n",
    "    # # 2: \"left\",\n",
    "    # 7: \"right\",\n",
    "    # 34: \"left\",\n",
    "    \"168_3\": \"left\",\n",
    "    \"195_5\": \"top\",\n",
    "    \"265_2\": \"top\",\n",
    "    \"492_1\": \"bottom\",\n",
    "}\n",
    "\n",
    "indices = [k for k in cuts_dict]\n",
    "cuts = [cuts_dict[k] for k in cuts_dict]\n",
    "data_name = \"test\"\n",
    "labels = get_labels(data_name)\n",
    "\n",
    "def cut_img(img, cut):\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    if cut == \"top\":\n",
    "        img = img[-w:]\n",
    "    elif cut == \"bottom\":\n",
    "        img = img[:w]\n",
    "    elif cut == \"left\":\n",
    "        img = img[:, -h:]\n",
    "    elif cut == \"right\":\n",
    "        img = img[:, :h]\n",
    "    return img\n",
    "\n",
    "# Make plot to compare the different heatmap methods for fb and fb_db\n",
    "model_names = [\"fb_spacing4\", \"fb_db_spacing4\"]\n",
    "\n",
    "heatmap_types = [\"gradcam\", \"occlusion\", \"inclusion\", \"patch\"]\n",
    "heatmap_type_mapping = {\n",
    "    \"gradcam\": \"Grad-CAM\",\n",
    "    \"occlusion\": \"occlusion\",\n",
    "    \"inclusion\": \"inclusion\",\n",
    "    \"patch\": \"patch prediction\",\n",
    "}\n",
    "n_cols = len(heatmap_types) + 1 # +1 for the original image\n",
    "n_rows = len(indices)\n",
    "# Make separate plots for each model\n",
    "for model_name in model_names[:1]:\n",
    "    print(model_name)\n",
    "    results = load_results(model_name, data_name)\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "    for i, img_idx in enumerate(indices):\n",
    "        img_name = img_idx\n",
    "        img = load_img(img_name, data_name)\n",
    "        # Resize to be 0.25 of the original size\n",
    "        img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Make image square by cropping the longer side on the cuts[j] side\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        img = cut_img(img, cuts[i])\n",
    "\n",
    "        # Open mask and make contour\n",
    "        img_with_mask = img.copy()\n",
    "        mask = load_mask(img_name, data_name)\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_OE_mask = img.copy()\n",
    "        mask = load_mask(f\"{img_name}OE\", data_name)\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_OE_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "\n",
    "        show_img = img.copy()\n",
    "        # Place small text of the image index in the top left corner\n",
    "        # Align text top and left\n",
    "        ax[i, 0].imshow(show_img)\n",
    "        ax[i, 0].text(5, 5, ['a','b','c','d'][i], fontsize=20, color='black', verticalalignment='top', horizontalalignment='left')\n",
    "        ax[i, 0].axis('off')\n",
    "\n",
    "        label = P53_CLASS_CODES[labels[img_name]]\n",
    "        prediction = P53_CLASS_CODES[np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])]\n",
    "        title = f\"Label {label}\\nPrediction {prediction}\"\n",
    "        ax[i, 0].set_title(title, \n",
    "                        #    align text right\n",
    "                            loc='right',\n",
    "                           )\n",
    "\n",
    "        # Set two y ticks for the second column, one for overexpression and one for nullmutation\n",
    "        # Calculate the tick locations to match up with the grid of heatmaps\n",
    "        tick_locs = [y * 2*h for y in [0.25, 0.75]]\n",
    "        ax[i, 1].set_yticks(tick_locs)\n",
    "        ax[i, 1].set_yticklabels([\"OE heatmap\", \"NM heatmap\"], rotation=90, verticalalignment='center', horizontalalignment='right')\n",
    "        for j, heatmap_type in enumerate(heatmap_types):\n",
    "            heatmaps = model_heatmaps[data_name][model_name][img_name][heatmap_type] # (2, H, W) torch float32\n",
    "\n",
    "            if heatmap_type == \"occlusion\":\n",
    "                heatmaps = [(hm - 0.5)/2 + 0.5 for hm in heatmaps]\n",
    "\n",
    "            # Resize the heatmaps to the original image size\n",
    "            heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "            # Cut the heatmaps to the same size as the image\n",
    "            heatmaps = [cut_img(hm, cuts[i]) for hm in heatmaps]\n",
    "\n",
    "            # Make 1x2 grid of heatmaps with make_grid\n",
    "            img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                            torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "            img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "            heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "\n",
    "            ax[i, j+1].imshow(img_underlay)\n",
    "            ax[i, j+1].imshow(heatmap, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "            ax[i, j+1].set_title(heatmap_type_mapping[heatmap_type])\n",
    "            ax[i, j+1].set_xticks([])\n",
    "            if j > 0:\n",
    "                ax[i, j+1].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    # Set suptitle\n",
    "    # fig.suptitle(model_name_mapping[model_name], fontsize=16, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HEX color_dict to a list of RGB [0,1] values\n",
    "print(color_dict)\n",
    "color_dict_RGB = {k: tuple([int(h[i:i+2], 16)/255 for i in (1, 3, 5)]) for k, h in color_dict.items()}\n",
    "print(color_dict_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biopsy dims\n",
    "with open(os.path.join(BASE_DIR[\"test\"], \"biopsy_dims.json\"), \"r\") as f:\n",
    "    biopsy_dims = json.load(f)\n",
    "\n",
    "# Load non-empty patch indices\n",
    "non_empty_patch_indices = torch.load(os.path.join(BASE_DIR[\"test\"], \"non_empty_patch_indices_gs256.pt\"))\n",
    "\n",
    "# Make plot to compare the different heatmap methods for fb and fb_db\n",
    "# model_names = list(reversed([\"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\", \"fb_spacing4\", \"fb_db_spacing4\"]))\n",
    "model_names = [\"fb_spacing4\", \"fb_db_spacing4\", \"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\"]\n",
    "\n",
    "heatmap_type = \"inclusion\"\n",
    "n_cols = len(model_names) + 1 # +1 for the original image\n",
    "n_rows = len(indices)\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "for j, model_name in enumerate(model_names[:]):\n",
    "    print(model_name)\n",
    "    results = load_results(model_name, data_name)\n",
    "    if \"CLAM\" in model_name:\n",
    "        attention_maps = load_attention_maps(model_name, data_name)\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "    for i, img_idx in enumerate(indices):\n",
    "        img_name = img_idx\n",
    "        img = load_img(img_name, data_name)\n",
    "        # Resize to be 0.25 of the original size\n",
    "        img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Make image square by cropping the longer side on the cuts[j] side\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        img = cut_img(img, cuts[i])\n",
    "\n",
    "        # Open mask and make contour\n",
    "        img_with_mask = img.copy()\n",
    "        NMmask = load_mask(img_name, data_name)\n",
    "        if NMmask is not None:\n",
    "            mask = cv2.resize(NMmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_OE_mask = img.copy()\n",
    "        OEmask = load_mask(f\"{img_name}OE\", data_name)\n",
    "        if OEmask is not None:\n",
    "            mask = cv2.resize(OEmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_OE_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_both_masks = img.copy()\n",
    "        if NMmask is not None and OEmask is not None:\n",
    "            mask = cv2.resize(NMmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_both_masks, contours[0], -1, color_dict_RGB[\"b\"], 3)\n",
    "            mask = cv2.resize(OEmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_both_masks, contours[0], -1, color_dict_RGB[\"o\"], 3)\n",
    "\n",
    "        blanco = np.ones_like(img)\n",
    "\n",
    "        show_img = img.copy()\n",
    "        # Place small text of the image index in the top left corner\n",
    "        # Align text top and left\n",
    "        ax[i, 0].imshow(show_img)\n",
    "        ax[i, 0].text(5, 5, ['a','b','c','d'][i], fontsize=20, color='black', verticalalignment='top', horizontalalignment='left')\n",
    "        ax[i, 0].axis('off')\n",
    "        \n",
    "        label = P53_CLASS_CODES[labels[img_name]]\n",
    "        pred = np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])\n",
    "        prediction = P53_CLASS_CODES[pred]\n",
    "        title = f\"Label {label}\"\n",
    "        ax[i, 0].set_title(title, \n",
    "                        #    align text right\n",
    "                            loc='right',\n",
    "                           )\n",
    "\n",
    "        # Set two y ticks for the second column, one for overexpression and one for nullmutation\n",
    "        # Calculate the tick locations to match up with the grid of heatmaps\n",
    "        tick_locs = [y * 2*h for y in [0.25, 0.75]]\n",
    "        ax[i, -1].set_yticks(tick_locs)\n",
    "        ax[i, -1].set_yticklabels([\"OE heatmap\", \"NM heatmap\"], rotation=90, verticalalignment='center', horizontalalignment='left')\n",
    "        # Place these on the right side of the image\n",
    "        ax[i, -1].yaxis.tick_right()\n",
    "\n",
    "        if model_name in [\"fb_spacing4\", \"fb_db_spacing4\"]:\n",
    "            heatmaps = model_heatmaps[data_name][model_name][img_name][heatmap_type] # (2, H, W) torch float32\n",
    "        else:\n",
    "            heatmaps = vis_CLAM(None, img_name, None, attention_maps, non_empty_patch_indices, biopsy_dims)\n",
    "            # if len(heatmaps) != 2:\n",
    "            #     heatmaps = [heatmaps[pred]]\n",
    "\n",
    "        # Resize the heatmaps to the original image size\n",
    "        heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "        # Cut the heatmaps to the same size as the image\n",
    "        heatmaps = [cut_img(hm, cuts[i]) for hm in heatmaps]\n",
    "\n",
    "        if len(heatmaps) == 2:\n",
    "            # Make 1x2 grid of heatmaps with make_grid\n",
    "            img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                            torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "            img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "            heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "        elif len(heatmaps) == 1:\n",
    "            if prediction == \"OE\":\n",
    "                img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                                torch.tensor(blanco).permute(2, 0, 1)]\n",
    "                heatmaps = [heatmaps[0], np.zeros_like(heatmaps[0])]\n",
    "            elif prediction == \"NM\":\n",
    "                img_underlay = [torch.tensor(blanco).permute(2, 0, 1),\n",
    "                                torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "                heatmaps = [np.zeros_like(heatmaps[0]), heatmaps[0]]\n",
    "            elif prediction == \"DC\":\n",
    "                img_underlay = [torch.tensor(blanco).permute(2, 0, 1),\n",
    "                                torch.tensor(img_with_both_masks).permute(2, 0, 1),\n",
    "                                torch.tensor(blanco).permute(2, 0, 1)]\n",
    "                heatmaps = [np.zeros_like(heatmaps[0]), heatmaps[0], np.zeros_like(heatmaps[0])]\n",
    "            img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "            heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "\n",
    "            if prediction == \"DC\":\n",
    "                # Cut h//2 from the top and bottom\n",
    "                img_underlay = img_underlay[h//2:-h//2]\n",
    "                heatmap = heatmap[h//2:-h//2]\n",
    "\n",
    "        ax[i, j+1].imshow(img_underlay)\n",
    "        ax[i, j+1].imshow(heatmap, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "        title = f\"{model_name_mapping[model_name]}\\nPrediction {prediction}\"\n",
    "        ax[i, j+1].set_title(title)\n",
    "        ax[i, j+1].set_xticks([])\n",
    "        if j < 5:\n",
    "            ax[i, j+1].set_yticks([])\n",
    "plt.tight_layout()\n",
    "# Set suptitle\n",
    "# fig.suptitle(\"heatmap comparison between models\", fontsize=16, y=1.02)\n",
    "\n",
    "# Decrease the space between the subplots\n",
    "plt.subplots_adjust(hspace=-0.4, wspace=0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how large ResNet18s receptive field is for different image sizes\n",
    "from resnet import ResNetModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dummy input image\n",
    "img_size = 2048\n",
    "img = torch.zeros(1, 3, img_size, img_size)\n",
    "img[:, :, img_size//2, img_size//2] = 1\n",
    "img[:, :, img_size//4, img_size//4] = 1\n",
    "# img[:, :, img_size//4:3*img_size//4, img_size//4:3*img_size//4] = 1\n",
    "\n",
    "plt.imshow(img.squeeze().numpy().transpose(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "# Load the model\n",
    "model = models.resnet18()\n",
    "# Set all parameters to 1\n",
    "# for param in model.parameters():\n",
    "#     param.data.fill_(1)\n",
    "# Cut off before the pooling layer\n",
    "# model = nn.Sequential(*list(list(model.children())[0].children())[:-2])\n",
    "model = nn.Sequential(*list(model.children())[:-2])\n",
    "# display(model)\n",
    "model.eval()\n",
    "\n",
    "# Get the output of the model\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "\n",
    "# Plot the output\n",
    "output = output.squeeze().numpy().transpose(1, 2, 0).max(axis=2)\n",
    "# display(output)\n",
    "plt.imshow(output, cmap='gray', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
