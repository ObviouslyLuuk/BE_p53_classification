{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from collections import defaultdict\n",
    "\n",
    "from load_data import DATA_DIR, TRANSFORMS, P53_CLASS_NAMES, \\\n",
    "    convert_presence_probs_to_status_probs\n",
    "from resnet import ResNetModel, ResNetModelDoubleBinary\n",
    "# from resnet_patch import ResNetModelDoubleBinary as ResNetModelDoubleBinaryPatch\n",
    "from pl_clam import CLAM_MB, CLAM_db\n",
    "\n",
    "P53_CLASS_CODES = [\"WT\", \"OE\", \"NM\", \"DC\"]\n",
    "\n",
    "BOLERO_DIR = os.path.join(DATA_DIR, '..', 'BOLERO')\n",
    "# PATHXL_DIR = os.path.join(DATA_DIR, '..', 'p53_consensus_study')\n",
    "\n",
    "BASE_DIR = {\n",
    "    'test': DATA_DIR,\n",
    "    'bolero': BOLERO_DIR,\n",
    "    # 'pathxl': PATHXL_DIR\n",
    "}\n",
    "\n",
    "RESULTS_DIR = os.path.join(DATA_DIR, '..', '..', 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "VIS_DIR = os.path.join(DATA_DIR, '..', '..', 'visualizations')\n",
    "MODELS_DIR = os.path.join(DATA_DIR, '..', '..', 'models')\n",
    "\n",
    "bag_latent_paths = {\n",
    "    \"test\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "    \"bolero\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "    # \"pathxl\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "color_dict = {\n",
    "    \"r\": '#DA4C4C', # Red\n",
    "    \"o\": '#E57439', # Orange\n",
    "    \"y\": '#EDB732', # Yellow\n",
    "    \"g\": '#479A5F', # Green\n",
    "    \"lb\": '#5BC5DB', # Light blue\n",
    "    \"b\": '#5387DD', # Blue\n",
    "    \"p\": '#7D54B2', # Purple\n",
    "    \"pi\": '#E87B9F', # Pink\n",
    "#  '#229487', # Dark green/Turquoise\n",
    "#  '#C565C7', # Lilac\n",
    "    \"r_p\": '#E89393', # Pale red\n",
    "    \"o_p\": '#EFAB88', # Pale orange\n",
    "    \"y_p\": '#F4D384', # Pale yellow\n",
    "    \"g_p\": '#90C29F', # Pale green\n",
    "    \"lb_p\":'#9CDCE9', # Pale light blue\n",
    "    \"b_p\": '#98B7EA', # Pale blue\n",
    "    \"p_p\": \"#B198D0\", # Pale purple\n",
    "}\n",
    "colors = list(color_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb: full biopsy, db: double binary, gs: grid spacing\n",
    "model_kwargs = {\n",
    "    \"CLAM\":     {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db\":  {\"model_class\": CLAM_db, \"gs\": 256},\n",
    "    \"CLAM_m\":   {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db_m\":{\"model_class\": CLAM_db, \"gs\": 256},\n",
    "}\n",
    "for name in [\"fb_db\", \"fb\"]:\n",
    "    for spacing in [2, 4, \n",
    "                    # 8, 16, 32, 64, 128, 256\n",
    "                    ]:\n",
    "        if \"db\" not in name and spacing == 2: # Skipped this one because it's too slow\n",
    "            continue\n",
    "        model_name = f\"{name}_spacing{spacing}\"\n",
    "        model_kwargs[model_name] = {\"spacing\": spacing}\n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"fb_db\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModelDoubleBinary\n",
    "    elif \"fb\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModel\n",
    "    checkpoint_dir = os.path.join(MODELS_DIR, model_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model_kwargs[model_name][\"checkpoint_paths\"] = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n",
    "\n",
    "def load_model(model_class, checkpoint_path):\n",
    "    if \"CLAM\" in model_class.__name__: # For some reason pl can't load these models with load_from_checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        # model = model_class(**checkpoint[\"model_kwargs\"])\n",
    "        model = model_class()\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        model = model_class.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many checkpoints we have for each model\n",
    "for model_name in model_kwargs:\n",
    "    print(f\"{model_name}: {len(model_kwargs[model_name]['checkpoint_paths'])} checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_dict = {\n",
    "    \"CLAM\": 4,\n",
    "    \"CLAM_db\": 0,\n",
    "    \"CLAM_m\": 3,\n",
    "    \"CLAM_db_m\": 2,\n",
    "    \"fb_spacing4\": 4,\n",
    "    \"fb_db_spacing4\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_name, data_name):\n",
    "    \"\"\"\n",
    "    Returns (H, W, C) float32 tensor\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(BASE_DIR[data_name], 'biopsies', f\"{img_name}.png\")\n",
    "    img = plt.imread(img_path) # (H, W, C) float32\n",
    "    return img\n",
    "    \n",
    "def load_patch_latents(img_name, data_name, bag_latents):\n",
    "    \"\"\"\n",
    "    Returns (N, 2048) tensor\n",
    "    \"\"\"\n",
    "    if data_name == \"test\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "    elif data_name == \"bolero\":\n",
    "        slide_name, biopsy_name = tuple(img_name.split(\"_\"))\n",
    "        slide_latents = bag_latents[int(slide_name)] # (n_biopsies, N, 2048)\n",
    "        return slide_latents[int(biopsy_name)+1] # (N, 2048)\n",
    "    elif data_name == \"pathxl\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "    \n",
    "def get_img_const_spacing(img_name, data_name, spacing):\n",
    "    \"\"\"\n",
    "    Returns (1, C, H, W) tensor with constant spacing (spacing x spacing pixels per grid cell)\n",
    "    \"\"\"\n",
    "    img = load_img(img_name, data_name)\n",
    "    h = img.shape[0] // spacing\n",
    "    w = img.shape[1] // spacing\n",
    "    img = torch.nn.functional.interpolate(torch.tensor(img).permute(2, 0, 1).unsqueeze(0), size=(h, w), mode='bilinear')\n",
    "    img = TRANSFORMS['normalize'](img)\n",
    "    return img\n",
    "\n",
    "def call_constant_spacing(model, img_name, data_name, spacing, **kwargs):\n",
    "    img = get_img_const_spacing(img_name, data_name, spacing)\n",
    "    with torch.no_grad():\n",
    "        return model(img.to(device)).cpu().detach().numpy()\n",
    "\n",
    "def call_CLAM(model, img_name, data_name, bag_latents, **kwargs):\n",
    "    patch_latents = load_patch_latents(img_name, data_name, bag_latents).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits, Y_prob, Y_hat, A_raw, results_dict = model(patch_latents.to(device))\n",
    "        return Y_prob.cpu().detach().numpy(), A_raw.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"spacing\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"call\"] = call_constant_spacing\n",
    "    elif \"CLAM\" in model_name:\n",
    "        model_kwargs[model_name][\"call\"] = call_CLAM\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown call type for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusion(model, img, patch_size=128, step_size=64, mode=\"inclusion\", print_progress=True):\n",
    "    # Make sure the image is divisible by the patch size\n",
    "    h, w = img.shape[-2], img.shape[-1]\n",
    "    h_padded, w_padded = int(np.ceil(h/patch_size))*patch_size, int(np.ceil(w/patch_size))*patch_size\n",
    "    pad_h, pad_w = h_padded - h, w_padded - w\n",
    "\n",
    "    pixels = img.squeeze().cpu()\n",
    "    img = torch.nn.functional.pad(img, (pad_w, 0, pad_h, 0)) # the order is left, right, top, bottom\n",
    "    h_padded, w_padded = img.shape[-2], img.shape[-1]\n",
    "\n",
    "    if mode == \"inclusion\":\n",
    "        # dummy_img = torch.zeros_like(img).squeeze()\n",
    "        dummy_img = torch.zeros(3, 1024, 1024)\n",
    "        # # Fill with mean color of the image\n",
    "        # dummy_img[0] = pixels[0].mean()\n",
    "        # dummy_img[1] = pixels[1].mean()\n",
    "        # dummy_img[2] = pixels[2].mean()\n",
    "\n",
    "        # Place patches in the middle of the dummy image\n",
    "        patches_changed = dummy_img.unsqueeze(0).clone() # Shape: (1, 3, img_h, img_w)\n",
    "        h_steps = h_padded // step_size\n",
    "        w_steps = w_padded // step_size\n",
    "        patches_changed = patches_changed.repeat(h_steps, w_steps, 1, 1, 1)\n",
    "        for i in range(0, h_padded, step_size):\n",
    "            for j in range(0, w_padded, step_size):\n",
    "                # Place patch in the middle of patches_changed\n",
    "                h_middle = dummy_img.shape[-2] // 2 - patch_size // 2\n",
    "                w_middle = dummy_img.shape[-1] // 2 - patch_size // 2\n",
    "                patch = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "                ph, pw = patch.shape[-2], patch.shape[-1]\n",
    "                patches_changed[i // step_size, j // step_size, :, h_middle:h_middle+ph, w_middle:w_middle+pw] = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "        # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "        patches_changed = patches_changed.view(-1, 3, dummy_img.shape[-2], dummy_img.shape[-1])\n",
    "    elif mode == \"patch\":\n",
    "        # Just have patches by themselves\n",
    "        h_steps = h_padded // step_size\n",
    "        w_steps = w_padded // step_size\n",
    "        patches_changed = torch.zeros(h_steps, w_steps, 3, patch_size, patch_size)\n",
    "        for i in range(0, h_padded, step_size):\n",
    "            for j in range(0, w_padded, step_size):\n",
    "                # Place patch in the middle of patches_changed\n",
    "                patch = img[:, :, i:i+patch_size, j:j+patch_size]\n",
    "                ph, pw = patch.shape[-2], patch.shape[-1]\n",
    "                h_middle = patch_size // 2 - ph // 2\n",
    "                w_middle = patch_size // 2 - pw // 2\n",
    "                patches_changed[i // step_size, j // step_size, :, h_middle:h_middle+ph, w_middle:w_middle+pw] = patch\n",
    "        # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "        patches_changed = patches_changed.view(-1, 3, patch_size, patch_size)\n",
    "    elif mode == \"occlusion\":\n",
    "        with torch.no_grad():\n",
    "            full_prediction = model(img.to(device)).cpu().detach().squeeze()\n",
    "            if len(full_prediction) == 4:\n",
    "                full_prediction = torch.nn.functional.softmax(full_prediction, dim=0)\n",
    "                full_prediction = full_prediction[[1,2]]\n",
    "        # Copy the image to the dummy image except for the patch\n",
    "        h_steps = h_padded // step_size\n",
    "        w_steps = w_padded // step_size\n",
    "        patches_changed = img.repeat(h_steps, w_steps, 1, 1, 1)\n",
    "        for i in range(0, h_padded, step_size):\n",
    "            for j in range(0, w_padded, step_size):\n",
    "                patches_changed[i // step_size, j // step_size, :, i:i+patch_size, j:j+patch_size] = 0\n",
    "        # Reshape the tensor to (B, 3, img_size, img_size)\n",
    "        patches_changed = patches_changed.view(-1, 3, h_padded, w_padded)\n",
    "\n",
    "    # Get the model output for each image\n",
    "    diff = torch.zeros(patches_changed.shape[0], 2)\n",
    "    iterator = range(patches_changed.shape[0])\n",
    "    if print_progress:\n",
    "        iterator = tqdm(iterator)\n",
    "    for i in iterator: # necessary for CUDA memory\n",
    "        current_img = patches_changed[i].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(current_img).cpu().detach().squeeze()\n",
    "            if len(preds) == 4:\n",
    "                preds = torch.nn.functional.softmax(preds, dim=0)\n",
    "                preds = preds[[1,2]]\n",
    "            diff[i] = preds\n",
    "    if mode != \"occlusion\":\n",
    "        # Threshold the difference at the minimum value\n",
    "        diff[:, 0] = (diff[:, 0] - diff[:, 0].min())\n",
    "        diff[:, 1] = (diff[:, 1] - diff[:, 1].min())\n",
    "    else:\n",
    "        # # Subtract the full prediction\n",
    "        diff[:, 0] = full_prediction[0] - diff[:, 0]\n",
    "        diff[:, 1] = full_prediction[1] - diff[:, 1]\n",
    "        # Center at 0.5\n",
    "        diff[:, 0] = diff[:, 0]*2 + 0.5\n",
    "        diff[:, 1] = diff[:, 1]*2 + 0.5\n",
    "\n",
    "    # Make heatmap as grid of patch outputs\n",
    "    overexpression_heatmap = np.zeros((h_steps, w_steps))\n",
    "    nullmutation_heatmap = np.zeros((h_steps, w_steps))\n",
    "\n",
    "    # Make a mask to keep track of the number of patches that overlap in each pixel\n",
    "    mask = np.zeros((h_steps, w_steps))\n",
    "\n",
    "    steps_per_patch = patch_size // step_size\n",
    "    for i in range(diff.shape[0]):\n",
    "        row = i // w_steps\n",
    "        col = i % w_steps\n",
    "        overexpression_heatmap[row:row+steps_per_patch, col:col+steps_per_patch] += diff[i, 0].item()\n",
    "        nullmutation_heatmap  [row:row+steps_per_patch, col:col+steps_per_patch] += diff[i, 1].item()\n",
    "        mask                  [row:row+steps_per_patch, col:col+steps_per_patch] += 1\n",
    "\n",
    "    # Crop to the original image size\n",
    "    pad_steps_h = pad_h // step_size\n",
    "    pad_steps_w = pad_w // step_size\n",
    "    overexpression_heatmap = overexpression_heatmap[pad_steps_h:, pad_steps_w:]\n",
    "    nullmutation_heatmap = nullmutation_heatmap[pad_steps_h:, pad_steps_w:]\n",
    "    mask = mask[pad_steps_h:, pad_steps_w:]\n",
    "\n",
    "    # Normalize the img by dividing by the mask\n",
    "    overexpression_heatmap /= mask\n",
    "    nullmutation_heatmap /= mask\n",
    "\n",
    "    return overexpression_heatmap, nullmutation_heatmap\n",
    "\n",
    "def vis_constant_spacing(model, img_name, data_name, spacing, **kwargs):\n",
    "    img = get_img_const_spacing(img_name, data_name, spacing) # (1, 3, H, W)\n",
    "    return list(occlusion(model, img, **kwargs))\n",
    "\n",
    "\n",
    "def load_attention_maps(model_name, data_name):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        results = defaultdict(lambda: defaultdict(dict))\n",
    "        for data_name in [\"test\", \"pathxl\"]:\n",
    "            results_data = load_attention_maps(model_name, data_name)\n",
    "            for key in results_data:\n",
    "                results[key].update(results_data[key])\n",
    "        return results\n",
    "\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    checkpoint_name = [f for f in os.listdir(results_dir) if f.startswith(data_name)][best_checkpoint_idx]\n",
    "    results_path = os.path.join(results_dir, checkpoint_name)\n",
    "    result_content = torch.load(results_path)\n",
    "    if \"CLAM_db\" in model_name:\n",
    "        for img_name, (presence_probs, A_raw) in result_content.items():\n",
    "            results[\"presence_probs\"][img_name] = presence_probs\n",
    "            results[\"status_probs\"][img_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "            results[\"A_raw\"][img_name] = A_raw\n",
    "    elif \"CLAM\" in model_name:\n",
    "        for img_name, (status_probs, A_raw) in result_content.items():\n",
    "            results[\"status_probs\"][img_name] = status_probs\n",
    "            results[\"A_raw\"][img_name] = A_raw\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    return results\n",
    "\n",
    "def vis_CLAM(model, img_name, data_name, attention_maps, non_empty_patch_indices, biopsy_dims):\n",
    "    prediction = np.round(attention_maps[\"status_probs\"][img_name],2)\n",
    "    pred_argmax = np.argmax(prediction)\n",
    "    print(\"Prediction:\", prediction)\n",
    "    # Rank the class predictions, like prediction=[0.1, 0.6, 0.2, 0.1] -> [1, 2, 0, 3]\n",
    "    pred_rank = torch.argsort(-torch.tensor(prediction))[0]\n",
    "    multipliers = [1,1,1,1]\n",
    "    for i in range(len(pred_rank)):\n",
    "        branch = pred_rank[i]\n",
    "        multipliers[branch] = 1/(i+1)\n",
    "    multipliers = multipliers[1:] # Skip the WT class\n",
    "\n",
    "    attention_map = attention_maps[\"A_raw\"][img_name] # (K, N)\n",
    "    K = attention_map.shape[0]\n",
    "    non_empty_indices = non_empty_patch_indices[img_name] # (N,)\n",
    "    img_h, img_w = biopsy_dims[str(img_name)]\n",
    "\n",
    "    patch_size = 256\n",
    "    patch_rows = max(round(img_h / patch_size), 1)\n",
    "    patch_cols = max(round(img_w / patch_size), 1)\n",
    "    # The patch attention is a 1D array and corresponds to the non-empty indices\n",
    "    all_patch_attention = torch.zeros(2, patch_rows * patch_cols)\n",
    "    if K == 4:\n",
    "        # for i in range(3):\n",
    "        #     all_patch_attention[i].fill_(attention_map[1:][i].min())\n",
    "        # all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)[1:] # Skip the WT class\n",
    "\n",
    "        all_patch_attention[0].fill_(attention_map[pred_argmax].min())\n",
    "        all_patch_attention[0, non_empty_indices] = torch.tensor(attention_map)[pred_argmax]\n",
    "        all_patch_attention = all_patch_attention[:1]\n",
    "\n",
    "        # all_patch_attention.fill_(attention_map.min())\n",
    "        # all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)[1:3]\n",
    "\n",
    "        # all_patch_attention = torch.zeros(4, patch_rows * patch_cols)\n",
    "        # all_patch_attention.fill_(attention_map.min())\n",
    "        # all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)\n",
    "    elif K == 2:\n",
    "        all_patch_attention.fill_(attention_map.min())\n",
    "        all_patch_attention[:, non_empty_indices] = torch.tensor(attention_map)\n",
    "        # Normalize the attention values to [0, 1]\n",
    "        # all_patch_attention = (all_patch_attention - all_patch_attention.min()) / (all_patch_attention.max() - all_patch_attention.min())\n",
    "        # Convert attention scores to percentiles\n",
    "        # all_patch_attention[pred_argmax] = torch.tensor(rankdata(all_patch_attention[pred_argmax], method='ordinal') / len(all_patch_attention[pred_argmax]))\n",
    "\n",
    "    for i in range(len(all_patch_attention)):\n",
    "        # all_patch_attention[i] = (all_patch_attention[i] - all_patch_attention[i].min()) / (all_patch_attention[i].max() - all_patch_attention[i].min()) * multipliers[i]\n",
    "        all_patch_attention[i] = (all_patch_attention[i] - all_patch_attention[i].min()) / (all_patch_attention.max() - all_patch_attention[i].min())\n",
    "\n",
    "    heatmaps = [a.numpy() for a in all_patch_attention.reshape(-1, patch_rows, patch_cols)]\n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def load_mask(img_name, data_name):\n",
    "    mask_path = os.path.join(BASE_DIR[data_name], 'masks', f\"{img_name}.png\")\n",
    "    if not os.path.exists(mask_path):\n",
    "        return None\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return mask\n",
    "\n",
    "def plot_heatmap(heatmaps, img_name, data_name, downsample=0.25, in_one=False):\n",
    "    img = load_img(img_name, data_name) # (H, W, C) float32\n",
    "    img = cv2.resize(img, (0,0), fx=downsample, fy=downsample)  \n",
    "    for i, heatmap in enumerate(heatmaps):\n",
    "        heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0])) # For upsampling, this uses linear interpolation\n",
    "        heatmaps[i] = heatmap\n",
    "    mask = load_mask(img_name, data_name)\n",
    "    if mask is not None:\n",
    "        mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "    heatmap_colors = ['Oranges', 'Blues'] if len(heatmaps) == 2 else ['Oranges', 'Blues', 'Purples']\n",
    "    if not in_one:\n",
    "        # Plot the img and then the heatmaps overlayed one by one to the side\n",
    "        fig, ax = plt.subplots(1, len(heatmaps)+1, figsize=(20, 10))\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].axis('off')\n",
    "        masks = [None, mask] if len(heatmaps) == 2 else [None, None, mask, mask]\n",
    "        for i, heatmap in enumerate(heatmaps):\n",
    "            show_img = img.copy()\n",
    "            if masks[i] is not None:\n",
    "                contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                cv2.drawContours(show_img, contours[0], -1, (0, 0, 0), 2)\n",
    "            ax[i+1].imshow(show_img)\n",
    "            ax[i+1].imshow(heatmap, alpha=0.3, cmap='jet', vmin=0, vmax=1)\n",
    "            ax[i+1].axis('off')\n",
    "    else:\n",
    "        # Plot the img and then the heatmaps overlayed on top of each other\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        # Add the mask\n",
    "        if mask is not None:\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img, contours[0], -1, (0, 0, 0), 2)\n",
    "            ax.imshow(img, alpha=0.3)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        for i, h in enumerate(heatmaps):\n",
    "            heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "            ax.imshow(h, alpha=0.3, cmap=heatmap_colors[i], vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"spacing\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"vis\"] = vis_constant_spacing\n",
    "    elif \"CLAM\" in model_name:\n",
    "        model_kwargs[model_name][\"vis\"] = vis_CLAM\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown call type for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_name = \"fb_db_spacing4\"\n",
    "model = load_model(model_kwargs[model_name][\"model_class\"], \n",
    "                   model_kwargs[model_name][\"checkpoint_paths\"][best_checkpoint_dict[model_name]])\n",
    "\n",
    "# img_name = 34\n",
    "img_name = 1133\n",
    "data_name = \"test\"\n",
    "spacing = 4\n",
    "# heatmaps = vis_constant_spacing(model, img_name, data_name, spacing, patch_size=128, step_size=64, \n",
    "#                      mode=\"inclusion\")\n",
    "# plot_heatmap(heatmaps, img_name, data_name, downsample=0.25, in_one=False)\n",
    "# heatmaps = vis_constant_spacing(model, img_name, data_name, spacing, patch_size=128, step_size=64, \n",
    "#                      mode=\"patch\")\n",
    "# plot_heatmap(heatmaps, img_name, data_name, downsample=0.25, in_one=False)\n",
    "\n",
    "heatmaps = {}\n",
    "# for img_name in [34, 1133, 192, 1416, 7]:\n",
    "for img_name in [\"0_0\", \"5_0\"]:\n",
    "    heatmaps[img_name] = vis_constant_spacing(model, img_name, data_name, spacing, patch_size=128, step_size=64, \n",
    "                        mode=\"occlusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name, hms in heatmaps.items():\n",
    "    oe_heatmap, nm_heatmap = tuple(hms)\n",
    "    new_heatmaps = [oe_heatmap.copy(), nm_heatmap.copy()]\n",
    "    for i, heatmap in enumerate(new_heatmaps):\n",
    "        print(\"min, max\")\n",
    "        print(heatmap.min(), heatmap.max())\n",
    "        # median = np.median(heatmap)\n",
    "        # heatmap = heatmap - median\n",
    "        # max_val = heatmap.max()\n",
    "        # if max_val != 0:\n",
    "        #     heatmap = heatmap / np.abs(heatmap).max() * 0.5\n",
    "\n",
    "        # print(heatmap.min(), heatmap.max())\n",
    "        # heatmap *= 2\n",
    "        # heatmap = heatmap + 0.5\n",
    "        new_heatmaps[i] = heatmap\n",
    "    # display(heatmaps)\n",
    "    plot_heatmap(new_heatmaps, img_name, data_name, downsample=0.25, in_one=False)\n",
    "    # vis_constant_spacing(model, img_name, data_name, spacing, patch_size=512, step_size=256, \n",
    "    #                      mode=\"occlusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "non_empty_patch_indices = torch.load(os.path.join(BASE_DIR[data_name], \"non_empty_patch_indices_gs256.pt\"))\n",
    "with open(os.path.join(BASE_DIR[data_name], \"biopsy_dims.json\"), \"r\") as f:\n",
    "    biopsy_dims = json.load(f)\n",
    "\n",
    "for model_name in [\"CLAM\", \"CLAM_m\", \"CLAM_db\", \"CLAM_db_m\"]:\n",
    "    print(model_name)\n",
    "    attention_maps = load_attention_maps(model_name, data_name)\n",
    "    heatmaps = vis_CLAM(None, img_name, None, attention_maps, non_empty_patch_indices, biopsy_dims)\n",
    "\n",
    "    plot_heatmap(heatmaps, img_name, data_name, downsample=0.25, in_one=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLAM_m model and call it on the image\n",
    "model_name = \"CLAM_m\"\n",
    "model = load_model(model_kwargs[model_name][\"model_class\"], \n",
    "                   model_kwargs[model_name][\"checkpoint_paths\"][best_checkpoint_dict[model_name]])\n",
    "img_name = 1133\n",
    "data_name = \"test\"\n",
    "bag_latents = torch.load(bag_latent_paths[data_name])\n",
    "Y_prob, A_raw = call_CLAM(model, img_name, data_name, bag_latents)\n",
    "heatmaps = vis_CLAM(None, img_name, None, {\"status_probs\":{img_name:Y_prob}, \"A_raw\":{img_name:A_raw}}, non_empty_patch_indices, biopsy_dims)\n",
    "\n",
    "plot_heatmap(heatmaps, img_name, data_name, downsample=0.25, in_one=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_conv_layer(model):\n",
    "    for name, layer in reversed(list(model.named_modules())):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            return name, layer\n",
    "    raise ValueError(\"No convolutional layer found in the model\")\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer, target_layer_name):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.model.eval()\n",
    "        self.feature_grad = None\n",
    "        self.feature_map = None\n",
    "        self.hook_feature_map()\n",
    "        self.hook_feature_grad()\n",
    "\n",
    "    def hook_feature_map(self):\n",
    "        def hook_fn(module, input, output):\n",
    "            self.feature_map = output\n",
    "        self.target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    def hook_feature_grad(self):\n",
    "        def hook_fn(module, grad_input, grad_output):\n",
    "            self.feature_grad = grad_output[0]\n",
    "        self.target_layer.register_backward_hook(hook_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, output, target_class):\n",
    "        self.model.zero_grad()\n",
    "        one_hot_output = torch.zeros((1, output.size()[-1]), dtype=torch.float32, device=output.device)\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        cams = []\n",
    "        output = self.forward(x)\n",
    "        n_outputs = output.size()[-1]\n",
    "        for target_class in range(n_outputs):\n",
    "            self.backward(output, target_class)\n",
    "            if self.feature_grad is None or self.feature_map is None:\n",
    "                raise ValueError(\"Feature gradients or feature maps are not set. Check hooks.\")\n",
    "            \n",
    "            weights = torch.mean(self.feature_grad, dim=(2, 3)).squeeze()\n",
    "            cam = torch.tensordot(weights, self.feature_map.squeeze(0), dims=([0], [0]))\n",
    "            # cam = torch.nn.functional.relu(cam)\n",
    "            \n",
    "            # Avoid NaNs in normalization\n",
    "            if torch.isnan(cam).any():\n",
    "                raise ValueError(\"CAM contains NaN values before normalization.\")\n",
    "            cam = cam.detach().cpu().numpy()\n",
    "\n",
    "            cam_min, cam_max = cam.min(), cam.max()\n",
    "            if cam_max == cam_min:\n",
    "                # print(\"CAM has uniform values. Check model and target class.\")\n",
    "                pass\n",
    "            else:\n",
    "                cam /= (cam_max - cam_min)\n",
    "            cam += 0.5\n",
    "            cams.append(cam)\n",
    "        return output, cams\n",
    "\n",
    "def get_grad_cam(model, img_name, data_name):\n",
    "    img = get_img_const_spacing(img_name, data_name, 4)\n",
    "    last_conv_layer_name, last_conv_layer = get_last_conv_layer(model)\n",
    "    grad_cam = GradCam(model, last_conv_layer, last_conv_layer_name)\n",
    "    output, cams = grad_cam(img.to(device))\n",
    "    cams = cams if len(cams) == 2 else cams[1:3]\n",
    "    return output, cams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fb_spacing4\"\n",
    "model = load_model(model_kwargs[model_name][\"model_class\"], \n",
    "                   model_kwargs[model_name][\"checkpoint_paths\"][best_checkpoint_dict[model_name]])\n",
    "\n",
    "display(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_names(data_name='test'):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        return get_img_names(\"test\") + get_img_names(\"pathxl\")\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv'))[\"id\"].tolist()\n",
    "    return [img_path.split('.')[0] for img_path in os.listdir(os.path.join(BASE_DIR[data_name], 'biopsies'))]\n",
    "\n",
    "def get_results(model, mdl_kwargs, data_name='test', bag_latents=None):\n",
    "    img_names = get_img_names(data_name)\n",
    "    results = {}\n",
    "    for img_name in tqdm(img_names):\n",
    "        results[img_name] = mdl_kwargs[\"call\"](model, img_name, data_name, bag_latents=bag_latents, **mdl_kwargs)\n",
    "    return results\n",
    "\n",
    "def save_results(results, model_name, data_name, checkpoint_name):\n",
    "    results_dir = os.path.join(VIS_DIR, model_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "    torch.save(results, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run visualizations for fb models\n",
    "# model_heatmaps = { # data_name: {model_name: {img_name: {heatmap_type: heatmaps}}}\n",
    "#     \"test\": {\n",
    "#         \"fb_spacing4\": defaultdict(dict),\n",
    "#         \"fb_db_spacing4\": defaultdict(dict),\n",
    "#     },\n",
    "# }\n",
    "vis_save_path = os.path.join(VIS_DIR, \"fb_models.pt\")\n",
    "model_heatmaps = torch.load(vis_save_path)\n",
    "\n",
    "data_name = \"test\"\n",
    "for model_name in [\"fb_spacing4\", \"fb_db_spacing4\"]:\n",
    "    print(model_name)\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    model = load_model(model_kwargs[model_name][\"model_class\"], model_kwargs[model_name][\"checkpoint_paths\"][best_checkpoint_idx])\n",
    "    img_names = get_img_names(data_name)\n",
    "    for img_name in tqdm(img_names):\n",
    "        for heatmap_type in [\"inclusion\", \"patch\", \"occlusion\", \"gradcam\"]:\n",
    "            # Skip if we already have the results\n",
    "            if img_name in model_heatmaps[data_name][model_name] and heatmap_type in model_heatmaps[data_name][model_name][img_name]:\n",
    "                continue\n",
    "\n",
    "            if heatmap_type == \"gradcam\":\n",
    "                output, heatmaps = get_grad_cam(model, img_name, data_name)\n",
    "            else:\n",
    "                heatmaps = vis_constant_spacing(model, img_name, data_name, model_kwargs[model_name][\"spacing\"], \n",
    "                                                        patch_size=128, step_size=64, mode=heatmap_type, print_progress=False)\n",
    "            model_heatmaps[data_name][model_name][img_name][heatmap_type] = heatmaps\n",
    "\n",
    "vis_save_path = os.path.join(VIS_DIR, \"fb_models.pt\")\n",
    "torch.save(model_heatmaps, vis_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Metrics\n",
    "Load results and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fb_db   has a dict like idx: shape (1,2) with the two mutation probabilities\n",
    "fb      has a dict like idx: shape (1,4) with the four class probabilities\n",
    "CLAM_db has a dict like idx: tuple of: (\n",
    "            shape (1,2) with the two mutation probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "    )\n",
    "CLAM    has a dict like idx: tuple of: (\n",
    "            shape (1,4) with the four class probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "        )\n",
    "\"\"\"\n",
    "model_type_result_keys = {\n",
    "    \"fb_db\": [\"presence_probs\"],\n",
    "    \"fb\": [\"status_probs\"],\n",
    "    \"CLAM_db\": [\"presence_probs\", \"A_raw\"],\n",
    "    \"CLAM\": [\"status_probs\", \"A_raw\"],\n",
    "}\n",
    "def get_result_keys(model_name):\n",
    "    for key in model_type_result_keys: # The order is important\n",
    "        if key in model_name:\n",
    "            return model_type_result_keys[key]\n",
    "\n",
    "def load_results(model_name, data_name):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        results = defaultdict(lambda: defaultdict(dict))\n",
    "        for data_name in [\"test\", \"pathxl\"]:\n",
    "            results_data = load_results(model_name, data_name)\n",
    "            for key in results_data:\n",
    "                results[key].update(results_data[key])\n",
    "        return results\n",
    "\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    for i, checkpoint_name in enumerate([f for f in os.listdir(results_dir) if f.startswith(data_name)]):\n",
    "        results_path = os.path.join(results_dir, checkpoint_name)\n",
    "        result_content = torch.load(results_path)\n",
    "        checkpoint_name = checkpoint_name.replace(f\"{data_name}_\", \"\").replace(\".pt\", \"\")\n",
    "        if \"CLAM_db\" in model_name:\n",
    "            for img_name, (presence_probs, A_raw) in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"CLAM\" in model_name:\n",
    "            for img_name, (status_probs, A_raw) in result_content.items():\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"fb_db\" in model_name:\n",
    "            for img_name, presence_probs in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "        elif \"fb\" in model_name:\n",
    "            for img_name, status_probs in result_content.items():\n",
    "                # results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = torch.nn.functional.softmax(torch.tensor(status_probs), dim=1).numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    return results\n",
    "\n",
    "def get_labels(data_name):\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv')).set_index(\"id\").to_dict(orient='dict')['label']\n",
    "    elif data_name == \"bolero\":\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[data_name], 'P53_BOLERO_T.csv'))\n",
    "        labels = labels.sort_values(by=\"Case ID\")\n",
    "        labels = labels.reset_index(drop=True)\n",
    "        # Map GS to {1:0, 2:1, 3:2, 4:4} where 4 is unknown\n",
    "        labels[\"GS\"] = labels[\"GS\"].map({1:0, 2:1, 3:2, 4:4})\n",
    "        # Only keep GS column\n",
    "        labels = labels[[\"GS\"]].to_dict(orient='dict')[\"GS\"]\n",
    "        return labels\n",
    "    elif data_name.startswith(\"pathxl\"):\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[\"pathxl\"], 'labels.csv'))\n",
    "        # idx is id column and biopsy_nr column separated by _\n",
    "        labels[\"idx\"] = labels[\"id\"].astype(str) + \"_\" + labels[\"biopsy_nr\"].astype(str)\n",
    "        labels = labels.set_index(\"idx\")\n",
    "        # Sort by id primarily and biopsy_nr secondarily\n",
    "        labels = labels.sort_values(by=[\"id\", \"biopsy_nr\"])\n",
    "        # Map label\n",
    "        mapping = {\"WT\":0, \"Overexpression\":1, \"Null\":2, \"Double clones\":3}\n",
    "        labels[\"label\"] = labels[\"label\"].map(mapping)\n",
    "        if data_name == \"pathxl\": # Filter out any concordance % < 75\n",
    "            labels = labels[labels[\"concordance %\"] >= 75]\n",
    "        elif data_name == \"pathxl-100\":\n",
    "            labels = labels[labels[\"concordance %\"] == 100]\n",
    "        labels = labels[[\"label\"]].to_dict(orient='dict')[\"label\"]\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl\"))\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl-100\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl-100\"))\n",
    "        return labels\n",
    "    \n",
    "# load_results(\"fb_spacing4\", \"test\")[\"status_probs\"]\n",
    "# get_labels(\"test+pathxl-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Biopsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load biopsy dims\n",
    "with open(os.path.join(BASE_DIR[\"test\"], \"biopsy_dims.json\"), \"r\") as f:\n",
    "    biopsy_dims = json.load(f)\n",
    "\n",
    "img_names = get_img_names(\"test\")\n",
    "print(img_names[:5])\n",
    "\n",
    "# Get largest biopsies in test set\n",
    "biopsy_dims = {str(k): biopsy_dims[str(k)] for k in img_names}\n",
    "sorted_biopsies = sorted(biopsy_dims.items(), key=lambda x: min(x[1]), reverse=True)\n",
    "sorted_biopsies[:10]\n",
    "display(sorted_biopsies[:10])\n",
    "\n",
    "# Show little thumbnails of these biopsies downsampled by 8x\n",
    "n = 4\n",
    "fig, ax = plt.subplots(n, 5, figsize=(20, 5*n))\n",
    "labels = get_labels(\"test\")\n",
    "for i, (img_name, dims) in enumerate(sorted_biopsies[:n*5]):\n",
    "    img = get_img_const_spacing(img_name, \"test\", 8)\n",
    "    ax[i//5, i%5].imshow(img.squeeze().permute(1, 2, 0))\n",
    "    ax[i//5, i%5].axis('off')\n",
    "    label = labels[img_name]\n",
    "    ax[i//5, i%5].set_title(img_name + f\"\\n({label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_mapping = {\n",
    "    \"fb_spacing4\": \"Full-Biopsy Multiclass\",\n",
    "    \"fb_db_spacing4\": \"Full-Biopsy Double-Binary\",\n",
    "    \"CLAM\": \"CLAM\",\n",
    "    \"CLAM_db\": \"CLAM Double-Binary\",\n",
    "    \"CLAM_m\": \"CLAM +DC\",\n",
    "    \"CLAM_db_m\": \"CLAM Double-Binary +DC\",\n",
    "}\n",
    "model_name_mapping = {\n",
    "    \"fb_spacing4\": \"FB\",\n",
    "    \"fb_db_spacing4\": \"FBdb\",\n",
    "    \"CLAM\": \"CLAM\",\n",
    "    \"CLAM_db\": \"CLAMdb\",\n",
    "    \"CLAM_m\": \"CLAM+DC\",\n",
    "    \"CLAM_db_m\": \"CLAMdb+DC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_dict = {\n",
    "    # # 1416: \"right\",\n",
    "    # 1133: \"top\",\n",
    "    # # 240: \"bottom\",\n",
    "    # 192: \"right\",\n",
    "    # # 2: \"left\",\n",
    "    # 7: \"right\",\n",
    "    # 34: \"left\",\n",
    "    \"168_3\": \"left\",\n",
    "    \"195_5\": \"top\",\n",
    "    \"265_2\": \"top\",\n",
    "    \"492_1\": \"bottom\",\n",
    "}\n",
    "\n",
    "indices = [k for k in cuts_dict]\n",
    "cuts = [cuts_dict[k] for k in cuts_dict]\n",
    "data_name = \"test\"\n",
    "labels = get_labels(data_name)\n",
    "\n",
    "def cut_img(img, cut):\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    if cut == \"top\":\n",
    "        img = img[-w:]\n",
    "    elif cut == \"bottom\":\n",
    "        img = img[:w]\n",
    "    elif cut == \"left\":\n",
    "        img = img[:, -h:]\n",
    "    elif cut == \"right\":\n",
    "        img = img[:, :h]\n",
    "    return img\n",
    "\n",
    "# Make plot to compare the different heatmap methods for fb and fb_db\n",
    "model_names = [\"fb_spacing4\", \"fb_db_spacing4\"]\n",
    "\n",
    "heatmap_types = [\"gradcam\", \"occlusion\", \"inclusion\", \"patch\"]\n",
    "heatmap_type_mapping = {\n",
    "    \"gradcam\": \"Grad-CAM\",\n",
    "    \"occlusion\": \"occlusion\",\n",
    "    \"inclusion\": \"inclusion\",\n",
    "    \"patch\": \"patch prediction\",\n",
    "}\n",
    "n_cols = len(heatmap_types) + 1 # +1 for the original image\n",
    "n_rows = len(indices)\n",
    "# Make separate plots for each model\n",
    "for model_name in model_names[:1]:\n",
    "    print(model_name)\n",
    "    results = load_results(model_name, data_name)\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "    for i, img_idx in enumerate(indices):\n",
    "        img_name = img_idx\n",
    "        img = load_img(img_name, data_name)\n",
    "        # Resize to be 0.25 of the original size\n",
    "        img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Make image square by cropping the longer side on the cuts[j] side\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        img = cut_img(img, cuts[i])\n",
    "\n",
    "        # Open mask and make contour\n",
    "        img_with_mask = img.copy()\n",
    "        mask = load_mask(img_name, data_name)\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_OE_mask = img.copy()\n",
    "        mask = load_mask(f\"{img_name}OE\", data_name)\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_OE_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "\n",
    "        show_img = img.copy()\n",
    "        # Place small text of the image index in the top left corner\n",
    "        # Align text top and left\n",
    "        ax[i, 0].imshow(show_img)\n",
    "        ax[i, 0].text(5, 5, ['a','b','c','d'][i], fontsize=20, color='black', verticalalignment='top', horizontalalignment='left')\n",
    "        ax[i, 0].axis('off')\n",
    "\n",
    "        label = P53_CLASS_CODES[labels[img_name]]\n",
    "        prediction = P53_CLASS_CODES[np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])]\n",
    "        title = f\"Label {label}\\nPrediction {prediction}\"\n",
    "        ax[i, 0].set_title(title, \n",
    "                        #    align text right\n",
    "                            loc='right',\n",
    "                           )\n",
    "\n",
    "        # Set two y ticks for the second column, one for overexpression and one for nullmutation\n",
    "        # Calculate the tick locations to match up with the grid of heatmaps\n",
    "        tick_locs = [y * 2*h for y in [0.25, 0.75]]\n",
    "        ax[i, 1].set_yticks(tick_locs)\n",
    "        ax[i, 1].set_yticklabels([\"OE heatmap\", \"NM heatmap\"], rotation=90, verticalalignment='center', horizontalalignment='right')\n",
    "        for j, heatmap_type in enumerate(heatmap_types):\n",
    "            heatmaps = model_heatmaps[data_name][model_name][img_name][heatmap_type] # (2, H, W) torch float32\n",
    "\n",
    "            if heatmap_type == \"occlusion\":\n",
    "                heatmaps = [(hm - 0.5)/2 + 0.5 for hm in heatmaps]\n",
    "\n",
    "            # Resize the heatmaps to the original image size\n",
    "            heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "            # Cut the heatmaps to the same size as the image\n",
    "            heatmaps = [cut_img(hm, cuts[i]) for hm in heatmaps]\n",
    "\n",
    "            # Make 1x2 grid of heatmaps with make_grid\n",
    "            img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                            torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "            img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "            heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "\n",
    "            ax[i, j+1].imshow(img_underlay)\n",
    "            ax[i, j+1].imshow(heatmap, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "            ax[i, j+1].set_title(heatmap_type_mapping[heatmap_type])\n",
    "            ax[i, j+1].set_xticks([])\n",
    "            if j > 0:\n",
    "                ax[i, j+1].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    # Set suptitle\n",
    "    # fig.suptitle(model_name_mapping[model_name], fontsize=16, y=1.02)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HEX color_dict to a list of RGB [0,1] values\n",
    "print(color_dict)\n",
    "color_dict_RGB = {k: tuple([int(h[i:i+2], 16)/255 for i in (1, 3, 5)]) for k, h in color_dict.items()}\n",
    "print(color_dict_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load biopsy dims\n",
    "with open(os.path.join(BASE_DIR[\"test\"], \"biopsy_dims.json\"), \"r\") as f:\n",
    "    biopsy_dims = json.load(f)\n",
    "\n",
    "# Load non-empty patch indices\n",
    "non_empty_patch_indices = torch.load(os.path.join(BASE_DIR[\"test\"], \"non_empty_patch_indices_gs256.pt\"))\n",
    "\n",
    "# Make plot to compare the different heatmap methods for fb and fb_db\n",
    "# model_names = list(reversed([\"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\", \"fb_spacing4\", \"fb_db_spacing4\"]))\n",
    "model_names = [\"fb_spacing4\", \"fb_db_spacing4\", \"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\"]\n",
    "\n",
    "heatmap_type = \"inclusion\"\n",
    "n_cols = len(model_names) + 1 # +1 for the original image\n",
    "n_rows = len(indices)\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "for j, model_name in enumerate(model_names[:]):\n",
    "    print(model_name)\n",
    "    results = load_results(model_name, data_name)\n",
    "    if \"CLAM\" in model_name:\n",
    "        attention_maps = load_attention_maps(model_name, data_name)\n",
    "    best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "    best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "    for i, img_idx in enumerate(indices):\n",
    "        img_name = img_idx\n",
    "        img = load_img(img_name, data_name)\n",
    "        # Resize to be 0.25 of the original size\n",
    "        img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Make image square by cropping the longer side on the cuts[j] side\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        img = cut_img(img, cuts[i])\n",
    "\n",
    "        # Open mask and make contour\n",
    "        img_with_mask = img.copy()\n",
    "        NMmask = load_mask(img_name, data_name)\n",
    "        if NMmask is not None:\n",
    "            mask = cv2.resize(NMmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_OE_mask = img.copy()\n",
    "        OEmask = load_mask(f\"{img_name}OE\", data_name)\n",
    "        if OEmask is not None:\n",
    "            mask = cv2.resize(OEmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_OE_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_both_masks = img.copy()\n",
    "        if NMmask is not None and OEmask is not None:\n",
    "            mask = cv2.resize(NMmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_both_masks, contours[0], -1, color_dict_RGB[\"b\"], 3)\n",
    "            mask = cv2.resize(OEmask, (w,h))\n",
    "            mask = cut_img(mask, cuts[i])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_both_masks, contours[0], -1, color_dict_RGB[\"o\"], 3)\n",
    "\n",
    "        blanco = np.ones_like(img)\n",
    "\n",
    "        show_img = img.copy()\n",
    "        # Place small text of the image index in the top left corner\n",
    "        # Align text top and left\n",
    "        ax[i, 0].imshow(show_img)\n",
    "        ax[i, 0].text(5, 5, ['a','b','c','d'][i], fontsize=20, color='black', verticalalignment='top', horizontalalignment='left')\n",
    "        ax[i, 0].axis('off')\n",
    "        \n",
    "        label = P53_CLASS_CODES[labels[img_name]]\n",
    "        pred = np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])\n",
    "        prediction = P53_CLASS_CODES[pred]\n",
    "        title = f\"Label {label}\"\n",
    "        ax[i, 0].set_title(title, \n",
    "                        #    align text right\n",
    "                            loc='right',\n",
    "                           )\n",
    "\n",
    "        # Set two y ticks for the second column, one for overexpression and one for nullmutation\n",
    "        # Calculate the tick locations to match up with the grid of heatmaps\n",
    "        tick_locs = [y * 2*h for y in [0.25, 0.75]]\n",
    "        ax[i, -1].set_yticks(tick_locs)\n",
    "        ax[i, -1].set_yticklabels([\"OE heatmap\", \"NM heatmap\"], rotation=90, verticalalignment='center', horizontalalignment='left')\n",
    "        # Place these on the right side of the image\n",
    "        ax[i, -1].yaxis.tick_right()\n",
    "\n",
    "        if model_name in [\"fb_spacing4\", \"fb_db_spacing4\"]:\n",
    "            heatmaps = model_heatmaps[data_name][model_name][img_name][heatmap_type] # (2, H, W) torch float32\n",
    "        else:\n",
    "            heatmaps = vis_CLAM(None, img_name, None, attention_maps, non_empty_patch_indices, biopsy_dims)\n",
    "            # if len(heatmaps) != 2:\n",
    "            #     heatmaps = [heatmaps[pred]]\n",
    "\n",
    "        # Resize the heatmaps to the original image size\n",
    "        heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "        # Cut the heatmaps to the same size as the image\n",
    "        heatmaps = [cut_img(hm, cuts[i]) for hm in heatmaps]\n",
    "\n",
    "        if len(heatmaps) == 2:\n",
    "            # Make 1x2 grid of heatmaps with make_grid\n",
    "            img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                            torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "            img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "            heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "        elif len(heatmaps) == 1:\n",
    "            if prediction == \"OE\":\n",
    "                img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                                torch.tensor(blanco).permute(2, 0, 1)]\n",
    "                heatmaps = [heatmaps[0], np.zeros_like(heatmaps[0])]\n",
    "            elif prediction == \"NM\":\n",
    "                img_underlay = [torch.tensor(blanco).permute(2, 0, 1),\n",
    "                                torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "                heatmaps = [np.zeros_like(heatmaps[0]), heatmaps[0]]\n",
    "            elif prediction == \"DC\":\n",
    "                img_underlay = [torch.tensor(blanco).permute(2, 0, 1),\n",
    "                                torch.tensor(img_with_both_masks).permute(2, 0, 1),\n",
    "                                torch.tensor(blanco).permute(2, 0, 1)]\n",
    "                heatmaps = [np.zeros_like(heatmaps[0]), heatmaps[0], np.zeros_like(heatmaps[0])]\n",
    "            img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "            heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "\n",
    "            if prediction == \"DC\":\n",
    "                # Cut h//2 from the top and bottom\n",
    "                img_underlay = img_underlay[h//2:-h//2]\n",
    "                heatmap = heatmap[h//2:-h//2]\n",
    "\n",
    "        ax[i, j+1].imshow(img_underlay)\n",
    "        ax[i, j+1].imshow(heatmap, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "        title = f\"{model_name_mapping[model_name]}\\nPrediction {prediction}\"\n",
    "        ax[i, j+1].set_title(title)\n",
    "        ax[i, j+1].set_xticks([])\n",
    "        if j < 5:\n",
    "            ax[i, j+1].set_yticks([])\n",
    "plt.tight_layout()\n",
    "# Set suptitle\n",
    "# fig.suptitle(\"heatmap comparison between models\", fontsize=16, y=1.02)\n",
    "\n",
    "# Decrease the space between the subplots\n",
    "plt.subplots_adjust(hspace=-0.4, wspace=0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the hard cases\n",
    "\n",
    "most importantly:\n",
    "- Why is FB better at NM than CLAM? Receptive field? Feature extractor? Also test the CLAM model with ResNet18 encoder\n",
    "- Why is CLAM suddenly better at NM and OE in BOLERO? (is it just better at generalizing to new data or is it actually better at this data than LANS, and FB worse at it?)\n",
    "    - See CLAM heatmaps of the 8 failed NM from LANS and the 4 NM from BOLERO\n",
    "    - See FB heatmaps of those same cases\n",
    "\n",
    "also:\n",
    "- Why do the FBdb DC still go wrong? Difference in NM in DC, maybe less of it or looks different?\n",
    "- Why is CLAMdb+DC not as good as CLAM+DC? (is it the process-of-elimination thing? Could test this looking at the predictions of CLAM+DC and see if it's indeed 0 if not the class with DC being constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested CLAM with ResNet18_tuned, and also with retccl_tuned and retccl_tuned_hybrid\n",
    "\n",
    "The ResNet18_tuned performed better on the NM in LANS and identical to CLAM further. The ones it underclassifies look like the CLAM models with NM a bit better (and similar tiny trouble with OE)\n",
    "On BOLERO however, it looks almost exactly like the FB model, likely because it uses the same feature extractor.\n",
    "\n",
    "The retccl_tuned performed even better on NM but worse on OE. looks more like the FB than the CLAM models on LANS, even though it didn't inherit anything from the FB models (except finetuning the feature extractor on their labels).\n",
    "On BOLERO, it also looks very much like the FB models.\n",
    "\n",
    "The retccl_tuned_hybrid was similar in every way to the retccl_tuned, but with a slightly better OE performance on LANS.\n",
    "\n",
    "This suggests that finetuning is what helps achieve better NM performance on LANS, but also decreases performance on BOLERO due to overfitting to the training domain. Spacing and patch size don't seem to play a role in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get img_names for the NM biopsies from the pathxl data that were classified as WT by CLAM\n",
    "# Load results\n",
    "model_name = \"CLAM\"\n",
    "data_name = \"pathxl\"\n",
    "results = load_results(model_name, data_name)\n",
    "best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"]['0_1']][best_checkpoint_idx]\n",
    "# Load labels\n",
    "labels = get_labels(data_name)\n",
    "# Get img_names\n",
    "img_names = get_img_names(data_name)\n",
    "# Get intersection of label keys and img_names\n",
    "img_names = list(set(labels.keys()).intersection(set(img_names)))\n",
    "# Get the NM biopsies\n",
    "img_names = [img_name for img_name in img_names if labels[img_name] == 2]\n",
    "# Get the WT predictions from CLAM\n",
    "img_names = [img_name for img_name in img_names if np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name]) == 0]\n",
    "\n",
    "img_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize NM and DC of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 15 NM and 15 DC with the FB inclusion heatmap\n",
    "# model_name = \"fb_spacing4\"\n",
    "model_name = \"CLAM_db_m\"\n",
    "data_name = \"test\"\n",
    "heatmap_type = \"inclusion\"\n",
    "results = load_results(model_name, data_name)\n",
    "best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "\n",
    "# First all 15 NM\n",
    "indices = [img_name for img_name in get_img_names(data_name) if labels[img_name] == 2]\n",
    "# Then all 15 DC\n",
    "indices += [img_name for img_name in get_img_names(data_name) if labels[img_name] == 3]\n",
    "\n",
    "fig, ax = plt.subplots(10, 3, figsize=(12, 10*4.5))\n",
    "for i, img_idx in enumerate(indices):\n",
    "    ax_row = i // 3\n",
    "    ax_col = i % 3\n",
    "\n",
    "    img_name = img_idx\n",
    "    img = load_img(img_name, data_name)\n",
    "    # Resize to be 0.25 of the original size\n",
    "    img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Make image square by cropping the longer side on the cuts[j] side\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    # cut = cuts[i]\n",
    "    cut = \"right\"\n",
    "    img = cut_img(img, cut)\n",
    "\n",
    "    # Open mask and make contour\n",
    "    img_with_mask = img.copy()\n",
    "    NMmask = load_mask(img_name, data_name)\n",
    "    if NMmask is not None:\n",
    "        mask = cv2.resize(NMmask, (w,h))\n",
    "        mask = cut_img(mask, cut)\n",
    "        contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(img_with_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "    img_with_OE_mask = img.copy()\n",
    "    OEmask = load_mask(f\"{img_name}OE\", data_name)\n",
    "    if OEmask is not None:\n",
    "        mask = cv2.resize(OEmask, (w,h))\n",
    "        mask = cut_img(mask, cut)\n",
    "        contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(img_with_OE_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "\n",
    "    # show_img = img.copy()\n",
    "    # Place small text of the image index in the top left corner\n",
    "    # Align text top and left\n",
    "    # ax[i, 0].imshow(show_img)\n",
    "    # ax[i, 0].text(5, 5, ['a','b','c','d'][i], fontsize=20, color='black', verticalalignment='top', horizontalalignment='left')\n",
    "    # ax[i, 0].axis('off')\n",
    "    \n",
    "    if model_name in [\"fb_spacing4\", \"fb_db_spacing4\"]:\n",
    "        heatmaps = model_heatmaps[data_name][model_name][img_name][heatmap_type] # (2, H, W) torch float32\n",
    "    elif \"CLAM\" in model_name:\n",
    "        attention_maps = load_attention_maps(model_name, data_name)\n",
    "        heatmaps = vis_CLAM(None, img_name, None, attention_maps, non_empty_patch_indices, biopsy_dims)\n",
    "\n",
    "    # Resize the heatmaps to the original image size\n",
    "    heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "    # Cut the heatmaps to the same size as the image\n",
    "    heatmaps = [cut_img(hm, cut) for hm in heatmaps]\n",
    "\n",
    "    # Make 1x2 grid of heatmaps with make_grid\n",
    "    img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                    torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "    img_underlay = torchvision.utils.make_grid(img_underlay, nrow=2, normalize=True).permute(1, 2, 0).numpy()\n",
    "    heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=2, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "\n",
    "    ax[ax_row, ax_col].imshow(img_underlay)\n",
    "    ax[ax_row, ax_col].imshow(heatmap, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "\n",
    "    # Patch aggregate prediction\n",
    "    # Take max for both OE and NM\n",
    "    scores = [hm.max() for hm in heatmaps]\n",
    "    \n",
    "    label = P53_CLASS_CODES[labels[img_name]]\n",
    "    pred = results[\"status_probs\"][img_name][best_checkpoint_name][0]\n",
    "    title = f\"Label {label}\\nPred {[round(p,2) for p in pred]}\\nScores {[round(s,2) for s in scores]}\"\n",
    "    ax[ax_row, ax_col].set_title(title, \n",
    "                    #    align text right\n",
    "                        loc='right',\n",
    "                          )\n",
    "\n",
    "    ax[ax_row, ax_col].set_xticks([])\n",
    "    ax[ax_row, ax_col].set_yticks([])\n",
    "plt.tight_layout()\n",
    "# Set suptitle for the model\n",
    "fig.suptitle(model_name_mapping[model_name], fontsize=16, y=.88)\n",
    "\n",
    "# Decrease the space between the subplots\n",
    "plt.subplots_adjust(hspace=-0.8, wspace=0.1)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(VIS_DIR, f\"{model_name}_inclusion_NM_DC_heatmap.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient-based class activation maps (Grad-CAM)\n",
    "#\n",
    "# Grad-CAM is a technique to visualize the regions of the image that are important for the model's\n",
    "# prediction. It does this by computing the gradients of the output class with respect to the\n",
    "# feature maps of the last convolutional layer of the model. The gradients are then used to compute\n",
    "# a weighted sum of the feature maps, where the weights are the gradients. The resulting heatmap\n",
    "# is then overlaid on the original image to visualize the important regions.\n",
    "#\n",
    "# The code below is adapted from the PyTorch Grad-CAM tutorial:\n",
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#grad-cam\n",
    "def get_last_conv_layer(model):\n",
    "    for name, layer in reversed(list(model.named_modules())):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            return name, layer\n",
    "    raise ValueError(\"No convolutional layer found in the model\")\n",
    "\n",
    "class GradCam:\n",
    "    def __init__(self, model, target_layer, target_layer_name):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.target_layer_name = target_layer_name\n",
    "        self.model.eval()\n",
    "        self.feature_grad = None\n",
    "        self.feature_map = None\n",
    "        self.hook_feature_map()\n",
    "        self.hook_feature_grad()\n",
    "\n",
    "    def hook_feature_map(self):\n",
    "        def hook_fn(module, input, output):\n",
    "            self.feature_map = output\n",
    "        self.target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    def hook_feature_grad(self):\n",
    "        def hook_fn(module, grad_input, grad_output):\n",
    "            self.feature_grad = grad_output[0]\n",
    "        self.target_layer.register_backward_hook(hook_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def backward(self, output, target_class):\n",
    "        self.model.zero_grad()\n",
    "        one_hot_output = torch.zeros((1, output.size()[-1]), dtype=torch.float32, device=output.device)\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        cams = []\n",
    "        output = self.forward(x)\n",
    "        print(output)\n",
    "        n_outputs = output.size()[-1]\n",
    "        for target_class in range(n_outputs):\n",
    "            self.backward(output, target_class)\n",
    "            if self.feature_grad is None or self.feature_map is None:\n",
    "                raise ValueError(\"Feature gradients or feature maps are not set. Check hooks.\")\n",
    "            if self.feature_grad.sum() == 0:\n",
    "                print(\"No gradient for target class\", target_class)\n",
    "            if self.feature_map.sum() == 0:\n",
    "                print(\"No feature map for target class\", target_class)\n",
    "\n",
    "            if n_outputs == 2:\n",
    "                weights = self.model.model.fc[0].weight[target_class]\n",
    "            else:\n",
    "                weights = self.model.model.fc.weight[target_class]\n",
    "            \n",
    "            weights = torch.mean(self.feature_grad, dim=(2, 3)).squeeze()\n",
    "            cam = torch.tensordot(weights, self.feature_map.squeeze(0), dims=([0], [0]))\n",
    "            # cam = torch.nn.functional.relu(cam)\n",
    "            \n",
    "            # Avoid NaNs in normalization\n",
    "            if torch.isnan(cam).any():\n",
    "                raise ValueError(\"CAM contains NaN values before normalization.\")\n",
    "            cam = cam.detach().cpu().numpy()\n",
    "            \n",
    "            cam_min, cam_max = cam.min(), cam.max()\n",
    "            print(cam_min, cam_max)\n",
    "            if cam_max == cam_min:\n",
    "                # print(\"CAM has uniform values. Check model and target class.\")\n",
    "                pass\n",
    "            else:\n",
    "                # cam /= (cam_max - cam_min)\n",
    "                cam /= np.abs(cam).max()\n",
    "                pass\n",
    "            cam += 0.5\n",
    "            cams.append(cam)\n",
    "        return output, cams\n",
    "\n",
    "def get_grad_cam(model, img_name, data_name):\n",
    "    img = get_img_const_spacing(img_name, data_name, 4)\n",
    "    last_conv_layer_name, last_conv_layer = get_last_conv_layer(model)\n",
    "    grad_cam = GradCam(model, last_conv_layer, last_conv_layer_name)\n",
    "    output, cams = grad_cam(img.to(device))\n",
    "    cams = cams if len(cams) == 2 else cams[1:3]\n",
    "    return output, cams\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_name = \"fb_db_spacing4\"\n",
    "checkpoint_path = model_kwargs[model_name][\"checkpoint_paths\"][best_checkpoint_dict[model_name]]\n",
    "model = load_model(model_kwargs[model_name][\"model_class\"], checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "img_name = 7\n",
    "data_name = \"test\"\n",
    "output, cams = get_grad_cam(model, img_name, data_name)\n",
    "\n",
    "plot_heatmap(cams, img_name, data_name, downsample=0.25, in_one=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how large ResNet18s receptive field is for different image sizes\n",
    "from resnet import ResNetModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dummy input image\n",
    "img_size = 2048\n",
    "img = torch.zeros(1, 3, img_size, img_size)\n",
    "img[:, :, img_size//2, img_size//2] = 1\n",
    "img[:, :, img_size//4, img_size//4] = 1\n",
    "# img[:, :, img_size//4:3*img_size//4, img_size//4:3*img_size//4] = 1\n",
    "\n",
    "plt.imshow(img.squeeze().numpy().transpose(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "# Load the model\n",
    "model = models.resnet18()\n",
    "# Set all parameters to 1\n",
    "# for param in model.parameters():\n",
    "#     param.data.fill_(1)\n",
    "# Cut off before the pooling layer\n",
    "# model = nn.Sequential(*list(list(model.children())[0].children())[:-2])\n",
    "model = nn.Sequential(*list(model.children())[:-2])\n",
    "# display(model)\n",
    "model.eval()\n",
    "\n",
    "# Get the output of the model\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "\n",
    "# Plot the output\n",
    "output = output.squeeze().numpy().transpose(1, 2, 0).max(axis=2)\n",
    "# display(output)\n",
    "plt.imshow(output, cmap='gray', vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find biopsies with highest predicted probability for WT, OE, NM and DC respectively, specifically for the CLAM_db_m model\n",
    "model_name = \"CLAM_db_m\"\n",
    "data_name = \"test\"\n",
    "results = load_results(model_name, data_name)\n",
    "labels = get_labels(data_name)\n",
    "best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "\n",
    "attention_maps = load_attention_maps(model_name, data_name)\n",
    "non_empty_patch_indices = torch.load(os.path.join(BASE_DIR[data_name], \"non_empty_patch_indices_gs256.pt\"))\n",
    "with open(os.path.join(BASE_DIR[data_name], \"biopsy_dims.json\"), \"r\") as f:\n",
    "    biopsy_dims = json.load(f)\n",
    "\n",
    "# Get the indices of the biopsies with the highest predicted probability for each class\n",
    "top_indices = {}\n",
    "indices = [k for k in results[\"status_probs\"]]\n",
    "for class_idx in range(4):\n",
    "    top_indices[class_idx] = sorted(indices, key=lambda x: results[\"status_probs\"][x][best_checkpoint_name][0][class_idx], reverse=True)[:5]\n",
    "    print(top_indices[class_idx])\n",
    "\n",
    "# Visualize the biopsies with the highest predicted probability for each class\n",
    "n_cols = 5\n",
    "n_rows = 4\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "for i, class_idx in enumerate(top_indices):\n",
    "    for j, img_idx in enumerate(top_indices[class_idx]):\n",
    "        img_name = img_idx\n",
    "        if \"_\" in str(img_name):\n",
    "            data_name = \"pathxl\"\n",
    "        else:\n",
    "            data_name = \"test\"\n",
    "        img = load_img(img_name, data_name)\n",
    "        # Resize to be 0.25 of the original size\n",
    "        img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Make image square by cropping the longer side on the cuts[j] side\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        # img = cut_img(img, cuts[j])\n",
    "\n",
    "        # Open mask and make contour\n",
    "        img_with_mask = img.copy()\n",
    "        mask = load_mask(img_name, \"test\")\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (w,h))\n",
    "            # mask = cut_img(mask, cuts[j])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "        img_with_OE_mask = img.copy()\n",
    "        mask = load_mask(f\"{img_name}OE\", \"test\")\n",
    "        if mask is not None:\n",
    "            mask = cv2.resize(mask, (w,h))\n",
    "            # mask = cut_img(mask, cuts[j])\n",
    "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(img_with_OE_mask, contours[0], -1, (0, 0, 0), 2)\n",
    "\n",
    "        show_img = img.copy()\n",
    "        # Place small text of the image index in the top left corner\n",
    "        # Align text top and left\n",
    "\n",
    "        # Load heatmaps\n",
    "        heatmaps = vis_CLAM(None, img_name, None, attention_maps, non_empty_patch_indices, biopsy_dims)\n",
    "\n",
    "        # Resize the heatmaps to the original image size\n",
    "        heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "\n",
    "        # Make 1x2 grid of heatmaps with make_grid\n",
    "        img_underlay = [torch.tensor(img_with_OE_mask).permute(2, 0, 1),\n",
    "                        torch.tensor(img_with_mask).permute(2, 0, 1)]\n",
    "        img_underlay = torchvision.utils.make_grid(img_underlay, nrow=1, normalize=True).permute(1, 2, 0).numpy()\n",
    "        heatmap = torchvision.utils.make_grid(torch.tensor(heatmaps).unsqueeze(1), nrow=1, normalize=False).permute(1, 2, 0).numpy()[:,:,0]\n",
    "\n",
    "        ax[i, j].imshow(img_underlay)\n",
    "        ax[i, j].imshow(heatmap, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "        # ax[i, j].imshow(show_img)\n",
    "\n",
    "        ax[i, j].axis('off')\n",
    "\n",
    "        label = P53_CLASS_CODES[labels[img_name]]\n",
    "        prediction = P53_CLASS_CODES[np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])]\n",
    "        title = f\"({img_name}) Label {label}\\nPrediction {prediction} {results['status_probs'][img_name][best_checkpoint_name][0][class_idx]:.2f}\"\n",
    "        ax[i, j].set_title(title, \n",
    "                        #    align text right\n",
    "                            loc='right',\n",
    "                            )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [192, 34, 0]\n",
    "cuts = [\"right\", \"left\", \"left\"]\n",
    "\n",
    "# Plot a 3x3 plot with each column for a different biopsy\n",
    "# The first row is the original image with mask contours, the second row is the heatmap for the OE class and the third row is the heatmap for the NM class\n",
    "n_cols = 3\n",
    "n_rows = len(indices)\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "for i, img_idx in enumerate(indices):\n",
    "    img_name = img_idx\n",
    "    img = load_img(img_name, \"test\")\n",
    "    # Resize to be 0.25 of the original size\n",
    "    img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Make image square by cropping the longer side on the cuts[i] side\n",
    "    h, w = img.shape[0], img.shape[1]\n",
    "    img = cut_img(img, cuts[i])\n",
    "\n",
    "    # Open mask and make contour\n",
    "    img_with_NM_mask = img.copy()\n",
    "    img_with_OE_mask = img.copy()\n",
    "    img_with_both_masks = img.copy()\n",
    "    mask = load_mask(img_name, \"test\")\n",
    "    OEmask = load_mask(f\"{img_name}OE\", \"test\")\n",
    "    if mask is not None:\n",
    "        mask = cv2.resize(mask, (w,h))\n",
    "        mask = cut_img(mask, cuts[i])\n",
    "        contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(img_with_NM_mask, contours[0], -1, (0,0,0), 1)\n",
    "        cv2.drawContours(img_with_both_masks, contours[0], -1, color_dict_RGB[\"b\"], 2)\n",
    "    if OEmask is not None:\n",
    "        mask = cv2.resize(OEmask, (w,h))\n",
    "        mask = cut_img(mask, cuts[i])\n",
    "        contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(img_with_OE_mask, contours[0], -1, (0,0,0), 1)\n",
    "        cv2.drawContours(img_with_both_masks, contours[0], -1, color_dict_RGB[\"o\"], 2)\n",
    "\n",
    "    # Place small text of the image index in the top left corner\n",
    "    # Align text top and left\n",
    "    ax[i, 0].imshow(img_with_both_masks)\n",
    "    ax[i, 0].text(5, 5, ['a','b','c','d'][i], fontsize=20, color='black', verticalalignment='top', horizontalalignment='left')\n",
    "    ax[i, 0].axis('off')\n",
    "\n",
    "    label = P53_CLASS_CODES[labels[img_name]]\n",
    "    prediction = P53_CLASS_CODES[np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])]\n",
    "    title = f\"Label {label}\"\n",
    "    # ax[0, j].set_title(title)\n",
    "\n",
    "    # Load heatmaps\n",
    "    heatmaps = vis_CLAM(None, img_name, None, attention_maps, non_empty_patch_indices, biopsy_dims)\n",
    "\n",
    "    # Resize the heatmaps to the original image size\n",
    "    heatmaps = [cv2.resize(hm, (w, h)) for hm in heatmaps]\n",
    "\n",
    "    # Cut the heatmaps to the same size as the image\n",
    "    heatmaps = [cut_img(hm, cuts[i]) for hm in heatmaps]\n",
    "\n",
    "    # Plot the img with the heatmaps overlaid\n",
    "    ax[i, 1].imshow(img_with_OE_mask)\n",
    "    ax[i, 1].imshow(heatmaps[0], cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "    # ax[i, 1].set_title(\"OE heatmap\")\n",
    "    ax[i, 1].axis('off')\n",
    "\n",
    "    ax[i, 2].imshow(img_with_NM_mask)\n",
    "    ax[i, 2].imshow(heatmaps[1], cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "    # ax[i, 2].set_title(\"NM heatmap\")\n",
    "    ax[i, 2].axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Make subplots close to each other\n",
    "plt.subplots_adjust(hspace=-0.25, wspace=0.05)\n",
    "\n",
    "ax[0, 0].set_title(\"biopsy with annotations\")\n",
    "ax[0, 1].set_title(\"overexpression heatmap\")\n",
    "ax[0, 2].set_title(\"null-mutation heatmap\")\n",
    "\n",
    "ax[0, 0].legend(handles=[\n",
    "    plt.Rectangle((0,0),1,1,fc=color_dict[\"o\"], linewidth=1, label=\"overexpression\"),\n",
    "    plt.Rectangle((0,0),1,1,fc=color_dict[\"b\"], linewidth=1, label=\"null-mutation\"),\n",
    "], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find heatmaps with highest predicted probability for OE and NM respectively\n",
    "model_name = \"fb_db_spacing4\"\n",
    "data_name = \"test\"\n",
    "results = load_results(model_name, data_name)\n",
    "labels = get_labels(data_name)\n",
    "best_checkpoint_idx = best_checkpoint_dict[model_name]\n",
    "best_checkpoint_name = [checkpoint_name for checkpoint_name in results[\"status_probs\"][indices[0]]][best_checkpoint_idx]\n",
    "\n",
    "# Get the indices of the heatmaps with the highest max for each class\n",
    "top_indices = {}\n",
    "indices = [k for k in results[\"status_probs\"]]\n",
    "heatmap_type = \"inclusion\"\n",
    "heatmaps = {img_name: model_heatmaps[data_name][model_name][img_name][heatmap_type] for img_name in indices}\n",
    "for class_idx in range(2):\n",
    "    top_indices[class_idx] = sorted(indices, key=lambda x: max([hm.max() for hm in heatmaps[x][class_idx]]), reverse=True)[:5]\n",
    "    print(top_indices[class_idx])\n",
    "\n",
    "# Also get indices of the heatmaps with the lowest min for both classes at the same time\n",
    "bottom_indices = []\n",
    "bottom_indices = sorted(indices, key=lambda x: min([hm.min() for hm in heatmaps[x][0]]), reverse=False)[:5]\n",
    "\n",
    "top_indices = {\n",
    "    0: [32, 1031, 222],\n",
    "    1: [1416, 240, 1489],\n",
    "}\n",
    "\n",
    "# Visualize the heatmaps with the highest max for each class\n",
    "n_cols = 5\n",
    "n_rows = 3\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, n_rows*4.5))\n",
    "for i, class_idx in enumerate(top_indices):\n",
    "    for j, img_idx in enumerate(top_indices[class_idx]):\n",
    "        img_name = img_idx\n",
    "        img = load_img(img_name, data_name)\n",
    "        # Resize to be 0.25 of the original size\n",
    "\n",
    "        # Make image square by cropping the longer side on the cuts[j] side\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        # img = cut_img(img, cuts[j])\n",
    "\n",
    "        # Open mask\n",
    "        if class_idx == 1:\n",
    "            mask = load_mask(img_name, data_name)\n",
    "            mask = cv2.resize(mask, (w,h)) / 255\n",
    "        else:\n",
    "            mask = np.ones((h, w))\n",
    "\n",
    "        # Take the 256x256 patch of the heatmap with the highest max\n",
    "        heatmap = model_heatmaps[data_name][model_name][img_name][heatmap_type][class_idx] # (H, W) torch float32\n",
    "        # Resize the heatmap to the original image size\n",
    "        heatmap = cv2.resize(heatmap, (w, h))\n",
    "        # Take AND of the heatmap and the mask\n",
    "        heatmap = heatmap * mask\n",
    "        # Get the indices of the max value in the heatmap\n",
    "        max_idx = np.unravel_index(heatmap.argmax(), heatmap.shape)\n",
    "        # Get the 256x256 patch around the max value, on the original image\n",
    "        img = img[max_idx[0]-128:max_idx[0]+128, max_idx[1]-128:max_idx[1]+128]\n",
    "        hm = heatmap[max_idx[0]-128:max_idx[0]+128, max_idx[1]-128:max_idx[1]+128]\n",
    "        \n",
    "        ax[i, j].imshow(img)\n",
    "        # ax[i, j].imshow(hm, cmap='jet', vmin=0, vmax=1, alpha=0.3)\n",
    "\n",
    "        ax[i, j].axis('off')\n",
    "\n",
    "        label = P53_CLASS_CODES[labels[img_name]]\n",
    "        prediction = P53_CLASS_CODES[np.argmax(results[\"status_probs\"][img_name][best_checkpoint_name])]\n",
    "        title = f\"({img_name}) Label {label}\\nPrediction {prediction} {hm.max():.2f}\"\n",
    "        ax[i, j].set_title(title, \n",
    "                        #    align text right\n",
    "                            loc='right',\n",
    "                            )\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
