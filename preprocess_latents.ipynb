{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\321lu\\AppData\\Local\\Temp\\ipykernel_32800\\3817817176.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from load_data import BiopsyDataset, get_balanced_dataloader, Nullmutation\n",
    "from eval import get_accuracy_per_class\n",
    "\n",
    "DATA_DIR = \"../../data/biopsies_s1.0_anon_data/\"\n",
    "PATHXL_DIR = os.path.join(DATA_DIR, \"..\", \"p53_consensus_study\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Load csv\n",
    "csv_file = os.path.join(DATA_DIR, \"biopsy_labels_anon_s1.0.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "pathxl_csv_file = os.path.join(PATHXL_DIR, \"labels.csv\")\n",
    "pathxl_df = pd.read_csv(pathxl_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 512\n",
      "Number of features: 2048\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet18\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "# ResNet18 is pretrained on images with size 224x224\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "resnet18.fc = nn.Identity()\n",
    "\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
    "# ResNet50 is pretrained on images with size 224x224\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "resnet50.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 2048\n"
     ]
    }
   ],
   "source": [
    "from RetCCL.custom_objects import retccl_resnet50, HistoRetCCLResnet50_Weights\n",
    "\n",
    "retccl = retccl_resnet50(weights=HistoRetCCLResnet50_Weights.RetCCLWeights).to(device)\n",
    "# retccl is pretrained on images with size 256x256 at 1 micron per pixel\n",
    "num_ftrs = retccl.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "retccl.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df:\n",
    "\tid\tlabel\n",
    "0\t0\t2\n",
    "1\t1\t2\n",
    "2\t2\t2\n",
    "3\t3\t2\n",
    "4\t4\t2\n",
    "...\t...\t...\n",
    "1527\t1527\t2\n",
    "1528\t1528\t2\n",
    "1529\t1529\t0\n",
    "1530\t1530\t0\n",
    "1531\t1531\t0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ImageNet normalization\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                      std=[0.229, 0.224, 0.225]) # What this does is it subtracts the mean from each channel and divides by the std\n",
    "])\n",
    "\n",
    "model_configs = {\n",
    "    \"resnet18\": {\n",
    "        \"model\": resnet18,\n",
    "        \"size\": 224,\n",
    "        \"num_ftrs\": 512,\n",
    "    },\n",
    "    \"resnet50\": {\n",
    "        \"model\": resnet50,\n",
    "        \"size\": 224,\n",
    "        \"num_ftrs\": 2048,\n",
    "    },\n",
    "    \"retccl\": {\n",
    "        \"model\": retccl,\n",
    "        # \"size\": 256,\n",
    "        \"size\": None,\n",
    "        \"num_ftrs\": 2048,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1532 [00:00<?, ?it/s]c:\\Users\\321lu\\.conda\\envs\\dl2023\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1532/1532 [11:32<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "spacing = 4\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    if model_name != \"retccl\":\n",
    "        continue\n",
    "\n",
    "    model = model_config[\"model\"]\n",
    "    model.eval()\n",
    "    size = model_config[\"size\"]\n",
    "    if size == None:\n",
    "        transform = TRANSFORM\n",
    "        size = \"Full\"\n",
    "    else:\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((size, size)),\n",
    "            TRANSFORM\n",
    "        ])\n",
    "    num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "    # Open image from dataset\n",
    "    latents = torch.zeros((len(df), 4, num_ftrs)) # 4 is the number of rotations\n",
    "    latent_file = os.path.join(DATA_DIR, f\"latents_s{spacing}_{model_name}.pt\")\n",
    "    if os.path.exists(latent_file):\n",
    "        latents = torch.load(latent_file).detach()\n",
    "\n",
    "    for idx, label in tqdm(df.values):\n",
    "        if torch.any(latents[idx] != 0):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(DATA_DIR, \"biopsies\", f\"{idx}.png\")\n",
    "        img = plt.imread(img_path) # HxWxC\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "\n",
    "        new_size = (img.shape[1]//spacing, img.shape[2]//spacing)\n",
    "        # Apply resize transform\n",
    "        img = torchvision.transforms.functional.resize(img, new_size)\n",
    "\n",
    "        # Apply other transform\n",
    "        img = transform(img).unsqueeze(0).float().to(device)\n",
    "\n",
    "        # for i in range(4):\n",
    "        #     img_rot = torch.rot90(img, i, [2,3])\n",
    "        #     img_latent = model(img_rot)\n",
    "        #     latents[idx, i] = img_latent.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_latent = model(img)\n",
    "            latents[idx] = img_latent\n",
    "\n",
    "        torch.save(latents, latent_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1532/1532 [02:23<00:00, 10.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from load_data import BagDataset, P53_CLASS_NAMES\n",
    "\n",
    "size = 256\n",
    "quantile_range_threshold = 0.1\n",
    "resize_factor = 1.0\n",
    "root_dir = DATA_DIR\n",
    "\n",
    "model_name = \"retccl\"\n",
    "model_config = model_configs[model_name]\n",
    "model = model_config[\"model\"]\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "indices_filename = os.path.join(DATA_DIR, \"non_empty_patch_indices_gs256_relaxed.pt\")\n",
    "non_empty_patch_indices_by_biopsy = torch.load(indices_filename)\n",
    "\n",
    "# Load latents\n",
    "latent_file = os.path.join(DATA_DIR, f\"bag_latents_gs{size}_{model_name}.pt\")\n",
    "latent_file_backup = os.path.join(DATA_DIR, f\"bag_latents_gs{size}_{model_name}_backup.pt\")\n",
    "if os.path.exists(latent_file):\n",
    "    bag_latents = torch.load(latent_file)\n",
    "else:\n",
    "    bag_latents = {}\n",
    "for idx, label in tqdm(df.values):\n",
    "    # if idx in bag_latents:\n",
    "    #     continue\n",
    "    if idx in non_empty_patch_indices_by_biopsy:\n",
    "        continue\n",
    "\n",
    "    img_file = os.path.join(root_dir, \"biopsies\", f\"{idx}.png\")\n",
    "    img = plt.imread(img_file)\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() # (3, h, w)\n",
    "\n",
    "    # Resize the img so that the height and width are multiples of grid_spacing, \n",
    "    # rounded to the nearest multiple.\n",
    "    # This prevents the last row and column of patches from being cut off.\n",
    "    # We also resize the image by the resize_factor instead of each patch separately\n",
    "    new_size = (max(round(img.shape[1]*resize_factor/size), 1)*size, \n",
    "                max(round(img.shape[2]*resize_factor/size), 1)*size)\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=new_size, \n",
    "        mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "    patches = torch.nn.functional.unfold(img.unsqueeze(0), \n",
    "        kernel_size=(size, size), \n",
    "        stride=(size, size)) # (1, 3*size*size, n_patches)\n",
    "    patches = patches.permute(0, 2, 1).reshape(-1, 3, size, size) # (n_patches, 3, size, size)\n",
    "    \n",
    "    # Remove patches with low difference between the 75th and 1st percentile\n",
    "    qranges = torch.quantile(patches.view(patches.shape[0], -1), 0.75, dim=-1) - \\\n",
    "              torch.quantile(patches.view(patches.shape[0], -1), 0.01, dim=-1)\n",
    "    non_empty_idx = torch.where(qranges >= quantile_range_threshold)[0]\n",
    "    # patches = TRANSFORM(patches[non_empty_idx])\n",
    "    # patches = patches.to(device)\n",
    "\n",
    "    non_empty_patch_indices_by_biopsy[idx] = non_empty_idx\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     patch_latents = model(patches) # (n_patches, num_ftrs)\n",
    "    # bag_latents[idx] = patch_latents.cpu()[:, None, :] # (n_patches, 1, num_ftrs)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        # torch.save(bag_latents, latent_file_backup)\n",
    "        torch.save(non_empty_patch_indices_by_biopsy, indices_filename)\n",
    "\n",
    "    # Clear memory\n",
    "    # del img, patches, patch_latents\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "# torch.save(bag_latents, latent_file)\n",
    "# torch.save(bag_latents, latent_file_backup)\n",
    "torch.save(non_empty_patch_indices_by_biopsy, indices_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_latents = torch.load(os.path.join(DATA_DIR, \"bag_latents_gs256_retccl.pt\"))\n",
    "\n",
    "print(type(bag_latents))\n",
    "print(len(bag_latents))\n",
    "print(bag_latents.keys())\n",
    "print(bag_latents[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 549/650 [01:22<00:43,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_1.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_2.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_3.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_5.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_6.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_7.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_8.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_9.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_10.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_11.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\159_1.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\159_2.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\159_3.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\159_4.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\159_5.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\159_6.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\160_1.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\161_2.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\161_4.png does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 650/650 [08:43<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\158_4.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\161_1.png does not exist\n",
      "File ../../data/biopsies_s1.0_anon_data/..\\p53_consensus_study\\biopsies\\161_3.png does not exist\n"
     ]
    }
   ],
   "source": [
    "# PathXL dataset\n",
    "\n",
    "from load_data import BagDataset, P53_CLASS_NAMES\n",
    "\n",
    "size = 256\n",
    "quantile_range_threshold = 0.1\n",
    "resize_factor = 1.0\n",
    "root_dir = PATHXL_DIR\n",
    "\n",
    "model_name = \"retccl\"\n",
    "model_config = model_configs[model_name]\n",
    "model = model_config[\"model\"]\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "indices_filename = os.path.join(PATHXL_DIR, \"non_empty_patch_indices_gs256.pt\")\n",
    "if os.path.exists(indices_filename):\n",
    "    non_empty_patch_indices_by_biopsy = torch.load(indices_filename)\n",
    "else:\n",
    "    non_empty_patch_indices_by_biopsy = {}\n",
    "\n",
    "# Load latents\n",
    "latent_file = os.path.join(PATHXL_DIR, f\"bag_latents_gs{size}_{model_name}.pt\")\n",
    "if os.path.exists(latent_file):\n",
    "    bag_latents = torch.load(latent_file)\n",
    "else:\n",
    "    bag_latents = {}\n",
    "for i, row in tqdm(enumerate(pathxl_df.values), total=len(pathxl_df)):\n",
    "    idx = f\"{row[0]}_{row[1]}\"\n",
    "    if idx in bag_latents and idx in non_empty_patch_indices_by_biopsy:\n",
    "        continue\n",
    "\n",
    "    img_file = os.path.join(root_dir, \"biopsies\", f\"{idx}.png\")\n",
    "    if not os.path.exists(img_file):\n",
    "        print(f\"File {img_file} does not exist\")\n",
    "        continue\n",
    "    img = plt.imread(img_file)\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() # (3, h, w)\n",
    "\n",
    "    # Resize the img so that the height and width are multiples of grid_spacing, \n",
    "    # rounded to the nearest multiple.\n",
    "    # This prevents the last row and column of patches from being cut off.\n",
    "    # We also resize the image by the resize_factor instead of each patch separately\n",
    "    new_size = (max(round(img.shape[1]*resize_factor/size), 1)*size, \n",
    "                max(round(img.shape[2]*resize_factor/size), 1)*size)\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=new_size, \n",
    "        mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "    patches = torch.nn.functional.unfold(img.unsqueeze(0), \n",
    "        kernel_size=(size, size), \n",
    "        stride=(size, size)) # (1, 3*size*size, n_patches)\n",
    "    patches = patches.permute(0, 2, 1).reshape(-1, 3, size, size) # (n_patches, 3, size, size)\n",
    "    \n",
    "    # Remove patches with low difference between the 75th and 1st percentile\n",
    "    qranges = torch.quantile(patches.view(patches.shape[0], -1), 0.75, dim=-1) - \\\n",
    "              torch.quantile(patches.view(patches.shape[0], -1), 0.01, dim=-1)\n",
    "    non_empty_idx = torch.where(qranges >= quantile_range_threshold)[0]\n",
    "    patches = TRANSFORM(patches[non_empty_idx])\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    non_empty_patch_indices_by_biopsy[idx] = non_empty_idx\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_latents = model(patches) # (n_patches, num_ftrs)\n",
    "    bag_latents[idx] = patch_latents.cpu()[:, None, :] # (n_patches, 1, num_ftrs)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        torch.save(bag_latents, latent_file)\n",
    "        torch.save(non_empty_patch_indices_by_biopsy, indices_filename)\n",
    "\n",
    "    # Clear memory\n",
    "    del img, patches, patch_latents\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(bag_latents, latent_file)\n",
    "torch.save(non_empty_patch_indices_by_biopsy, indices_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test how padding affects the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "img_path = os.path.join(DATA_DIR, \"biopsies\", f\"{df.iloc[0,0]}.png\")\n",
    "img = plt.imread(img_path) # HxWxC\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = torch.tensor(img).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "img_latent = model(img)\n",
    "# Zero pad to square\n",
    "if img.shape[2] != img.shape[3]:\n",
    "    pad = abs(img.shape[2] - img.shape[3]) // 2\n",
    "    if img.shape[2] > img.shape[3]:\n",
    "        img = torch.nn.functional.pad(img, (pad,pad,0,0))\n",
    "    else:\n",
    "        img = torch.nn.functional.pad(img, (0,0,pad,pad))\n",
    "\n",
    "# Show again\n",
    "show_img = img.squeeze().permute(1,2,0).cpu().numpy()\n",
    "plt.imshow(show_img)\n",
    "plt.show()\n",
    "\n",
    "img_latent_padded = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_latent_padded - img_latent)\n",
    "\n",
    "# Did the padding change the latent representation?\n",
    "print(torch.allclose(img_latent, img_latent_padded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
