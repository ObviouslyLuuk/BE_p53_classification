{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from load_data import BiopsyDataset, get_balanced_dataloader, Nullmutation\n",
    "from eval import get_accuracy_per_class\n",
    "\n",
    "DATA_DIR = \"../../data/biopsies_s1.0_anon_data/\"\n",
    "PATHXL_DIR = os.path.join(DATA_DIR, \"..\", \"p53_consensus_study\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Load csv\n",
    "csv_file = os.path.join(DATA_DIR, \"biopsy_labels_anon_s1.0.csv\")\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "pathxl_csv_file = os.path.join(PATHXL_DIR, \"labels.csv\")\n",
    "pathxl_df = pd.read_csv(pathxl_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet18\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "# ResNet18 is pretrained on images with size 224x224\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "resnet18.fc = nn.Identity()\n",
    "\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
    "# ResNet50 is pretrained on images with size 224x224\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "resnet50.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RetCCL.custom_objects import retccl_resnet50, HistoRetCCLResnet50_Weights\n",
    "\n",
    "retccl = retccl_resnet50(weights=HistoRetCCLResnet50_Weights.RetCCLWeights).to(device)\n",
    "# retccl is pretrained on images with size 256x256 at 1 micron per pixel\n",
    "num_ftrs = retccl.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "retccl.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df:\n",
    "\tid\tlabel\n",
    "0\t0\t2\n",
    "1\t1\t2\n",
    "2\t2\t2\n",
    "3\t3\t2\n",
    "4\t4\t2\n",
    "...\t...\t...\n",
    "1527\t1527\t2\n",
    "1528\t1528\t2\n",
    "1529\t1529\t0\n",
    "1530\t1530\t0\n",
    "1531\t1531\t0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ImageNet normalization\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                      std=[0.229, 0.224, 0.225]) # What this does is it subtracts the mean from each channel and divides by the std\n",
    "])\n",
    "\n",
    "model_configs = {\n",
    "    \"resnet18\": {\n",
    "        \"model\": resnet18,\n",
    "        \"size\": 224,\n",
    "        \"num_ftrs\": 512,\n",
    "    },\n",
    "    \"resnet50\": {\n",
    "        \"model\": resnet50,\n",
    "        \"size\": 224,\n",
    "        \"num_ftrs\": 2048,\n",
    "    },\n",
    "    \"retccl\": {\n",
    "        \"model\": retccl,\n",
    "        # \"size\": 256,\n",
    "        \"size\": None,\n",
    "        \"num_ftrs\": 2048,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Biopsies\n",
    "not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 4\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    if model_name != \"retccl\":\n",
    "        continue\n",
    "\n",
    "    model = model_config[\"model\"]\n",
    "    model.eval()\n",
    "    size = model_config[\"size\"]\n",
    "    if size == None:\n",
    "        transform = TRANSFORM\n",
    "        size = \"Full\"\n",
    "    else:\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((size, size)),\n",
    "            TRANSFORM\n",
    "        ])\n",
    "    num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "    # Open image from dataset\n",
    "    latents = torch.zeros((len(df), 4, num_ftrs)) # 4 is the number of rotations\n",
    "    latent_file = os.path.join(DATA_DIR, f\"latents_s{spacing}_{model_name}.pt\")\n",
    "    if os.path.exists(latent_file):\n",
    "        latents = torch.load(latent_file).detach()\n",
    "\n",
    "    for idx, label in tqdm(df.values):\n",
    "        if torch.any(latents[idx] != 0):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(DATA_DIR, \"biopsies\", f\"{idx}.png\")\n",
    "        img = plt.imread(img_path) # HxWxC\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "\n",
    "        new_size = (img.shape[1]//spacing, img.shape[2]//spacing)\n",
    "        # Apply resize transform\n",
    "        img = torchvision.transforms.functional.resize(img, new_size)\n",
    "\n",
    "        # Apply other transform\n",
    "        img = transform(img).unsqueeze(0).float().to(device)\n",
    "\n",
    "        # for i in range(4):\n",
    "        #     img_rot = torch.rot90(img, i, [2,3])\n",
    "        #     img_latent = model(img_rot)\n",
    "        #     latents[idx, i] = img_latent.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_latent = model(img)\n",
    "            latents[idx] = img_latent\n",
    "\n",
    "        torch.save(latents, latent_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import BagDataset, P53_CLASS_NAMES\n",
    "\n",
    "size = 256\n",
    "quantile_range_threshold = 0.1\n",
    "resize_factor = 1.0\n",
    "root_dir = DATA_DIR\n",
    "\n",
    "model_name = \"retccl\"\n",
    "model_config = model_configs[model_name]\n",
    "model = model_config[\"model\"]\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "indices_filename = os.path.join(DATA_DIR, \"non_empty_patch_indices_gs256_relaxed.pt\")\n",
    "non_empty_patch_indices_by_biopsy = torch.load(indices_filename)\n",
    "\n",
    "# Load latents\n",
    "latent_file = os.path.join(DATA_DIR, f\"bag_latents_gs{size}_{model_name}.pt\")\n",
    "latent_file_backup = os.path.join(DATA_DIR, f\"bag_latents_gs{size}_{model_name}_backup.pt\")\n",
    "if os.path.exists(latent_file):\n",
    "    bag_latents = torch.load(latent_file)\n",
    "else:\n",
    "    bag_latents = {}\n",
    "for idx, label in tqdm(df.values):\n",
    "    # if idx in bag_latents:\n",
    "    #     continue\n",
    "    if idx in non_empty_patch_indices_by_biopsy:\n",
    "        continue\n",
    "\n",
    "    img_file = os.path.join(root_dir, \"biopsies\", f\"{idx}.png\")\n",
    "    img = plt.imread(img_file)\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() # (3, h, w)\n",
    "\n",
    "    # Resize the img so that the height and width are multiples of grid_spacing, \n",
    "    # rounded to the nearest multiple.\n",
    "    # This prevents the last row and column of patches from being cut off.\n",
    "    # We also resize the image by the resize_factor instead of each patch separately\n",
    "    new_size = (max(round(img.shape[1]*resize_factor/size), 1)*size, \n",
    "                max(round(img.shape[2]*resize_factor/size), 1)*size)\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=new_size, \n",
    "        mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "    patches = torch.nn.functional.unfold(img.unsqueeze(0), \n",
    "        kernel_size=(size, size), \n",
    "        stride=(size, size)) # (1, 3*size*size, n_patches)\n",
    "    patches = patches.permute(0, 2, 1).reshape(-1, 3, size, size) # (n_patches, 3, size, size)\n",
    "    \n",
    "    # Remove patches with low difference between the 75th and 1st percentile\n",
    "    qranges = torch.quantile(patches.view(patches.shape[0], -1), 0.75, dim=-1) - \\\n",
    "              torch.quantile(patches.view(patches.shape[0], -1), 0.01, dim=-1)\n",
    "    non_empty_idx = torch.where(qranges >= quantile_range_threshold)[0]\n",
    "    # patches = TRANSFORM(patches[non_empty_idx])\n",
    "    # patches = patches.to(device)\n",
    "\n",
    "    non_empty_patch_indices_by_biopsy[idx] = non_empty_idx\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     patch_latents = model(patches) # (n_patches, num_ftrs)\n",
    "    # bag_latents[idx] = patch_latents.cpu()[:, None, :] # (n_patches, 1, num_ftrs)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        # torch.save(bag_latents, latent_file_backup)\n",
    "        torch.save(non_empty_patch_indices_by_biopsy, indices_filename)\n",
    "\n",
    "    # Clear memory\n",
    "    # del img, patches, patch_latents\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "# torch.save(bag_latents, latent_file)\n",
    "# torch.save(bag_latents, latent_file_backup)\n",
    "torch.save(non_empty_patch_indices_by_biopsy, indices_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_latents = torch.load(os.path.join(DATA_DIR, \"bag_latents_gs256_retccl.pt\"))\n",
    "\n",
    "print(type(bag_latents))\n",
    "print(len(bag_latents))\n",
    "print(bag_latents.keys())\n",
    "print(bag_latents[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PathXL dataset\n",
    "\n",
    "from load_data import BagDataset, P53_CLASS_NAMES\n",
    "\n",
    "size = 256\n",
    "quantile_range_threshold = 0.1\n",
    "resize_factor = 1.0\n",
    "root_dir = PATHXL_DIR\n",
    "\n",
    "model_name = \"retccl\"\n",
    "model_config = model_configs[model_name]\n",
    "model = model_config[\"model\"]\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "indices_filename = os.path.join(PATHXL_DIR, \"non_empty_patch_indices_gs256.pt\")\n",
    "if os.path.exists(indices_filename):\n",
    "    non_empty_patch_indices_by_biopsy = torch.load(indices_filename)\n",
    "else:\n",
    "    non_empty_patch_indices_by_biopsy = {}\n",
    "\n",
    "# Load latents\n",
    "latent_file = os.path.join(PATHXL_DIR, f\"bag_latents_gs{size}_{model_name}.pt\")\n",
    "if os.path.exists(latent_file):\n",
    "    bag_latents = torch.load(latent_file)\n",
    "else:\n",
    "    bag_latents = {}\n",
    "for i, row in tqdm(enumerate(pathxl_df.values), total=len(pathxl_df)):\n",
    "    idx = f\"{row[0]}_{row[1]}\"\n",
    "    if idx in bag_latents and idx in non_empty_patch_indices_by_biopsy:\n",
    "        continue\n",
    "\n",
    "    img_file = os.path.join(root_dir, \"biopsies\", f\"{idx}.png\")\n",
    "    if not os.path.exists(img_file):\n",
    "        print(f\"File {img_file} does not exist\")\n",
    "        continue\n",
    "    img = plt.imread(img_file)\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() # (3, h, w)\n",
    "\n",
    "    # Resize the img so that the height and width are multiples of grid_spacing, \n",
    "    # rounded to the nearest multiple.\n",
    "    # This prevents the last row and column of patches from being cut off.\n",
    "    # We also resize the image by the resize_factor instead of each patch separately\n",
    "    new_size = (max(round(img.shape[1]*resize_factor/size), 1)*size, \n",
    "                max(round(img.shape[2]*resize_factor/size), 1)*size)\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=new_size, \n",
    "        mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "    patches = torch.nn.functional.unfold(img.unsqueeze(0), \n",
    "        kernel_size=(size, size), \n",
    "        stride=(size, size)) # (1, 3*size*size, n_patches)\n",
    "    patches = patches.permute(0, 2, 1).reshape(-1, 3, size, size) # (n_patches, 3, size, size)\n",
    "    \n",
    "    # Remove patches with low difference between the 75th and 1st percentile\n",
    "    qranges = torch.quantile(patches.view(patches.shape[0], -1), 0.75, dim=-1) - \\\n",
    "              torch.quantile(patches.view(patches.shape[0], -1), 0.01, dim=-1)\n",
    "    non_empty_idx = torch.where(qranges >= quantile_range_threshold)[0]\n",
    "    patches = TRANSFORM(patches[non_empty_idx])\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    non_empty_patch_indices_by_biopsy[idx] = non_empty_idx\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_latents = model(patches) # (n_patches, num_ftrs)\n",
    "    bag_latents[idx] = patch_latents.cpu()[:, None, :] # (n_patches, 1, num_ftrs)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        torch.save(bag_latents, latent_file)\n",
    "        torch.save(non_empty_patch_indices_by_biopsy, indices_filename)\n",
    "\n",
    "    # Clear memory\n",
    "    del img, patches, patch_latents\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(bag_latents, latent_file)\n",
    "torch.save(non_empty_patch_indices_by_biopsy, indices_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
