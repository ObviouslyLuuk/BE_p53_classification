{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\321lu\\AppData\\Local\\Temp\\ipykernel_19316\\1849884700.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from load_data import BiopsyDataset, get_balanced_dataloader, Nullmutation\n",
    "from eval import get_accuracy_per_class\n",
    "\n",
    "DATA_DIR = \"../../data/biopsies_s1.0_anon_data/\"\n",
    "PATHXL_DIR = os.path.join(DATA_DIR, \"..\", \"p53_consensus_study\")\n",
    "BOLERO_DIR = os.path.join(DATA_DIR, \"..\", \"BOLERO\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Load csv\n",
    "csv_file = os.path.join(DATA_DIR, \"biopsy_labels_anon_s1.0.csv\")\n",
    "main_df = pd.read_csv(csv_file)\n",
    "\n",
    "pathxl_csv_file = os.path.join(PATHXL_DIR, \"labels.csv\")\n",
    "pathxl_df = pd.read_csv(pathxl_csv_file)\n",
    "\n",
    "bolero_files = os.listdir(os.path.join(BOLERO_DIR, \"biopsies\"))\n",
    "bolero_indices = [f.split(\".\")[0].split('_') for f in bolero_files]\n",
    "bolero_df = pd.DataFrame(bolero_indices, columns=[\"case\", \"biopsy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 512\n",
      "Number of features: 2048\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet18\n",
    "resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "# ResNet18 is pretrained on images with size 224x224\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "resnet18.fc = nn.Identity()\n",
    "\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
    "# ResNet50 is pretrained on images with size 224x224\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "resnet50.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "MODEL: ResNetModel\n",
      "MODEL ARGS: num_classes=4, latents_path=None, rotation_invariant=True, lr=0.001, weight_decay=0.0005, lr_step_size=30, lr_gamma=0.1\n"
     ]
    }
   ],
   "source": [
    "from resnet import ResNetModel\n",
    "checkpoint_path = os.path.join(DATA_DIR, \"..\", \"..\", \"models\", \"fb_spacing4\", \"acc0.74_epoch42_s4_e50_end-to-end_ks.ckpt\")\n",
    "# Load lightning model from checkpoint\n",
    "resnet18_tuned = ResNetModel.load_from_checkpoint(checkpoint_path).to(device)\n",
    "resnet18_tuned.eval()\n",
    "resnet18_tuned.model.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: ResNetModel\n",
      "MODEL ARGS: num_classes=4, lr=0.001, weight_decay=0.0005, lr_step_size=30, lr_gamma=0.1\n"
     ]
    }
   ],
   "source": [
    "from _resnet_patch import ResNetModel\n",
    "checkpoint_path = os.path.join(DATA_DIR, \"..\", \"..\", \"models\", \"_other\", \"acc0.73_epoch49_patch_end-to-end.ckpt\")\n",
    "# Load lightning model from checkpoint\n",
    "retccl_tuned = ResNetModel.load_from_checkpoint(checkpoint_path).to(device)\n",
    "retccl_tuned.eval()\n",
    "retccl_tuned.model.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 2048\n"
     ]
    }
   ],
   "source": [
    "from RetCCL.custom_objects import retccl_resnet50, HistoRetCCLResnet50_Weights\n",
    "\n",
    "retccl = retccl_resnet50(weights=HistoRetCCLResnet50_Weights.RetCCLWeights).to(device)\n",
    "# retccl is pretrained on images with size 256x256 at 1 micron per pixel\n",
    "num_ftrs = retccl.fc.in_features\n",
    "print(\"Number of features: {}\".format(num_ftrs))\n",
    "retccl.fc = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df:\n",
    "\tid\tlabel\n",
    "0\t0\t2\n",
    "1\t1\t2\n",
    "2\t2\t2\n",
    "3\t3\t2\n",
    "4\t4\t2\n",
    "...\t...\t...\n",
    "1527\t1527\t2\n",
    "1528\t1528\t2\n",
    "1529\t1529\t0\n",
    "1530\t1530\t0\n",
    "1531\t1531\t0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ImageNet normalization\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "TRANSFORM = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                      std=[0.229, 0.224, 0.225]) # What this does is it subtracts the mean from each channel and divides by the std\n",
    "])\n",
    "\n",
    "TRANSFORM_NORMAL = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                                      std=[0.5, 0.5, 0.5]) # What this does is it subtracts the mean from each channel and divides by the std\n",
    "])\n",
    "\n",
    "model_configs = {\n",
    "    \"resnet18\": {\n",
    "        \"model\": resnet18,\n",
    "        \"size\": 224,\n",
    "        \"num_ftrs\": 512,\n",
    "        \"transform\": TRANSFORM,\n",
    "        \"spacing\": 1.0, # Not actually fine-tuned on any spacing\n",
    "    },\n",
    "    \"resnet50\": {\n",
    "        \"model\": resnet50,\n",
    "        \"size\": 224,\n",
    "        \"num_ftrs\": 2048,\n",
    "        \"transform\": TRANSFORM,\n",
    "        \"spacing\": 1.0, # Not actually fine-tuned on any spacing\n",
    "    },\n",
    "    \"retccl\": {\n",
    "        \"model\": retccl,\n",
    "        \"size\": 256,\n",
    "        # \"size\": None,\n",
    "        \"num_ftrs\": 2048,\n",
    "        \"transform\": TRANSFORM,\n",
    "        \"spacing\": 1.0,\n",
    "    },\n",
    "    \"retccl_tuned\": {\n",
    "        \"model\": retccl_tuned,\n",
    "        \"size\": 256,\n",
    "        # \"size\": None,\n",
    "        \"num_ftrs\": 2048,\n",
    "        \"transform\": TRANSFORM,\n",
    "        \"spacing\": 1.0,\n",
    "    },\n",
    "    \"resnet18_tuned\": {\n",
    "        \"model\": resnet18_tuned, # Fine-tuned on FB dataset at 4mpp\n",
    "        \"size\": 64,\n",
    "        \"num_ftrs\": 512,\n",
    "        \"transform\": TRANSFORM_NORMAL,\n",
    "        \"spacing\": 4.0,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full biopsies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 4\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    if model_name != \"retccl\":\n",
    "        continue\n",
    "\n",
    "    model = model_config[\"model\"]\n",
    "    model.eval()\n",
    "    size = model_config[\"size\"]\n",
    "    if size == None:\n",
    "        transform = TRANSFORM\n",
    "        size = \"Full\"\n",
    "    else:\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((size, size)),\n",
    "            TRANSFORM\n",
    "        ])\n",
    "    num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "    # Open image from dataset\n",
    "    latents = torch.zeros((len(df), 4, num_ftrs)) # 4 is the number of rotations\n",
    "    latent_file = os.path.join(DATA_DIR, f\"latents_s{spacing}_{model_name}.pt\")\n",
    "    if os.path.exists(latent_file):\n",
    "        latents = torch.load(latent_file).detach()\n",
    "\n",
    "    for idx, label in tqdm(df.values):\n",
    "        if torch.any(latents[idx] != 0):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(DATA_DIR, \"biopsies\", f\"{idx}.png\")\n",
    "        img = plt.imread(img_path) # HxWxC\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "\n",
    "        new_size = (img.shape[1]//spacing, img.shape[2]//spacing)\n",
    "        # Apply resize transform\n",
    "        img = torchvision.transforms.functional.resize(img, new_size)\n",
    "\n",
    "        # Apply other transform\n",
    "        img = transform(img).unsqueeze(0).float().to(device)\n",
    "\n",
    "        # for i in range(4):\n",
    "        #     img_rot = torch.rot90(img, i, [2,3])\n",
    "        #     img_latent = model(img_rot)\n",
    "        #     latents[idx, i] = img_latent.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_latent = model(img)\n",
    "            latents[idx] = img_latent\n",
    "\n",
    "        torch.save(latents, latent_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1532/1532 [1:33:30<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from load_data import BagDataset, P53_CLASS_NAMES\n",
    "\n",
    "# model_name = \"resnet18_tuned\"\n",
    "model_name = \"retccl_tuned\"\n",
    "model_config = model_configs[model_name]\n",
    "model = model_config[\"model\"]\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "size = model_config[\"size\"]\n",
    "# size = 128\n",
    "resize_factor = 1 / model_config[\"spacing\"]\n",
    "transform = model_config[\"transform\"]\n",
    "quantile_range_threshold = 0.1\n",
    "root_dir = DATA_DIR\n",
    "\n",
    "df = main_df\n",
    "\n",
    "\n",
    "oe_patch_indices_file = os.path.join(root_dir, f\"oe_patch_indices_gs256.pt\")\n",
    "nm_patch_indices_file = os.path.join(root_dir, f\"nm_patch_indices_gs256.pt\")\n",
    "oe_patch_indices_by_biopsy = torch.load(oe_patch_indices_file)\n",
    "nm_patch_indices_by_biopsy = torch.load(nm_patch_indices_file)\n",
    "wt_patch_inclusion_p = 1/16\n",
    "\n",
    "\n",
    "num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "indices_filename = os.path.join(root_dir, \"non_empty_patch_indices_gs256.pt\")\n",
    "non_empty_patch_indices_by_biopsy = torch.load(indices_filename)\n",
    "\n",
    "# Load latents\n",
    "latent_file = os.path.join(root_dir, f\"bag_latents_gs{size}_{model_name}.pt\")\n",
    "# latent_file_backup = os.path.join(DATA_DIR, f\"bag_latents_gs{size}_{model_name}_backup.pt\")\n",
    "if os.path.exists(latent_file):\n",
    "    bag_latents = torch.load(latent_file)\n",
    "else:\n",
    "    bag_latents = {}\n",
    "for idx, label in tqdm(df.values):\n",
    "    if idx in bag_latents:\n",
    "        continue\n",
    "    # if idx in non_empty_patch_indices_by_biopsy:\n",
    "    #     continue\n",
    "\n",
    "    img_file = os.path.join(root_dir, \"biopsies\", f\"{idx}.png\")\n",
    "    img = plt.imread(img_file)\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() # (3, h, w)\n",
    "\n",
    "    # Resize the img so that the height and width are multiples of grid_spacing, \n",
    "    # rounded to the nearest multiple.\n",
    "    # This prevents the last row and column of patches from being cut off.\n",
    "    # We also resize the image by the resize_factor instead of each patch separately\n",
    "    new_size = (max(round(img.shape[1]*resize_factor/size), 1)*size, \n",
    "                max(round(img.shape[2]*resize_factor/size), 1)*size)\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=new_size, \n",
    "        mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "    patches = torch.nn.functional.unfold(img.unsqueeze(0), \n",
    "        kernel_size=(size, size), \n",
    "        stride=(size, size)) # (1, 3*size*size, n_patches)\n",
    "    patches = patches.permute(0, 2, 1).reshape(-1, 3, size, size) # (n_patches, 3, size, size)\n",
    "\n",
    "    non_empty_patch_indices = non_empty_patch_indices_by_biopsy[idx].tolist()\n",
    "\n",
    "    # # Save patch with wt inclusion probability\n",
    "    # for i, patch_idx in enumerate(non_empty_patch_indices):\n",
    "    #     if np.random.rand() > wt_patch_inclusion_p:\n",
    "    #         continue\n",
    "\n",
    "    #     patch = patches[patch_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "    #     patch = (patch * 255).astype(np.uint8)\n",
    "    #     patch_file = os.path.join(root_dir, \"patches\", f\"{idx}_{patch_idx}.png\")\n",
    "    #     # Convert to BGR\n",
    "    #     patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "    #     cv2.imwrite(patch_file, patch)\n",
    "\n",
    "    # oe_patch_indices = oe_patch_indices_by_biopsy[idx] if idx in oe_patch_indices_by_biopsy else []\n",
    "    # nm_patch_indices = nm_patch_indices_by_biopsy[idx] if idx in nm_patch_indices_by_biopsy else []\n",
    "    # oe_patch_indices = list(set(non_empty_patch_indices).intersection(oe_patch_indices))\n",
    "    # nm_patch_indices = list(set(non_empty_patch_indices).intersection(nm_patch_indices))\n",
    "    # # Take union of OE and NM patches\n",
    "    # oe_nm_patch_indices = list(set(oe_patch_indices).union(nm_patch_indices))\n",
    "    # # Save OE and NM patches\n",
    "    # for i, patch_idx in enumerate(oe_nm_patch_indices):\n",
    "    #     patch = patches[patch_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "    #     patch = (patch * 255).astype(np.uint8)\n",
    "    #     patch_file = os.path.join(root_dir, \"patches\", f\"{idx}_{patch_idx}.png\")\n",
    "    #     # Convert to BGR\n",
    "    #     patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "    #     cv2.imwrite(patch_file, patch)\n",
    "    \n",
    "    # Remove patches with low difference between the 75th and 1st percentile\n",
    "    qranges = torch.quantile(patches.view(patches.shape[0], -1), 0.75, dim=-1) - \\\n",
    "              torch.quantile(patches.view(patches.shape[0], -1), 0.01, dim=-1)\n",
    "    non_empty_idx = torch.where(qranges >= quantile_range_threshold)[0]\n",
    "    patches = transform(patches[non_empty_idx])\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    # non_empty_patch_indices_by_biopsy[idx] = non_empty_idx\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_latents = model(patches) # (n_patches, num_ftrs)\n",
    "    bag_latents[idx] = patch_latents.cpu()[:, None, :] # (n_patches, 1, num_ftrs)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        torch.save(bag_latents, latent_file)\n",
    "        # torch.save(bag_latents, latent_file_backup)\n",
    "    #     torch.save(non_empty_patch_indices_by_biopsy, indices_filename)\n",
    "\n",
    "    # Clear memory\n",
    "    del img, patches, patch_latents\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(bag_latents, latent_file)\n",
    "# torch.save(bag_latents, latent_file_backup)\n",
    "# torch.save(non_empty_patch_indices_by_biopsy, indices_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_latents = torch.load(os.path.join(DATA_DIR, \"bag_latents_gs256_retccl.pt\"))\n",
    "\n",
    "print(type(bag_latents))\n",
    "print(len(bag_latents))\n",
    "print(bag_latents.keys())\n",
    "print(bag_latents[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/226 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [12:37<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# PathXL dataset\n",
    "\n",
    "from load_data import BagDataset, P53_CLASS_NAMES\n",
    "\n",
    "size = 256\n",
    "quantile_range_threshold = 0.1\n",
    "# root_dir = PATHXL_DIR\n",
    "# df = pathxl_df\n",
    "root_dir = BOLERO_DIR\n",
    "df = bolero_df\n",
    "\n",
    "model_name = \"retccl_tuned\"\n",
    "model_config = model_configs[model_name]\n",
    "resize_factor = 1 / model_config[\"spacing\"]\n",
    "transform = model_config[\"transform\"]\n",
    "model = model_config[\"model\"]\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "num_ftrs = model_config[\"num_ftrs\"]\n",
    "\n",
    "# indices_filename = os.path.join(PATHXL_DIR, \"non_empty_patch_indices_gs256.pt\")\n",
    "# if os.path.exists(indices_filename):\n",
    "#     non_empty_patch_indices_by_biopsy = torch.load(indices_filename)\n",
    "# else:\n",
    "#     non_empty_patch_indices_by_biopsy = {}\n",
    "\n",
    "# Load latents\n",
    "latent_file = os.path.join(root_dir, f\"bag_latents_gs{size}_{model_name}.pt\")\n",
    "if os.path.exists(latent_file):\n",
    "    bag_latents = torch.load(latent_file)\n",
    "else:\n",
    "    bag_latents = {}\n",
    "for i, row in tqdm(enumerate(df.values), total=len(df)):\n",
    "    idx = f\"{row[0]}_{row[1]}\"\n",
    "    # if idx in bag_latents and idx in non_empty_patch_indices_by_biopsy:\n",
    "    #     continue\n",
    "    if idx in bag_latents:\n",
    "        continue\n",
    "\n",
    "    img_file = os.path.join(root_dir, \"biopsies\", f\"{idx}.png\")\n",
    "    if not os.path.exists(img_file):\n",
    "        print(f\"File {img_file} does not exist\")\n",
    "        continue\n",
    "    img = plt.imread(img_file)\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() # (3, h, w)\n",
    "\n",
    "    # Resize the img so that the height and width are multiples of grid_spacing, \n",
    "    # rounded to the nearest multiple.\n",
    "    # This prevents the last row and column of patches from being cut off.\n",
    "    # We also resize the image by the resize_factor instead of each patch separately\n",
    "    new_size = (max(round(img.shape[1]*resize_factor/size), 1)*size, \n",
    "                max(round(img.shape[2]*resize_factor/size), 1)*size)\n",
    "    img = torch.nn.functional.interpolate(img.unsqueeze(0), size=new_size, \n",
    "        mode='bilinear', align_corners=False)[0]\n",
    "\n",
    "    patches = torch.nn.functional.unfold(img.unsqueeze(0), \n",
    "        kernel_size=(size, size), \n",
    "        stride=(size, size)) # (1, 3*size*size, n_patches)\n",
    "    patches = patches.permute(0, 2, 1).reshape(-1, 3, size, size) # (n_patches, 3, size, size)\n",
    "    \n",
    "    # Remove patches with low difference between the 75th and 1st percentile\n",
    "    qranges = torch.quantile(patches.view(patches.shape[0], -1), 0.75, dim=-1) - \\\n",
    "              torch.quantile(patches.view(patches.shape[0], -1), 0.01, dim=-1)\n",
    "    non_empty_idx = torch.where(qranges >= quantile_range_threshold)[0]\n",
    "    patches = transform(patches[non_empty_idx])\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    # non_empty_patch_indices_by_biopsy[idx] = non_empty_idx\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_latents = model(patches) # (n_patches, num_ftrs)\n",
    "    bag_latents[idx] = patch_latents.cpu()[:, None, :] # (n_patches, 1, num_ftrs)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        torch.save(bag_latents, latent_file)\n",
    "        # torch.save(non_empty_patch_indices_by_biopsy, indices_filename)\n",
    "\n",
    "    # Clear memory\n",
    "    del img, patches, patch_latents\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.save(bag_latents, latent_file)\n",
    "# torch.save(non_empty_patch_indices_by_biopsy, indices_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test how padding affects the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_configs[\"resnet18\"][\"model\"]\n",
    "\n",
    "img_path = os.path.join(DATA_DIR, \"biopsies\", f\"{df.iloc[0,0]}.png\")\n",
    "img = plt.imread(img_path) # HxWxC\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = torch.tensor(img).permute(2,0,1).unsqueeze(0).float().to(device)\n",
    "img_latent = model(img)\n",
    "img_latent_padded = {}\n",
    "pad = 0\n",
    "if img.shape[2] != img.shape[3]:\n",
    "    pad = abs(img.shape[2] - img.shape[3]) // 2\n",
    "for pad_value in [0, 0.5, 1]:\n",
    "    if img.shape[2] > img.shape[3]:\n",
    "        img_pad = torch.nn.functional.pad(img, (pad, pad,0,0), value=pad_value)\n",
    "    else:\n",
    "        img_pad = torch.nn.functional.pad(img, (0,0,pad,pad), value=pad_value)\n",
    "\n",
    "    # Show again\n",
    "    show_img = img_pad.squeeze().permute(1,2,0).cpu().numpy()\n",
    "    plt.imshow(show_img)\n",
    "    plt.show()\n",
    "\n",
    "    img_latent_padded[pad_value] = model(img_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_latent_pad in img_latent_padded.values():\n",
    "    # print(img_latent_pad - img_latent)\n",
    "\n",
    "    # Print MSE between the two\n",
    "    print(torch.nn.functional.mse_loss(img_latent, img_latent_pad))\n",
    "\n",
    "    # Did the padding change the latent representation?\n",
    "    print(torch.allclose(img_latent, img_latent_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix retccl and retccl tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open retccl bag latents\n",
    "root_dir = BOLERO_DIR\n",
    "bag_latents_file_retccl = os.path.join(root_dir, \"bag_latents_gs256_retccl.pt\")\n",
    "bag_latents_retccl = torch.load(bag_latents_file_retccl)\n",
    "bag_latents_file_retccl_tuned = os.path.join(root_dir, \"bag_latents_gs256_retccl_tuned.pt\")\n",
    "bag_latents_retccl_tuned = torch.load(bag_latents_file_retccl_tuned)\n",
    "\n",
    "# Avg the two into a new dict: bag_latents_retccl_hybrid\n",
    "bag_latents_retccl_hybrid = {}\n",
    "for idx in bag_latents_retccl_tuned:\n",
    "    slide_idx = int(idx.split(\"_\")[0])\n",
    "    biopsy_idx = int(idx.split(\"_\")[1])\n",
    "    non_tuned = bag_latents_retccl[slide_idx][biopsy_idx+1].cpu().unsqueeze(1)\n",
    "    tuned = bag_latents_retccl_tuned[idx]\n",
    "    bag_latents_retccl_hybrid[idx] = (non_tuned + tuned) / 2\n",
    "\n",
    "# Save the hybrid latents\n",
    "bag_latents_file_retccl_hybrid = os.path.join(root_dir, \"bag_latents_gs256_retccl_tuned_hybrid.pt\")\n",
    "torch.save(bag_latents_retccl_hybrid, bag_latents_file_retccl_hybrid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
