{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from load_data import DATA_DIR, TRANSFORMS, P53_CLASS_NAMES, \\\n",
    "    convert_presence_probs_to_status_probs\n",
    "from resnet import ResNetModel, ResNetModelDoubleBinary\n",
    "# from resnet_patch import ResNetModelDoubleBinary as ResNetModelDoubleBinaryPatch\n",
    "from pl_clam import CLAM_MB, CLAM_db\n",
    "\n",
    "P53_CLASS_CODES = [\"WT\", \"OE\", \"NM\", \"DC\"]\n",
    "\n",
    "BOLERO_DIR = os.path.join(DATA_DIR, '..', 'BOLERO')\n",
    "PATHXL_DIR = os.path.join(DATA_DIR, '..', 'p53_consensus_study')\n",
    "\n",
    "BASE_DIR = {\n",
    "    'test': DATA_DIR,\n",
    "    'bolero': BOLERO_DIR,\n",
    "    'pathxl': PATHXL_DIR\n",
    "}\n",
    "\n",
    "RESULTS_DIR = os.path.join(DATA_DIR, '..', '..', 'results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "bag_latent_paths = {\n",
    "    \"test\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs256_retccl__backup.pt\"),\n",
    "    \"bolero\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "    \"pathxl\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs256_retccl.pt\"),\n",
    "    \"test_resnet18t_64\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs64_resnet18_tuned.pt\"),\n",
    "    \"bolero_resnet18t_64\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs64_resnet18_tuned.pt\"),\n",
    "    \"pathxl_resnet18t_64\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs64_resnet18_tuned.pt\"),\n",
    "    \"test_resnet18t_128\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs128_resnet18_tuned.pt\"),\n",
    "    \"bolero_resnet18t_128\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs128_resnet18_tuned.pt\"),\n",
    "    \"pathxl_resnet18t_128\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs128_resnet18_tuned.pt\"),\n",
    "    \"test_retccl-t\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs256_retccl_tuned.pt\"),\n",
    "    \"bolero_retccl-t\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs256_retccl_tuned.pt\"),\n",
    "    \"pathxl_retccl-t\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs256_retccl_tuned.pt\"),\n",
    "    \"test_retccl-h\":   os.path.join(BASE_DIR[\"test\"], \"bag_latents_gs256_retccl_tuned_hybrid.pt\"),\n",
    "    \"bolero_retccl-h\": os.path.join(BASE_DIR[\"bolero\"], \"bag_latents_gs256_retccl_tuned_hybrid.pt\"),\n",
    "    \"pathxl_retccl-h\": os.path.join(BASE_DIR[\"pathxl\"], \"bag_latents_gs256_retccl_tuned_hybrid.pt\"),\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "color_dict = {\n",
    "    \"r\": '#DA4C4C', # Red\n",
    "    \"o\": '#E57439', # Orange\n",
    "    \"y\": '#EDB732', # Yellow\n",
    "    \"g\": '#479A5F', # Green\n",
    "    \"lb\": '#5BC5DB', # Light blue\n",
    "    \"b\": '#5387DD', # Blue\n",
    "    \"p\": '#7D54B2', # Purple\n",
    "    \"pi\": '#E87B9F', # Pink\n",
    "#  '#229487', # Dark green/Turquoise\n",
    "#  '#C565C7', # Lilac\n",
    "    \"r_p\": '#E89393', # Pale red\n",
    "    \"o_p\": '#EFAB88', # Pale orange\n",
    "    \"y_p\": '#F4D384', # Pale yellow\n",
    "    \"g_p\": '#90C29F', # Pale green\n",
    "    \"lb_p\":'#9CDCE9', # Pale light blue\n",
    "    \"b_p\": '#98B7EA', # Pale blue\n",
    "    \"p_p\": \"#B198D0\", # Pale purple\n",
    "}\n",
    "colors = list(color_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = os.path.join(DATA_DIR, '..', '..', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb: full biopsy, db: double binary, gs: grid spacing\n",
    "model_kwargs = {\n",
    "    \"CLAM\":     {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db\":  {\"model_class\": CLAM_db, \"gs\": 256},\n",
    "    \"CLAM_m\":   {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_db_m\":{\"model_class\": CLAM_db, \"gs\": 256},\n",
    "\n",
    "    \"CLAM_ResNet18_tuned_64\": {\"model_class\": CLAM_MB, \"gs\": 64},\n",
    "    # \"CLAM_ResNet18_tuned_128\": {\"model_class\": CLAM_MB, \"gs\": 128},\n",
    "    \"CLAM_retccl_tuned\": {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "    \"CLAM_retccl_tuned_hybrid\": {\"model_class\": CLAM_MB, \"gs\": 256},\n",
    "}\n",
    "for name in [\"fb_db\", \"fb\"]:\n",
    "    # for size in [256, 1024]:\n",
    "    #     model_name = f\"{name}_size{size}\"\n",
    "    #     model_kwargs[model_name] = {\"size\": size}\n",
    "    for spacing in [2, 4, 8, 16, 32, 64, 128, 256]:\n",
    "        if \"db\" not in name and spacing == 2: # Skipped this one because it's too slow\n",
    "            continue\n",
    "        model_name = f\"{name}_spacing{spacing}\"\n",
    "        model_kwargs[model_name] = {\"spacing\": spacing}\n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"fb_db\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModelDoubleBinary\n",
    "    elif \"fb\" in model_name:\n",
    "        model_kwargs[model_name][\"model_class\"] = ResNetModel\n",
    "    checkpoint_dir = os.path.join(MODELS_DIR, model_name)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    model_kwargs[model_name][\"checkpoint_paths\"] = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n",
    "\n",
    "def load_model(model_class, checkpoint_path, **kwargs):\n",
    "    if \"CLAM\" in model_class.__name__: # For some reason pl can't load these models with load_from_checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        # model = model_class(**checkpoint[\"model_kwargs\"])\n",
    "        model = model_class(**kwargs)\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else:\n",
    "        model = model_class.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many checkpoints we have for each model\n",
    "for model_name in model_kwargs:\n",
    "    print(f\"{model_name}: {len(model_kwargs[model_name]['checkpoint_paths'])} checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patch_latents(img_name, data_name, bag_latents):\n",
    "    if data_name == \"test\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)\n",
    "    elif data_name == \"bolero\":\n",
    "        if not type(list(bag_latents.keys())[0]) == int:\n",
    "            return bag_latents[img_name].squeeze(1)\n",
    "        else:\n",
    "            slide_name, biopsy_name = tuple(img_name.split(\"_\"))\n",
    "            slide_latents = bag_latents[int(slide_name)] # (n_biopsies, N, 2048)\n",
    "            return slide_latents[int(biopsy_name)+1] # (N, 2048)\n",
    "    elif data_name == \"pathxl\":\n",
    "        return bag_latents[img_name].squeeze(1) # (N, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_name, data_name):\n",
    "    img_path = os.path.join(BASE_DIR[data_name], 'biopsies', f\"{img_name}.png\")\n",
    "    img = plt.imread(img_path) # (H, W, C) float32\n",
    "    return img\n",
    "    \n",
    "def get_labels(data_name):\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv')).set_index(\"id\").to_dict(orient='dict')['label']\n",
    "    elif data_name == \"bolero\":\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[data_name], 'P53_BOLERO_T.csv'))\n",
    "        labels = labels.sort_values(by=\"Case ID\")\n",
    "        labels = labels.reset_index(drop=True)\n",
    "        # Map GS to {1:0, 2:1, 3:2, 4:4} where 4 is unknown\n",
    "        labels[\"GS\"] = labels[\"GS\"].map({1:0, 2:1, 3:2, 4:4})\n",
    "        # Only keep GS column\n",
    "        labels = labels[[\"GS\"]].to_dict(orient='dict')[\"GS\"]\n",
    "        return labels\n",
    "    elif data_name.startswith(\"pathxl\"):\n",
    "        labels = pd.read_csv(os.path.join(BASE_DIR[\"pathxl\"], 'labels.csv'))\n",
    "        # idx is id column and biopsy_nr column separated by _\n",
    "        labels[\"idx\"] = labels[\"id\"].astype(str) + \"_\" + labels[\"biopsy_nr\"].astype(str)\n",
    "        labels = labels.set_index(\"idx\")\n",
    "        # Sort by id primarily and biopsy_nr secondarily\n",
    "        labels = labels.sort_values(by=[\"id\", \"biopsy_nr\"])\n",
    "        # Map label\n",
    "        mapping = {\"WT\":0, \"Overexpression\":1, \"Null\":2, \"Double clones\":3}\n",
    "        labels[\"label\"] = labels[\"label\"].map(mapping)\n",
    "        if data_name == \"pathxl\": # Filter out any concordance % < 75\n",
    "            labels = labels[labels[\"concordance %\"] >= 75]\n",
    "        elif data_name == \"pathxl-100\":\n",
    "            labels = labels[labels[\"concordance %\"] == 100]\n",
    "        labels = labels[[\"label\"]].to_dict(orient='dict')[\"label\"]\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl\"))\n",
    "        return labels\n",
    "    elif data_name == \"test+pathxl-100\":\n",
    "        labels = get_labels(\"test\")\n",
    "        labels.update(get_labels(\"pathxl-100\"))\n",
    "        return labels\n",
    "\n",
    "\n",
    "def call_constant_size(model, img_name, data_name, size, **kwargs):\n",
    "    img = load_img(img_name, data_name)\n",
    "    img = torch.nn.functional.interpolate(torch.tensor(img).permute(2, 0, 1).unsqueeze(0), size=(size, size), mode='bilinear')\n",
    "    img = TRANSFORMS['normalize'](img)\n",
    "    with torch.no_grad():\n",
    "        return model(img.to(device)).cpu().detach().numpy()\n",
    "\n",
    "def call_constant_spacing(model, img_name, data_name, spacing, **kwargs):\n",
    "    img = load_img(img_name, data_name)\n",
    "    h = img.shape[0] // spacing\n",
    "    w = img.shape[1] // spacing\n",
    "    img = torch.nn.functional.interpolate(torch.tensor(img).permute(2, 0, 1).unsqueeze(0), size=(h, w), mode='bilinear')\n",
    "    img = TRANSFORMS['normalize'](img)\n",
    "    with torch.no_grad():\n",
    "        return model(img.to(device)).cpu().detach().numpy()\n",
    "\n",
    "def call_CLAM(model, img_name, data_name, bag_latents, **kwargs):\n",
    "    patch_latents = load_patch_latents(img_name, data_name, bag_latents).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits, Y_prob, Y_hat, A_raw, results_dict = model(patch_latents.to(device))\n",
    "        return Y_prob.cpu().detach().numpy(), A_raw.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "for model_name in model_kwargs:\n",
    "    if \"size\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"call\"] = call_constant_size\n",
    "    elif \"spacing\" in model_kwargs[model_name]:\n",
    "        model_kwargs[model_name][\"call\"] = call_constant_spacing\n",
    "    elif \"CLAM\" in model_name:\n",
    "        model_kwargs[model_name][\"call\"] = call_CLAM\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown call type for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_names(data_name='test'):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        return get_img_names(\"test\") + get_img_names(\"pathxl\")\n",
    "    if data_name == \"test\":\n",
    "        return pd.read_csv(os.path.join(BASE_DIR[data_name], 'test.csv'))[\"id\"].tolist()\n",
    "    return [img_path.split('.')[0] for img_path in os.listdir(os.path.join(BASE_DIR[data_name], 'biopsies'))]\n",
    "\n",
    "def get_results(model, mdl_kwargs, data_name='test', bag_latents=None):\n",
    "    img_names = get_img_names(data_name)\n",
    "    results = {}\n",
    "    for img_name in tqdm(img_names):\n",
    "        results[img_name] = mdl_kwargs[\"call\"](model, img_name, data_name, bag_latents=bag_latents, **mdl_kwargs)\n",
    "    return results\n",
    "\n",
    "def save_results(results, model_name, data_name, checkpoint_name):\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "    torch.save(results, results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take intersection of img_names and labels\n",
    "img_names = get_img_names('pathxl')\n",
    "print(len(img_names), \"files. already doesn't have the missing sme cases (22 biop) and the other one (2 biop)\")\n",
    "labels = get_labels('pathxl')\n",
    "print(len(pd.read_csv(os.path.join(BASE_DIR[\"pathxl\"], 'labels.csv'))), \"labels. already doesn't have the missing one case (2 biop)\")\n",
    "print(len(labels), \"75+ concordance labels. excludes 53 low concordance biop, 3 of which are sme missing\")\n",
    "img_names = [img_name for img_name in img_names if img_name in labels]\n",
    "print(len(img_names), \"files after intersection. excludes the remaining 19 sme missing out of the 22\")\n",
    "print()\n",
    "# Now for pathxl-100\n",
    "img_names = get_img_names('pathxl')\n",
    "print(len(img_names), \"files. already doesn't have the missing sme cases (22 biop) and the other one (2 biop)\")\n",
    "labels = get_labels('pathxl-100')\n",
    "print(len(pd.read_csv(os.path.join(BASE_DIR[\"pathxl\"], 'labels.csv'))), \"labels. already doesn't have the missing one case (2 biop)\")\n",
    "print(len(labels), \"100%-concordance labels. excludes 53 low concordance biop, 3 of which are sme missing, and an additional 114, 6 of which are missing\")\n",
    "img_names = [img_name for img_name in img_names if img_name in labels]\n",
    "print(len(img_names), \"files after intersection. excludes the remaining 13 sme missing out of the 22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name in [\"test\", \"bolero\", \"pathxl\"]:\n",
    "    print(f\"\\nRUNNING {data_name.upper()}\")\n",
    "    bag_latents = torch.load(bag_latent_paths[data_name], map_location=device)\n",
    "    for model_name, mdl_kwargs in model_kwargs.items():\n",
    "        if \"tuned\" in model_name:\n",
    "            continue\n",
    "        for i, checkpoint_path in enumerate(mdl_kwargs[\"checkpoint_paths\"][:]):\n",
    "            checkpoint_name = os.path.basename(checkpoint_path).replace(\".ckpt\", \"\")\n",
    "            results_path = os.path.join(RESULTS_DIR, model_name, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "\n",
    "            if os.path.exists(results_path):\n",
    "                print(f\"Already done {model_name}\")\n",
    "                continue\n",
    "            print(f\"Running {model_name} {i}\")\n",
    "            model = load_model(mdl_kwargs[\"model_class\"], checkpoint_path)\n",
    "            results = get_results(model, mdl_kwargs, data_name=data_name, bag_latents=bag_latents)\n",
    "            save_results(results, model_name, data_name=data_name, checkpoint_name=checkpoint_name)\n",
    "\n",
    "\n",
    "# Just for resnet18_tuned\n",
    "for data_name in [\"test\", \"pathxl\", \"bolero\"]:\n",
    "    print(f\"\\nRUNNING {data_name.upper()}\")\n",
    "    for model_name, mdl_kwargs in model_kwargs.items():\n",
    "        if \"ResNet18_tuned\" not in model_name:\n",
    "            continue\n",
    "        gs = mdl_kwargs[\"gs\"]\n",
    "        bag_latents = torch.load(bag_latent_paths[data_name+f\"_resnet18t_{gs}\"], map_location=device)\n",
    "        for i, checkpoint_path in enumerate(mdl_kwargs[\"checkpoint_paths\"][:]):\n",
    "            checkpoint_name = os.path.basename(checkpoint_path).replace(\".ckpt\", \"\")\n",
    "            results_path = os.path.join(RESULTS_DIR, model_name, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "\n",
    "            if os.path.exists(results_path):\n",
    "                print(f\"Already done {model_name}\")\n",
    "                continue\n",
    "            print(f\"Running {model_name} {i}\")\n",
    "            model = load_model(mdl_kwargs[\"model_class\"], checkpoint_path, encoding_size=512)\n",
    "            results = get_results(model, mdl_kwargs, data_name=data_name, bag_latents=bag_latents)\n",
    "            save_results(results, model_name, data_name=data_name, checkpoint_name=checkpoint_name)\n",
    "\n",
    "# Just for retccl-tuned\n",
    "for data_name in [\"test\", \"pathxl\", \"bolero\"]:\n",
    "    print(f\"\\nRUNNING {data_name.upper()}\")\n",
    "    for model_name, mdl_kwargs in model_kwargs.items():\n",
    "        if \"retccl_tuned\" not in model_name:\n",
    "            continue\n",
    "        hybrid = \"h\" if \"hybrid\" in model_name else \"t\"\n",
    "        bag_latents = torch.load(bag_latent_paths[data_name+f\"_retccl-{hybrid}\"], map_location=device)\n",
    "        for i, checkpoint_path in enumerate(mdl_kwargs[\"checkpoint_paths\"][:]):\n",
    "            checkpoint_name = os.path.basename(checkpoint_path).replace(\".ckpt\", \"\")\n",
    "            results_path = os.path.join(RESULTS_DIR, model_name, f\"{data_name}_{checkpoint_name}.pt\")\n",
    "\n",
    "            if os.path.exists(results_path):\n",
    "                print(f\"Already done {model_name}\")\n",
    "                continue\n",
    "            print(f\"Running {model_name} {i}\")\n",
    "            model = load_model(mdl_kwargs[\"model_class\"], checkpoint_path)\n",
    "            results = get_results(model, mdl_kwargs, data_name=data_name, bag_latents=bag_latents)\n",
    "            save_results(results, model_name, data_name=data_name, checkpoint_name=checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make synthetic DC from combinations of OE and NM bags of latents\n",
    "for data_name in [\"test\", \"pathxl\"]:\n",
    "    print(f\"\\nRUNNING {data_name.upper()}\")\n",
    "    bag_latents = torch.load(bag_latent_paths[data_name], map_location=device)\n",
    "    labels = get_labels(data_name)\n",
    "    # Only select indices that are also in img_names\n",
    "    img_names = get_img_names(data_name)\n",
    "    labels = {idx: label for idx, label in labels.items() if idx in img_names}\n",
    "    oe_indices = [idx for idx, label in labels.items() if label == 1]\n",
    "    nm_indices = [idx for idx, label in labels.items() if label == 2]\n",
    "    synth_dc_indices = []\n",
    "    for oe_idx in oe_indices:\n",
    "        for nm_idx in nm_indices:\n",
    "            synth_dc_indices.append((oe_idx, nm_idx))\n",
    "    for model_name, mdl_kwargs in model_kwargs.items():\n",
    "        if model_name in [\"CLAM\", \"CLAM_ResNet18_tuned\"] or \"CLAM\" not in model_name: # Only works with patch-based models but not CLAM itself\n",
    "            continue\n",
    "        for i, checkpoint_path in enumerate(mdl_kwargs[\"checkpoint_paths\"][:]):\n",
    "            checkpoint_name = os.path.basename(checkpoint_path).replace(\".ckpt\", \"\")\n",
    "            results_path = os.path.join(RESULTS_DIR, model_name + \"_synth\", f\"{data_name}_synth_dc_{checkpoint_name}.pt\")\n",
    "\n",
    "            if os.path.exists(results_path):\n",
    "                print(f\"Already done {model_name}\")\n",
    "                continue\n",
    "            print(f\"Running {model_name} {i}\")\n",
    "            model = load_model(mdl_kwargs[\"model_class\"], checkpoint_path)\n",
    "            results = {}\n",
    "            for oe_idx, nm_idx in tqdm(synth_dc_indices):\n",
    "                oe_latents = load_patch_latents(oe_idx, data_name, bag_latents)\n",
    "                nm_latents = load_patch_latents(nm_idx, data_name, bag_latents)\n",
    "                synth_dc_latents = torch.cat([oe_latents, nm_latents], dim=0).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    logits, Y_prob, Y_hat, A_raw, results_dict = model(synth_dc_latents.to(device))\n",
    "                    results[(oe_idx, nm_idx)] = (Y_prob.cpu().detach().numpy(), A_raw.cpu().detach().numpy())\n",
    "            save_results(results, model_name, data_name=data_name + \"_synth_dc\", checkpoint_name=checkpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Metrics\n",
    "Load results and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fb_db   has a dict like idx: shape (1,2) with the two mutation probabilities\n",
    "fb      has a dict like idx: shape (1,4) with the four class probabilities\n",
    "CLAM_db has a dict like idx: tuple of: (\n",
    "            shape (1,2) with the two mutation probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "    )\n",
    "CLAM    has a dict like idx: tuple of: (\n",
    "            shape (1,4) with the four class probabilities,\n",
    "            shape (2, n_patches) Attention map \n",
    "        )\n",
    "\"\"\"\n",
    "model_type_result_keys = {\n",
    "    \"fb_db\": [\"presence_probs\"],\n",
    "    \"fb\": [\"status_probs\"],\n",
    "    \"CLAM_db\": [\"presence_probs\", \"A_raw\"],\n",
    "    \"CLAM\": [\"status_probs\", \"A_raw\"],\n",
    "}\n",
    "def get_result_keys(model_name):\n",
    "    for key in model_type_result_keys: # The order is important\n",
    "        if key in model_name:\n",
    "            return model_type_result_keys[key]\n",
    "\n",
    "def load_results(model_name, data_name):\n",
    "    if data_name.startswith(\"test+pathxl\"):\n",
    "        results = defaultdict(lambda: defaultdict(dict))\n",
    "        for data_name in [\"test\", \"pathxl\"]:\n",
    "            results_data = load_results(model_name, data_name)\n",
    "            for key in results_data:\n",
    "                results[key].update(results_data[key])\n",
    "        return results\n",
    "\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "    for i, checkpoint_name in enumerate([f for f in os.listdir(results_dir) if f.startswith(data_name)]):\n",
    "        results_path = os.path.join(results_dir, checkpoint_name)\n",
    "        result_content = torch.load(results_path)\n",
    "        checkpoint_name = checkpoint_name.replace(f\"{data_name}_\", \"\").replace(\".pt\", \"\")\n",
    "        if \"CLAM_db\" in model_name:\n",
    "            for img_name, (presence_probs, A_raw) in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"CLAM\" in model_name:\n",
    "            for img_name, (status_probs, A_raw) in result_content.items():\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"A_raw\"][img_name][checkpoint_name] = A_raw\n",
    "        elif \"fb_db\" in model_name:\n",
    "            for img_name, presence_probs in result_content.items():\n",
    "                results[\"presence_probs\"][img_name][checkpoint_name] = presence_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = convert_presence_probs_to_status_probs(torch.tensor(presence_probs)).numpy()\n",
    "        elif \"fb\" in model_name:\n",
    "            for img_name, status_probs in result_content.items():\n",
    "                # results[\"status_probs\"][img_name][checkpoint_name] = status_probs\n",
    "                results[\"status_probs\"][img_name][checkpoint_name] = torch.nn.functional.softmax(torch.tensor(status_probs), dim=1).numpy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type {model_name}\")\n",
    "    return results\n",
    "\n",
    "    \n",
    "# load_results(\"fb_spacing4\", \"test\")[\"status_probs\"]\n",
    "# get_labels(\"test+pathxl-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get accuracy, confusion matrices, roc curves, area under roc curve\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import auc as calc_auc\n",
    "\n",
    "def get_accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def get_roc_curves(y_true, y_score, multi_class=True):\n",
    "    if multi_class:\n",
    "        # Get fpr, tpr for each of the four classes\n",
    "        fpr, tpr = {}, {}\n",
    "        for i in range(4):\n",
    "            if np.sum(y_true == i) == 0:\n",
    "                continue\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true == i, y_score[:, i])\n",
    "        return fpr, tpr\n",
    "    else:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        return fpr, tpr\n",
    "\n",
    "def get_auc(fpr, tpr, multi_class=True):\n",
    "    if multi_class:\n",
    "        # Get auc for each of the four classes\n",
    "        auc = {}\n",
    "        for i in range(4):\n",
    "            if i in fpr:\n",
    "                auc[i] = calc_auc(fpr[i], tpr[i])\n",
    "        return auc\n",
    "    else:\n",
    "        return calc_auc(fpr, tpr)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_score, multi_class=True):\n",
    "    accuracy = get_accuracy(y_true, y_pred)\n",
    "    conf_matrix = get_confusion_matrix(y_true, y_pred)\n",
    "    fpr, tpr = get_roc_curves(y_true, y_score, multi_class=multi_class)\n",
    "    auc = get_auc(fpr, tpr, multi_class=multi_class)\n",
    "    if multi_class:\n",
    "        mean_auc = np.nanmean(list(auc.values()))\n",
    "    else:\n",
    "        mean_auc = auc\n",
    "    return accuracy, conf_matrix, fpr, tpr, auc, mean_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = defaultdict(lambda: defaultdict(dict)) # data_name -> model_name -> checkpoint_name -> metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name in [\"test\", \"test+pathxl-100\"]:\n",
    "    print(f\"\\nMETRICS FOR {data_name.upper()}\")\n",
    "    labels = get_labels(data_name)\n",
    "    # Take the intersection of the image names (some images might be missing in the labels file or the input data)\n",
    "    img_names_input = get_img_names(data_name)\n",
    "    img_names_label = list(labels.keys())\n",
    "    img_names = list(set(img_names_input).intersection(img_names_label))\n",
    "    for model_name in tqdm(model_kwargs):\n",
    "        # Check if we have results for this model\n",
    "        results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "        if (not os.path.exists(results_dir) or not any([f.startswith(data_name) for f in os.listdir(results_dir)])) and not data_name.startswith(\"test+pathxl\"):\n",
    "            continue\n",
    "\n",
    "        results = load_results(model_name, data_name)\n",
    "        for checkpoint_name in list(results[\"status_probs\"].values())[0].keys():\n",
    "            status_probs = np.array([results[\"status_probs\"][img_name][checkpoint_name][0] for img_name in img_names]) # (N, 4)\n",
    "            y_pred = np.argmax(status_probs, axis=1)\n",
    "            y_true = np.array([labels[img_name] for img_name in img_names])\n",
    "            accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "\n",
    "            # Also calculate these if we compare WT to Mutated\n",
    "            # Sum the mutated classes 1,2,3\n",
    "            status_probs_mutated = np.array([np.sum(results[\"status_probs\"][img_name][checkpoint_name][0][1:]) for img_name in img_names]) # (N,)\n",
    "            y_pred_mutated = status_probs_mutated > 0.5\n",
    "            y_true_mutated = y_true > 0\n",
    "            accuracy_m, conf_matrix_m, fpr_m, tpr_m, auc_m,_ = calculate_metrics(y_true_mutated, y_pred_mutated, status_probs_mutated.reshape(-1, 1), multi_class=False)\n",
    "            # # Just don't count doubleclones\n",
    "            # status_probs_mutated = np.array([results[\"status_probs\"][img_name][checkpoint_name][0][:2] for img_name in img_names]) # (N, 3)\n",
    "            # y_pred = np.argmax(status_probs, axis=1)\n",
    "            # y_true = np.array([labels[img_name] for img_name in img_names])\n",
    "            # accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "\n",
    "            model_results[data_name][model_name][checkpoint_name] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"conf_matrix\": conf_matrix,\n",
    "                \"fpr\": fpr,\n",
    "                \"tpr\": tpr,\n",
    "                \"auc\": auc,\n",
    "                \"mean_auc\": mean_auc,\n",
    "                \"accuracy_m\": accuracy_m,\n",
    "                \"conf_matrix_m\": conf_matrix_m,\n",
    "                \"fpr_m\": fpr_m,\n",
    "                \"tpr_m\": tpr_m,\n",
    "                \"auc_m\": auc_m\n",
    "            }\n",
    "\n",
    "def get_status_probs_for_bolero(results, slide_names, checkpoint_name):\n",
    "    biopsy_status_probs = defaultdict(list)\n",
    "    biopsy_pred_confidence = defaultdict(list)\n",
    "    for img_name in results[\"status_probs\"]:\n",
    "        slide_name = int(img_name.split(\"_\")[0])\n",
    "        biopsy_status_probs[slide_name].append(results[\"status_probs\"][img_name][checkpoint_name][0][:3]) # Ignore the double clones class for BOLERO\n",
    "        # Special case if doubleclones scores highest: add that to the highest out of OE and NM\n",
    "        if np.argmax(results[\"status_probs\"][img_name][checkpoint_name][0]) == 3:\n",
    "            biopsy_status_probs[slide_name][-1][np.argmax(biopsy_status_probs[slide_name][-1][1:3])+1] += results[\"status_probs\"][img_name][checkpoint_name][0][3]\n",
    "        biopsy_pred_confidence[slide_name].append(np.max(results[\"status_probs\"][img_name][checkpoint_name][0][1:])) # Confidence in the presence of a mutation\n",
    "    # Take the vote of the most confident biopsy\n",
    "    status_probs = np.array([biopsy_status_probs[slide_name][np.argmax(biopsy_pred_confidence[slide_name])] for slide_name in slide_names]) # (N, 3)\n",
    "    return status_probs\n",
    "\n",
    "# Metrics for BOLERO is slightly different because we only have slide labels and have to take the max vote of the biopsies\n",
    "print(\"\\nMETRICS FOR BOLERO\")\n",
    "data_name = \"bolero\"\n",
    "labels = get_labels(data_name)\n",
    "slide_names_input = [int(img_name.split(\"_\")[0]) for img_name in get_img_names(data_name)]\n",
    "slide_names_label = [int(slide_name) for slide_name in labels.keys() if labels[slide_name] != 4]\n",
    "slide_names = list(set(slide_names_input).intersection(slide_names_label))\n",
    "# Remove slide name 20 because it's very blurry\n",
    "slide_names = [slide_name for slide_name in slide_names if slide_name != 20]\n",
    "for model_name in tqdm(model_kwargs):\n",
    "    # Check if we have results for this model\n",
    "    results_dir = os.path.join(RESULTS_DIR, model_name)\n",
    "    if not os.path.exists(results_dir) or not any([f.startswith(data_name) for f in os.listdir(results_dir)]):\n",
    "        continue\n",
    "\n",
    "    results = load_results(model_name, data_name)\n",
    "    for checkpoint_name in list(results[\"status_probs\"].values())[0].keys():\n",
    "        status_probs = get_status_probs_for_bolero(results, slide_names, checkpoint_name)\n",
    "        y_pred = np.argmax(status_probs, axis=1)\n",
    "        y_true = np.array([labels[slide_name] for slide_name in slide_names])\n",
    "        accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "\n",
    "        # Also calculate these if we compare WT to Mutated\n",
    "        # Sum the mutated classes 1,2,3\n",
    "        status_probs_mutated = np.array([np.sum(status_probs[i][1:]) for i in range(len(slide_names))]) # (N,)\n",
    "        y_pred_mutated = status_probs_mutated > 0.5\n",
    "        y_true_mutated = y_true > 0\n",
    "        accuracy_m, conf_matrix_m, fpr_m, tpr_m, auc_m,_ = calculate_metrics(y_true_mutated, y_pred_mutated, status_probs_mutated.reshape(-1, 1), multi_class=False)\n",
    "        # # Just don't count doubleclones\n",
    "        # status_probs_mutated = np.array([results[\"status_probs\"][img_name][checkpoint_name][0][:2] for img_name in img_names]) # (N, 3)\n",
    "        # y_pred = np.argmax(status_probs, axis=1)\n",
    "        # y_true = np.array([labels[img_name] for img_name in img_names])\n",
    "        # accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "\n",
    "        model_results[data_name][model_name][checkpoint_name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"conf_matrix\": conf_matrix,\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"auc\": auc,\n",
    "            \"mean_auc\": mean_auc,\n",
    "            \"accuracy_m\": accuracy_m,\n",
    "            \"conf_matrix_m\": conf_matrix_m,\n",
    "            \"fpr_m\": fpr_m,\n",
    "            \"tpr_m\": tpr_m,\n",
    "            \"auc_m\": auc_m,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST FOR SYNTHETIC DC\n",
    "\n",
    "for data_name in [\"test+pathxl-100\"]:\n",
    "    print(f\"\\nMETRICS FOR {data_name.upper()}\")\n",
    "    for model_name in tqdm(model_kwargs):\n",
    "        if model_name in [\"CLAM\", \"CLAM_ResNet18_tuned\"] or \"CLAM\" not in model_name: # Only works with patch-based models but not CLAM itself\n",
    "            continue\n",
    "        results = load_results(model_name+\"_synth\", data_name)\n",
    "        img_names = results[\"status_probs\"].keys()\n",
    "        for checkpoint_name in list(results[\"status_probs\"].values())[0].keys():\n",
    "            status_probs = np.array([results[\"status_probs\"][img_name][checkpoint_name][0] for img_name in img_names]) # (N, 4)\n",
    "            y_pred = np.argmax(status_probs, axis=1)\n",
    "            y_true = np.array([3 for img_name in img_names])\n",
    "            accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "\n",
    "            # Also calculate these if we compare WT to Mutated\n",
    "            # Sum the mutated classes 1,2,3\n",
    "            status_probs_mutated = np.array([np.sum(results[\"status_probs\"][img_name][checkpoint_name][0][1:]) for img_name in img_names]) # (N,)\n",
    "            y_pred_mutated = status_probs_mutated > 0.5\n",
    "            y_true_mutated = y_true > 0\n",
    "            accuracy_m, conf_matrix_m, fpr_m, tpr_m, auc_m,_ = calculate_metrics(y_true_mutated, y_pred_mutated, status_probs_mutated.reshape(-1, 1), multi_class=False)\n",
    "            # # Just don't count doubleclones\n",
    "            # status_probs_mutated = np.array([results[\"status_probs\"][img_name][checkpoint_name][0][:2] for img_name in img_names]) # (N, 3)\n",
    "            # y_pred = np.argmax(status_probs, axis=1)\n",
    "            # y_true = np.array([labels[img_name] for img_name in img_names])\n",
    "            # accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "\n",
    "            model_results[data_name+\"_synth_dc\"][model_name][checkpoint_name] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"conf_matrix\": conf_matrix,\n",
    "                \"fpr\": fpr,\n",
    "                \"tpr\": tpr,\n",
    "                \"auc\": auc,\n",
    "                \"mean_auc\": mean_auc,\n",
    "                \"accuracy_m\": accuracy_m,\n",
    "                \"conf_matrix_m\": conf_matrix_m,\n",
    "                \"fpr_m\": fpr_m,\n",
    "                \"tpr_m\": tpr_m,\n",
    "                \"auc_m\": auc_m\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results for synth dc, mean over all checkpoints\n",
    "data_name = \"test+pathxl-100_synth_dc\"\n",
    "print(f\"\\nMETRICS FOR {data_name.upper()}\")\n",
    "for model_name in model_results[data_name]:\n",
    "    print(f\"\\nMODEL: {model_name}\")\n",
    "    for metric_name in [\"accuracy\", \"accuracy_m\"]:\n",
    "        print(f\"{metric_name}: {np.mean([model_results[data_name][model_name][checkpoint_name][metric_name] for checkpoint_name in model_results[data_name][model_name]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(fpr, tpr, auc):\n",
    "    plt.figure()\n",
    "    for i in range(4):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"{P53_CLASS_NAMES[i]} (AUC = {auc[i]:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, normalize=True):\n",
    "    n = len(conf_matrix)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    normalized_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    if normalize:\n",
    "        conf_matrix = normalized_conf_matrix\n",
    "    plt.figure()\n",
    "    plt.imshow(normalized_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(n)\n",
    "    plt.xticks(tick_marks, P53_CLASS_NAMES[:n], rotation=45)\n",
    "    plt.yticks(tick_marks, P53_CLASS_NAMES[:n], rotation=45)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # Put xticks on top\n",
    "    plt.gca().xaxis.set_ticks_position('top')\n",
    "\n",
    "    # Put text on the plot\n",
    "    thresh = normalized_conf_matrix.max() / 2.\n",
    "    for i, j in [(i, j) for i in range(n) for j in range(n)]:\n",
    "        val = conf_matrix[i, j]\n",
    "        if val == 0:\n",
    "            continue\n",
    "        if normalize:\n",
    "            plt.text(j, i, f\"{val:.2f}\", horizontalalignment=\"center\", color=\"white\" if normalized_conf_matrix[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, f\"{val}\", horizontalalignment=\"center\", color=\"white\" if normalized_conf_matrix[i, j] > thresh else \"black\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_multi_confusion_matrix(confusion_matrices, std_confusion_matrices=None, class_names=P53_CLASS_CODES, normalize=True, \n",
    "        title=None, subtitle=None, model_names=None, model_colors=None, put_text=True, percentages=False, center_text=True, figsize=(5,5),\n",
    "        legendloc_bbox=('upper right', (1,0)),\n",
    "        save_path=None):\n",
    "    \"\"\"Instead of colouring the cell for intensity, plot multiple bars for each cell.\n",
    "    Each bar corresponds to a different confusion matrix, and has a corresponding color.\n",
    "    Now there is only one subplot, and each bar is aligned to where the cell would be.\"\"\"\n",
    "    class_names = class_names[:len(confusion_matrices[0])]\n",
    "\n",
    "    # Normalize the confusion matrices\n",
    "    if normalize:\n",
    "        norm_conf_matrices = []\n",
    "        norm_std_conf_matrices = []\n",
    "        for i, confusion_matrix in enumerate(confusion_matrices):\n",
    "            norm_factor = 1 / confusion_matrix.sum(axis=1)\n",
    "            norm_conf_matrices.append(confusion_matrix.astype('float') * norm_factor[:, np.newaxis])\n",
    "            if std_confusion_matrices is not None:\n",
    "                norm_std_conf_matrices.append(std_confusion_matrices[i].astype('float') * norm_factor[:, np.newaxis])\n",
    "        confusion_matrices = norm_conf_matrices\n",
    "        if std_confusion_matrices is not None:\n",
    "            std_confusion_matrices = norm_std_conf_matrices\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    colors = model_colors if model_colors is not None else colors\n",
    "    width = 1/len(confusion_matrices)\n",
    "\n",
    "    # Place grid lines at the cell boundaries\n",
    "    for i in range(len(class_names)):\n",
    "        ax.axhline(i, color='k', linewidth=1)\n",
    "        ax.axvline(i, color='k', linewidth=1)\n",
    "    ax.set_xticks(np.arange(len(class_names))+0.5)\n",
    "    ax.set_yticks(np.arange(len(class_names))+0.5)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.set_yticklabels(reversed(class_names), rotation=45)\n",
    "    ax.set_xlim(0, len(class_names))\n",
    "    ax.set_ylim(0, len(class_names))\n",
    "    # Place x ticks and label above the plot\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.xlabel('predicted', labelpad=-10)\n",
    "    plt.ylabel('true',\n",
    "            #    Further to the right, closer to the plot\n",
    "               labelpad=-10)\n",
    "    plt.suptitle(title, fontsize=12, y=1.01, x=0.51)\n",
    "    plt.title(subtitle, fontsize=10, color=\"#444444\")\n",
    "\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            cell_x = j\n",
    "            cell_y = width/2 + len(class_names)-1-i\n",
    "            for k in range(len(confusion_matrices)):\n",
    "                bar_y = cell_y + width * k\n",
    "                # Plot rectangle as horizontal bar\n",
    "                if std_confusion_matrices is not None:\n",
    "                    ax.barh(bar_y, confusion_matrices[k][i, j].item(), width, left=cell_x, color=colors[k], alpha=1, xerr=std_confusion_matrices[k][i, j].item(), error_kw=dict(ecolor='#444444', lw=1))\n",
    "                else:\n",
    "                    ax.barh(bar_y, confusion_matrices[k][i, j].item(), width, left=cell_x, color=colors[k], alpha=1)\n",
    "                # Put the number in the center of the bar\n",
    "                num = confusion_matrices[k][i, j].item()\n",
    "                if num >= 0.01 and put_text:\n",
    "                    color = \"black\"\n",
    "                    num_str = f\"{num:.3f}\"\n",
    "                    if percentages:\n",
    "                        num_str = f\"{num*100:.0f}%\"\n",
    "                    if center_text:\n",
    "                        ax.text(cell_x + 0.5, bar_y, num_str, ha='center', va='center', color=color)\n",
    "                    else:\n",
    "                        if (i,j) in [(0,0),(1,1),(2,2)]: # Make text avoid the bars\n",
    "                            ax.text(cell_x + 0.01, bar_y, num_str, ha='left', va='center', color=color)\n",
    "                        else:\n",
    "                            ax.text(cell_x + 0.99, bar_y, num_str, ha='right', va='center', color=color)\n",
    "\n",
    "    if model_names is not None:\n",
    "        plt.legend(handles=[\n",
    "            plt.Rectangle((0,0),1,1, color=colors[i], label=model_names[i]) for i in reversed(range(len(confusion_matrices)))] +\\\n",
    "            [plt.Line2D([0], [0], color='#444444', lw=1, label='std')]*int(std_confusion_matrices is not None),\n",
    "        # Place the legend outside the plot\n",
    "        loc=legendloc_bbox[0], bbox_to_anchor=legendloc_bbox[1], ncol=3)\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Results into a visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_visualizations = defaultdict(lambda: defaultdict(dict)) # data_name -> model_name -> visualization_name\n",
    "\n",
    "for data_name in [\n",
    "    \"test\", \n",
    "    # \"pathxl\", \"test+pathxl\", \n",
    "    \"test+pathxl-100\", \n",
    "    \"bolero\"]:\n",
    "    for model_name in model_results[data_name]:\n",
    "        # We want to aggregate the results for each checkpoint\n",
    "        # For the confusion matrix we want to get a mean_conf_matrix and std_conf_matrix\n",
    "        conf_matrix = np.array([model_results[data_name][model_name][checkpoint_name][\"conf_matrix\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "        \n",
    "        # For the roc curves we want to get a mean_auc and std_auc (not sure how to aggregate fpr and tpr)\n",
    "        auc = []\n",
    "        for checkpoint_name in model_results[data_name][model_name]:\n",
    "            results = model_results[data_name][model_name][checkpoint_name]\n",
    "            auc.append(np.array(list(results[\"auc\"].values())))\n",
    "        mean_auc = np.array([model_results[data_name][model_name][checkpoint_name][\"mean_auc\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "\n",
    "        # For the accuracy we want to get mean_accuracy and std_accuracy\n",
    "        accuracy = np.array([model_results[data_name][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "\n",
    "        # Same for the WT vs Mutated\n",
    "        accuracy_m = np.array([model_results[data_name][model_name][checkpoint_name][\"accuracy_m\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "        conf_matrix_m = np.array([model_results[data_name][model_name][checkpoint_name][\"conf_matrix_m\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "        auc_m = np.array([model_results[data_name][model_name][checkpoint_name][\"auc_m\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "\n",
    "        for metric_name, values in {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"conf_matrix\": conf_matrix,\n",
    "            \"auc_per_class\": auc,\n",
    "            \"auc\": mean_auc,\n",
    "            \"accuracy_m\": accuracy_m,\n",
    "            \"conf_matrix_m\": conf_matrix_m,\n",
    "            \"auc_m\": auc_m,\n",
    "        }.items():\n",
    "            model_visualizations[data_name][model_name].update({\n",
    "                f\"mean_{metric_name}\": np.mean(values, axis=0),\n",
    "                f\"std_{metric_name}\": np.std(values, axis=0),\n",
    "                f\"min_{metric_name}\": np.min(values, axis=0),\n",
    "                f\"max_{metric_name}\": np.max(values, axis=0),\n",
    "            })\n",
    "\n",
    "        # The min and max conf matrices don't make sense like this so we replace them with the conf matrices corresponding to the highest accuracy\n",
    "        best_checkpoint_idx = np.argmax(accuracy)\n",
    "        best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "        model_visualizations[data_name][model_name][\"min_conf_matrix\"] = None\n",
    "        model_visualizations[data_name][model_name][\"max_conf_matrix\"] = model_results[data_name][model_name][best_checkpoint_name][\"conf_matrix\"]\n",
    "\n",
    "        # Same for accuracy_m\n",
    "        best_checkpoint_idx = np.argmax(accuracy_m)\n",
    "        best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "        model_visualizations[data_name][model_name][\"min_conf_matrix_m\"] = None\n",
    "        model_visualizations[data_name][model_name][\"max_conf_matrix_m\"] = model_results[data_name][model_name][best_checkpoint_name][\"conf_matrix_m\"]\n",
    "\n",
    "        # We won't aggregate the ROC curves per class, and averaging the classes doesn't make sense in this case\n",
    "        # So we only consider the ROC curve for the WT vs Mutated case, and again idk how to average ROC curves\n",
    "        # So we just take the WT vs Mutated ROC curve that has the highest AUC of the checkpoints\n",
    "        best_checkpoint_idx = np.argmax([model_results[data_name][model_name][checkpoint_name][\"auc_m\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "        best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "        best_fpr_m = model_results[data_name][model_name][best_checkpoint_name][\"fpr_m\"]\n",
    "        best_tpr_m = model_results[data_name][model_name][best_checkpoint_name][\"tpr_m\"]\n",
    "        model_visualizations[data_name][model_name][\"fpr_m\"] = best_fpr_m\n",
    "        model_visualizations[data_name][model_name][\"tpr_m\"] = best_tpr_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best models comparison\n",
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multi bar confusion matrix\n",
    "data_name = \"test+pathxl-100\"\n",
    "# data_name = \"test\"\n",
    "metric_name = \"conf_matrix\"\n",
    "model_names = [\"CLAM_ResNet18_tuned_64\", \n",
    "            #    \"CLAM_ResNet18_tuned_128\", \n",
    "            \"CLAM_retccl_tuned\",\n",
    "            \"CLAM_retccl_tuned_hybrid\",\n",
    "               \"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\", \"fb_spacing4\", \"fb_db_spacing4\"]\n",
    "model_name_mapping = {\n",
    "        \"CLAM\":\"CLAM\",\n",
    "        \"CLAM_db\":\"CLAMdb\",\n",
    "        \"CLAM_m\":\"CLAM+DC\",\n",
    "        \"CLAM_db_m\":\"CLAMdb+DC\",\n",
    "        \"fb_db_spacing4\":\"FBdb\",\n",
    "        \"fb_spacing4\":\"FB\",\n",
    "        \"CLAM_ResNet18_tuned_64\":\"CLAMtuned64\",\n",
    "        \"CLAM_ResNet18_tuned_128\":\"CLAMtuned128\",\n",
    "        \"CLAM_retccl_tuned\":\"CLAMretccl-t\",\n",
    "        \"CLAM_retccl_tuned_hybrid\":\"CLAMretccl-h\",\n",
    "    }\n",
    "model_colors = {\n",
    "        \"CLAM\":color_dict[\"r_p\"],\n",
    "        \"CLAM_db\":color_dict[\"r\"],\n",
    "        \"CLAM_m\":color_dict[\"y_p\"],\n",
    "        \"CLAM_db_m\":color_dict[\"y\"],\n",
    "        \"fb_db_spacing4\":color_dict[\"p\"],\n",
    "        \"fb_spacing4\":color_dict[\"p_p\"],\n",
    "        \"CLAM_ResNet18_tuned_64\":color_dict[\"g\"],\n",
    "        \"CLAM_ResNet18_tuned_128\":color_dict[\"g_p\"],\n",
    "        \"CLAM_retccl_tuned\":color_dict[\"b\"],\n",
    "        \"CLAM_retccl_tuned_hybrid\":color_dict[\"b_p\"],\n",
    "    }\n",
    "# Pick checkpoint with the highest accuracy on test\n",
    "# confusion_matrices = []\n",
    "# for model_name in model_names:\n",
    "#     best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "#     best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "#     print(f\"Best checkpoint for {model_name}: {best_checkpoint_idx} {best_checkpoint_name}\")\n",
    "#     confusion_matrices.append(model_results[data_name][model_name][best_checkpoint_name][metric_name])\n",
    "\n",
    "# std_confusion_matrices = None\n",
    "confusion_matrices = [model_visualizations[data_name][model_name][f\"mean_{metric_name}\"] for model_name in model_names]\n",
    "std_confusion_matrices = [model_visualizations[data_name][model_name][f\"std_{metric_name}\"] for model_name in model_names]\n",
    "plot_multi_confusion_matrix(confusion_matrices, std_confusion_matrices, class_names=P53_CLASS_CODES,\n",
    "    # title=f\"Confusion Matrix\",\n",
    "    # subtitle=f\"on test 1+2 data (n={len(get_labels(data_name))})\",\n",
    "    model_names=[model_name_mapping[model_name] for model_name in model_names],\n",
    "    model_colors=[model_colors[model_name] for model_name in model_names],\n",
    "    put_text=True,\n",
    "    percentages=False,\n",
    "    figsize=(6,6),\n",
    "    save_path=os.path.join(RESULTS_DIR, \"..\", \"images\", f\"confusion_matrix.png\"),\n",
    "    legendloc_bbox=('lower center', (0.5, 1.08))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathxl_hard_NMs = ['139_2', '58_4', '65_4', '63_1', '115_1', '63_3', '78_3', '118_2', '34_4']\n",
    "\n",
    "# print CLAM_resnet18_tuned_128 predictions for the hard NMs\n",
    "model_name = \"CLAM_ResNet18_tuned_128\"\n",
    "results = load_results(model_name, \"pathxl\")\n",
    "best_checkpoint_idx = 2\n",
    "best_checkpoint_name = list(results[\"status_probs\"]['0_1'].keys())[best_checkpoint_idx]\n",
    "for img_name in pathxl_hard_NMs:\n",
    "    print(f\"{img_name}: {results['status_probs'][img_name][best_checkpoint_name][0].argmax()}\")\n",
    "    \n",
    "print()\n",
    "model_name = \"CLAM_ResNet18_tuned_64\"\n",
    "results = load_results(model_name, \"pathxl\")\n",
    "best_checkpoint_idx = 0\n",
    "best_checkpoint_name = list(results[\"status_probs\"]['0_1'].keys())[best_checkpoint_idx]\n",
    "for img_name in pathxl_hard_NMs:\n",
    "    print(f\"{img_name}: {results['status_probs'][img_name][best_checkpoint_name][0].argmax()}\")\n",
    "\n",
    "print()\n",
    "model_name = \"CLAM\"\n",
    "results = load_results(model_name, \"pathxl\")\n",
    "best_checkpoint_idx = 3\n",
    "best_checkpoint_name = list(results[\"status_probs\"]['0_1'].keys())[best_checkpoint_idx]\n",
    "for img_name in pathxl_hard_NMs:\n",
    "    print(f\"{img_name}: {results['status_probs'][img_name][best_checkpoint_name][0].argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test significance in confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test statistical significance of the difference in accuracy between CLAM and CLAM_db\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "comparisons = [\n",
    "    (\"CLAM\", \"CLAM_db\"),\n",
    "    (\"CLAM_m\", \"CLAM_db_m\"),\n",
    "    (\"CLAM_db\", \"CLAM_m\"),\n",
    "    (\"CLAM_db\", \"CLAM_db_m\"),\n",
    "    (\"fb_spacing4\", \"fb_db_spacing4\"),\n",
    "    (\"fb_spacing4\", \"CLAM\"),\n",
    "    (\"CLAM_db_m\", \"fb_db_spacing4\"),\n",
    "    (\"CLAM_db\", \"fb_db_spacing4\"),\n",
    "    (\"CLAM\", \"CLAM_ResNet18_tuned_64\"),\n",
    "    (\"CLAM\", \"CLAM_ResNet18_tuned_128\"),\n",
    "]\n",
    "for comparison in comparisons:\n",
    "    model_name1 = comparison[0]\n",
    "    model_name2 = comparison[1]\n",
    "\n",
    "    print(f\"\\n{model_name1} vs {model_name2} on {data_name}\")\n",
    "\n",
    "    # Check difference between each diagonal cell in the confusion matrix, using the 5 checkpoints as independent samples\n",
    "    # We will use a t-test for each cell\n",
    "    confusion_matrices1 = [model_results[\"test+pathxl-100\"][model_name1][checkpoint_name][\"conf_matrix\"] for checkpoint_name in model_results[\"test\"][model_name1]]\n",
    "    confusion_matrices2 = [model_results[\"test+pathxl-100\"][model_name2][checkpoint_name][\"conf_matrix\"] for checkpoint_name in model_results[\"test\"][model_name2]]\n",
    "\n",
    "    t_stats = np.zeros((1,4))\n",
    "    p_values = np.zeros((1,4))\n",
    "    for i in range(4):\n",
    "        j = i\n",
    "        values1 = [confusion_matrix[i,j] for confusion_matrix in confusion_matrices1]\n",
    "        values2 = [confusion_matrix[i,j] for confusion_matrix in confusion_matrices2]\n",
    "        res = ttest_ind(values1, values2)\n",
    "        # Res is a tuple with the t-statistic and the p-value\n",
    "        t_stats[0,i] = res[0]\n",
    "        p_values[0,i] = res[1]\n",
    "\n",
    "    # Plot the t-stats and p-values\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "    cmap = plt.get_cmap(\"coolwarm\")\n",
    "    cmap.set_bad(color='gray')\n",
    "    im = ax[0].imshow(t_stats, cmap=cmap, vmin=-3, vmax=3)\n",
    "    im = ax[1].imshow(p_values, cmap=cmap, vmin=0, vmax=1)\n",
    "\n",
    "    # Add colorbar\n",
    "    # cbar = fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.05, pad=0.05)\n",
    "    # cbar.set_label('t-statistic' if ax[0] == ax else 'p-value')\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    for i in range(2):\n",
    "        ax[i].set_xticks(np.arange(4))\n",
    "        # ... and label them with the respective list entries\n",
    "        ax[i].set_xticklabels(P53_CLASS_CODES)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax[0].get_xticklabels(), rotation=45, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(4):\n",
    "        text = ax[0].text(i, 0, f\"{t_stats[0,i]:.2f}\",\n",
    "                    ha=\"center\", va=\"center\", color=\"black\")\n",
    "        p_text = f\"{p_values[0,i]:.3f}\"\n",
    "        # Add number of stars to p-value\n",
    "        if p_values[0,i] < 0.001:\n",
    "            p_text += \"***\"\n",
    "        elif p_values[0,i] < 0.01:\n",
    "            p_text += \"**\"\n",
    "        elif p_values[0,i] < 0.05:\n",
    "            p_text += \"*\"\n",
    "        text = ax[1].text(i, 0, p_text,\n",
    "                    ha=\"center\", va=\"center\", color=\"black\")\n",
    "            \n",
    "    ax[0].set_title(\"t-statistic\")\n",
    "    ax[1].set_title(\"p-value\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect ones in common between best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the best models, load the predictions for test+pathxl-100\n",
    "data_name = \"test+pathxl-100\"\n",
    "model_names = [\"CLAM_ResNet18_tuned_64\", \n",
    "            #    \"CLAM_ResNet18_tuned_128\",\n",
    "            \"CLAM_retccl_tuned\",\n",
    "            \"CLAM_retccl_tuned_hybrid\",\n",
    "    \"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\", \"fb_spacing4\", \"fb_db_spacing4\"]\n",
    "model_name_mapping = {\n",
    "        \"CLAM\":\"CLAM\",\n",
    "        \"CLAM_db\":\"CLAMdb\",\n",
    "        \"CLAM_m\":\"CLAM+DC\",\n",
    "        \"CLAM_db_m\":\"CLAMdb+DC\",\n",
    "        \"fb_db_spacing4\":\"FBdb\",\n",
    "        \"fb_spacing4\":\"FB\",\n",
    "        \"CLAM_ResNet18_tuned_64\":\"CLAMtuned64\",\n",
    "        \"CLAM_ResNet18_tuned_128\":\"CLAMtuned128\",\n",
    "        \"CLAM_retccl_tuned\":\"CLAMretccl-t\",\n",
    "        \"CLAM_retccl_tuned_hybrid\":\"CLAMretccl-h\",\n",
    "    }\n",
    "case_to_pred = {}\n",
    "case_to_confidence = {}\n",
    "model_accuracy = {}\n",
    "for model_name in model_names:\n",
    "    # Pick checkpoint that had the highest accuracy\n",
    "    best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "    best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "    model_accuracy[model_name] = model_results[data_name][model_name][best_checkpoint_name][\"accuracy\"]\n",
    "\n",
    "    results = load_results(model_name, data_name)\n",
    "    labels = get_labels(data_name)\n",
    "    case_names = list(set(results[\"status_probs\"].keys()).intersection(labels.keys()))\n",
    "\n",
    "    case_to_pred[model_name] = {}\n",
    "    case_to_confidence[model_name] = {}\n",
    "\n",
    "    for case_name in case_names:\n",
    "        status_probs = results[\"status_probs\"][case_name][best_checkpoint_name][0]\n",
    "        case_to_pred[model_name][case_name] = np.argmax(status_probs)\n",
    "        case_to_confidence[model_name][case_name] = np.max(status_probs)\n",
    "\n",
    "color_mapping = {0:color_dict['g'], 1:color_dict['o'], 2:color_dict['b'], 3:color_dict['p']}\n",
    "\n",
    "# # Make a horizontal barchart for the confidence of correct and incorrect predictions\n",
    "# # On the x axis, the confidence\n",
    "# # On the y axis, the models, with the bar for correct and incorrect predictions against each other\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# bar_width = 0.4\n",
    "# for i, model_name in enumerate(model_names):\n",
    "#     correct_confidences = [case_to_confidence[model_name][case_name] for case_name in case_to_pred[model_name] if case_to_pred[model_name][case_name] == labels[case_name]]\n",
    "#     incorrect_confidences = [case_to_confidence[model_name][case_name] for case_name in case_to_pred[model_name] if case_to_pred[model_name][case_name] != labels[case_name]]\n",
    "#     correct_confidence = np.mean(correct_confidences) if len(correct_confidences) > 0 else 0\n",
    "#     incorrect_confidence = np.mean(incorrect_confidences) if len(incorrect_confidences) > 0 else 0\n",
    "#     ax.barh(i, correct_confidence, bar_width, color=color_dict['g'], label=\"correct\")\n",
    "#     ax.barh(i+bar_width, incorrect_confidence, bar_width, color=color_dict['r'], label=\"incorrect\")\n",
    "#     ax.text(correct_confidence+0.01, i, f\"{correct_confidence:.2f}\", ha='left', va='center', color=\"black\")\n",
    "#     ax.text(incorrect_confidence+0.01, i+bar_width, f\"{incorrect_confidence:.2f}\", ha='left', va='center', color=\"black\")\n",
    "# ax.set_yticks(np.arange(len(model_names))+bar_width/2)\n",
    "# ax.set_yticklabels([model_name_mapping[model_name] for model_name in model_names])\n",
    "# plt.xlabel('confidence')\n",
    "# plt.ylabel('model')\n",
    "# plt.title('Confidence of correct and incorrect predictions')\n",
    "# # plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "def count_fn(lst):\n",
    "    counts = defaultdict(int)\n",
    "    for i in lst:\n",
    "        counts[i] += 1\n",
    "    return counts\n",
    "\n",
    "def is_correct(model_name, case_name, mode=\"precise\"):\n",
    "    if mode == \"precise\":\n",
    "        return (case_to_pred[model_name][case_name] == labels[case_name])\n",
    "    elif mode == \"mutated\":\n",
    "        return (case_to_pred[model_name][case_name] > 0) == (labels[case_name] > 0)\n",
    "    elif mode == \"no_FN\": # No false negatives, so only wrong if the model falsely predicts WT\n",
    "        return is_correct(model_name, case_name, \"mutated\") or (case_to_pred[model_name][case_name] > 0 and labels[case_name] == 0)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "# Make a histogram with on the x-axis the amount of models (0-6) that predicted the correct class\n",
    "# On the y-axis the amount of cases\n",
    "correct_preds = {case_name: sum([is_correct(model_name, case_name, \"no_FN\") for model_name in model_names]) for case_name in case_names}\n",
    "correct_pred_counts = count_fn(correct_preds.values())\n",
    "correct_pred_counts = {i:correct_pred_counts[i] for i in range(7)}\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# ax.bar(correct_pred_counts.keys(), correct_pred_counts.values())\n",
    "# ax.set_xticks(np.arange(7))\n",
    "# plt.xlabel('correct predictions')\n",
    "# plt.ylabel('cases')\n",
    "# plt.title('Amount of correct predictions per case')\n",
    "# # Put text on the plot\n",
    "# for i in range(7):\n",
    "#     ax.text(i, correct_pred_counts[i], str(correct_pred_counts[i]), ha='center', va='bottom', color=\"black\")\n",
    "# plt.show()\n",
    "\n",
    "# Print list of cases where all models predicted the wrong class\n",
    "all_wrong_cases = [case_name for case_name in case_names if correct_preds[case_name] != len(model_names)]\n",
    "\n",
    "if len(all_wrong_cases) > 30:\n",
    "    print(len(all_wrong_cases))\n",
    "    raise_error\n",
    "# Sort by number of models that predicted the correct class\n",
    "all_wrong_cases = sorted(all_wrong_cases, key=lambda x: correct_preds[x])\n",
    "# Sort by WT,OE,NM,DC\n",
    "all_wrong_cases = sorted(all_wrong_cases, key=lambda x: labels[x])\n",
    "print(f\"Cases where any models predicted the wrong class: {all_wrong_cases}\")\n",
    "\n",
    "# Plot the predictions for these cases for each model in a grid of n_models x n_wrong_cases using squares in the color from the color mapping\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "for i, model_name in enumerate(model_names):\n",
    "    for j, case_id in enumerate(all_wrong_cases):\n",
    "        color = color_mapping[case_to_pred[model_name][case_id]]\n",
    "        ax.add_patch(plt.Rectangle((j+.15, i+.15), .7, .7, color=color, alpha=case_to_confidence[model_name][case_id]**2))\n",
    "        # Put a black x in the middle if the pred was wrong\n",
    "        if case_to_pred[model_name][case_id] != labels[case_id]:\n",
    "            ax.plot([j+.15, j+.85], [i+.15, i+.85], color='#444444', linewidth=1)\n",
    "            # Only draw the second line if it's classified as WT\n",
    "            if case_to_pred[model_name][case_id] == 0:\n",
    "                ax.plot([j+.15, j+.85], [i+.85, i+.15], color='#444444', linewidth=1)\n",
    "        # else:\n",
    "        #     # Write the code in the middle if the prediction was correct\n",
    "# Add 'model' which is just the ground truth\n",
    "for j, case_id in enumerate(all_wrong_cases):\n",
    "    color = color_mapping[labels[case_id]]\n",
    "    ax.add_patch(plt.Rectangle((j+.15, len(model_names)+.45), .7, .1, color=color))\n",
    "    ax.text(j+.5, len(model_names)+.5, P53_CLASS_CODES[labels[case_id]], ha='center', va='bottom', color=\"black\")\n",
    "ax.set_xticks(np.arange(len(all_wrong_cases))+0.5, np.arange(len(all_wrong_cases))+1)\n",
    "# ax.set_xticks([])\n",
    "ax.set_yticks(np.arange(len(model_names)+1)+0.5, [model_name_mapping[model_name] for model_name in model_names]+[\"consensus\"], \n",
    "            #   rotation=45\n",
    "              )\n",
    "# ax.set_ylabel('model')\n",
    "ax.set_xlabel('biopsy')\n",
    "# ax.set_title('all mutated biopsies underclassified as WT by any model', y=1.07)\n",
    "# plt.suptitle(f'from test 1+2', fontsize=10, y=.82, x=0.51, color='#444444')\n",
    "# Make x and y axis equal\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_ylim(0, len(model_names)+1)\n",
    "ax.set_xlim(0, len(all_wrong_cases))\n",
    "\n",
    "# Remove borders\n",
    "for key, spine in ax.spines.items():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Print amount of mutated biopsies\n",
    "n_mutated = sum([labels[case_id] > 0 for case_id in case_names])\n",
    "n_total = len(case_names)\n",
    "print(f\"Amount of mutated biopsies: {n_mutated}/{n_total}\")\n",
    "\n",
    "\n",
    "# # Plot these case images in a 4x5 grid using the load_img function\n",
    "# fig, ax = plt.subplots(4, 5, figsize=(10, 10))\n",
    "# for i, case_name in enumerate(all_wrong_cases[:20]):\n",
    "#     data_nm = \"pathxl\" if \"_\" in str(case_name) else \"test\"\n",
    "#     img = load_img(case_name, data_nm)\n",
    "#     # Resize to 0.25 of original size\n",
    "#     img = cv2.resize(img, (0,0), fx=0.25, fy=0.25)\n",
    "#     ax[i//5, i%5].imshow(img)\n",
    "#     ax[i//5, i%5].axis(\"off\")\n",
    "#     ax[i//5, i%5].set_title(f\"{case_name} - {P53_CLASS_CODES[labels[case_name]]}\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of concordance in pathxl\n",
    "\n",
    "Excluding 75 concordance anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For the pathxl results, check if the best models predictions correlated to the concordance between pathologists\n",
    "# data_name = \"pathxl\"\n",
    "\n",
    "# labels = pd.read_csv(os.path.join(BASE_DIR[data_name], 'labels.csv'))\n",
    "# # idx is id column and biopsy_nr column separated by _\n",
    "# labels[\"idx\"] = labels[\"id\"].astype(str) + \"_\" + labels[\"biopsy_nr\"].astype(str)\n",
    "# labels = labels.set_index(\"idx\")\n",
    "# # Sort by id primarily and biopsy_nr secondarily\n",
    "# labels = labels.sort_values(by=[\"id\", \"biopsy_nr\"])\n",
    "# # Map label\n",
    "# mapping = {\"WT\":0, \"Overexpression\":1, \"Null\":2, \"Double clones\":3}\n",
    "# labels[\"label\"] = labels[\"label\"].map(mapping)\n",
    "# # Filter out any concordance % < 75\n",
    "# labels = labels[labels[\"concordance %\"] >= 75]\n",
    "\n",
    "# # Separate the labels into 75 concordance and 100 concordance\n",
    "# labels_75 = labels[labels[\"concordance %\"] == 75]\n",
    "# labels_100 = labels[labels[\"concordance %\"] == 100]\n",
    "# # Print len\n",
    "# print(f\"75 concordance: {len(labels_75)}\")\n",
    "# print(f\"100 concordance: {len(labels_100)}\")\n",
    "\n",
    "# # Show label distribution for both 75 and 100\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "# for i, labels in enumerate([labels_75, labels_100]):\n",
    "#     label_counts = labels[\"label\"].value_counts()\n",
    "#     axs[i].bar(range(4), label_counts)\n",
    "#     axs[i].set_xticks(range(4))\n",
    "#     axs[i].set_xticklabels(P53_CLASS_CODES)\n",
    "#     axs[i].set_title(f\"Label distribution {75 + i*25} concordance\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# # For each model calculate the accuracy on the 75 concordance and 100 concordance\n",
    "# data_name = \"test+pathxl-100\"\n",
    "# model_conf_matrices = []\n",
    "# for model_name in model_names:\n",
    "#     print(f\"\\n{model_name}\")\n",
    "#     accuracies_75 = []\n",
    "#     accuracies_100 = []\n",
    "#     conf_matrices_75 = []\n",
    "#     conf_matrices_100 = []\n",
    "#     results = load_results(model_name, data_name)\n",
    "\n",
    "#     best_checkpoint_idx = np.argmax([model_results['test'][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "#     best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "\n",
    "#     for concor in [75, 100]:\n",
    "#         labels = labels_75 if concor == 75 else labels_100\n",
    "#         # for checkpoint_name in list(results[\"status_probs\"].values())[0].keys():\n",
    "#         for checkpoint_name in [best_checkpoint_name]:\n",
    "            \n",
    "#             status_probs = []\n",
    "#             for img_name in labels.index:\n",
    "#                 if img_name not in results[\"status_probs\"]:\n",
    "#                     continue\n",
    "#                 status_probs.append(results[\"status_probs\"][img_name][checkpoint_name][0])\n",
    "\n",
    "#             status_probs = np.array(status_probs)\n",
    "#             y_pred = np.argmax(status_probs, axis=1)\n",
    "#             y_true = np.array([labels.loc[img_name][\"label\"] for img_name in labels.index if img_name in results[\"status_probs\"]])\n",
    "#             accuracy, conf_matrix, fpr, tpr, auc, mean_auc = calculate_metrics(y_true, y_pred, status_probs)\n",
    "#             if concor == 75:\n",
    "#                 accuracies_75.append(accuracy)\n",
    "#                 conf_matrices_75.append(conf_matrix)\n",
    "#             else:\n",
    "#                 accuracies_100.append(accuracy)\n",
    "#                 conf_matrices_100.append(conf_matrix)\n",
    "                \n",
    "#     print(f\"75 concordance: {np.mean(accuracies_75):.3f} ({np.std(accuracies_75):.3f})\")\n",
    "#     print(f\"100 concordance: {np.mean(accuracies_100):.3f} ({np.std(accuracies_100):.3f})\")\n",
    "#     model_conf_matrices.append({\n",
    "#         \"conf_matrix_75\": conf_matrices_75[0],\n",
    "#         \"conf_matrix_100\": conf_matrices_100[0],\n",
    "#     })\n",
    "    \n",
    "# # Plot multi conf matrix\n",
    "# for concor in [75, 100]:\n",
    "#     confusion_matrices = [model[\"conf_matrix_75\" if concor == 75 else \"conf_matrix_100\"] for model in model_conf_matrices]\n",
    "#     # mean_conf_matrices = [np.mean(conf_matrices, axis=0) for conf_matrices in confusion_matrices]\n",
    "#     std_conf_matrices = None\n",
    "#     # std_conf_matrices = [np.std(conf_matrices, axis=0) for conf_matrices in confusion_matrices]\n",
    "#     plot_multi_confusion_matrix(confusion_matrices, std_conf_matrices, class_names=P53_CLASS_CODES,\n",
    "#         title=f\"Confusion Matrix\",\n",
    "#         subtitle=f\"on {concor} concordance data (n={len(labels_75) if concor == 75 else len(labels_100)})\",\n",
    "#         model_names=[model_name_mapping[model_name] for model_name in model_names],\n",
    "#         model_colors=[model_colors[model_name] for model_name in model_names],\n",
    "#         put_text=True,\n",
    "#         percentages=False,\n",
    "#         figsize=(6,6),\n",
    "#         save_path=os.path.join(RESULTS_DIR, \"..\", \"images\", f\"confusion_matrix_{concor}.png\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves for the best models on test+pathxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the ROC curves of these models for the WT vs Mutated case\n",
    "data_name = \"test+pathxl-100\"\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "closest_point_to_0_1 = (1,0)\n",
    "closest_model = None\n",
    "def dist_to_0_1(point):\n",
    "    return np.sqrt((point[0]-0)**2 + (point[1]-1)**2)\n",
    "model_AUCs = {}\n",
    "for model_name in model_names:\n",
    "    color = model_colors[model_name]\n",
    "\n",
    "    best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "    best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "    fpr = model_results[data_name][model_name][best_checkpoint_name][\"fpr_m\"]\n",
    "    tpr = model_results[data_name][model_name][best_checkpoint_name][\"tpr_m\"]\n",
    "    AUC = model_results[data_name][model_name][best_checkpoint_name][\"auc_m\"]\n",
    "\n",
    "    # AUC = model_visualizations[data_name][model_name][\"max_auc_m\"]\n",
    "    # fpr = model_visualizations[data_name][model_name][\"fpr_m\"]\n",
    "    # tpr = model_visualizations[data_name][model_name][\"tpr_m\"]\n",
    "    plt.plot(fpr, tpr,\n",
    "        color=color, alpha=1, linewidth=2,\n",
    "        # label=f\"{model_name_mapping[model_name]}{' '*(20-len(model_name_mapping[model_name]))}(AUC = {AUC:.2f})\"\n",
    "        label=f\"(AUC = {AUC:.2f}) {model_name_mapping[model_name]}\",\n",
    "        linestyle='solid' if 'db' in model_name else 'dashed',\n",
    "    )\n",
    "    model_AUCs[model_name] = AUC\n",
    "    # Find the point closest to (0,1) for each model\n",
    "    closest_point = min(zip(fpr, tpr), key=dist_to_0_1)\n",
    "    if dist_to_0_1(closest_point) < dist_to_0_1(closest_point_to_0_1):\n",
    "        closest_point_to_0_1 = closest_point\n",
    "        closest_model = model_name    \n",
    "plt.xlabel('1 - specificity')\n",
    "plt.ylabel('sensitivity')\n",
    "# plt.suptitle(f'ROC Curves (Any Mutation)', fontsize=12, y=0.95)\n",
    "# Add subtitle\n",
    "# plt.title(f'on test 1+2 data (n={len(get_labels(data_name))})', fontsize=10, color='#444444')\n",
    "# Put legend reversed\n",
    "dashed = [False, True]\n",
    "plt.legend(\n",
    "    handles=[plt.Line2D([0], [0], color=model_colors[model_name], lw=2, \n",
    "                        label=f\"(AUC = {model_AUCs[model_name]:.2f}) {model_name_mapping[model_name]}\",\n",
    "                        linestyle='solid' if 'db' in model_name else 'dashed') for model_name in reversed(model_names)],\n",
    "    loc='lower right'\n",
    ")\n",
    "# Make x and y axis equal\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1.01)\n",
    "\n",
    "# Put a label on the point closest to (0,1)\n",
    "plt.annotate(f\"(1 - {1-closest_point_to_0_1[0]:.2f}, {closest_point_to_0_1[1]:.2f})\", closest_point_to_0_1, \n",
    "    # Different arrowstyles are: '-', '->', '-[', '|-|', '-|>', '<-', '<->', '<|-', '<|-|>', 'fancy', 'simple', 'wedge'\n",
    "    textcoords=\"offset points\", xytext=(50,-50), ha='center', color='black', fontsize=10, arrowprops=dict(arrowstyle='->', color='black'), rotation=0)\n",
    "\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Switch x ticks and label to top\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().xaxis.set_label_position('top')\n",
    "\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"..\", \"images\", \"roc_curves.png\"), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# # Create a row of three subplots, each displaying a model pair of (CLAM, CLAM_db), (CLAM_m, CLAM_db_m), (fb_spacing4, fb_db_spacing4)\n",
    "# # Each subplot will have two ROC curves, one for each of the two models in the pair\n",
    "# data_name = \"bolero\"\n",
    "# model_pairs = [(\"CLAM\", \"CLAM_db\"), (\"CLAM_m\", \"CLAM_db_m\"), (\"fb_spacing4\", \"fb_db_spacing4\")]\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# for i, (model_name1, model_name2) in enumerate(model_pairs):\n",
    "#     ax = axs[i]\n",
    "#     closest_point_to_0_1 = (1,0)\n",
    "#     closest_model = None\n",
    "#     for model_name in [model_name1, model_name2]:\n",
    "#         color = model_colors[model_name]\n",
    "\n",
    "#         best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "#         best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "#         fpr = model_results[data_name][model_name][best_checkpoint_name][\"fpr_m\"]\n",
    "#         tpr = model_results[data_name][model_name][best_checkpoint_name][\"tpr_m\"]\n",
    "#         AUC = model_results[data_name][model_name][best_checkpoint_name][\"auc_m\"]\n",
    "\n",
    "#         ax.plot(fpr, tpr, \n",
    "#                 label=f\"(AUC = {AUC:.2f}) {model_name_mapping[model_name]}\",\n",
    "#             color=color, alpha=1, linewidth=2)\n",
    "#         # Find the point closest to (0,1) for each model\n",
    "#         closest_point = min(zip(fpr, tpr), key=dist_to_0_1)\n",
    "#         if dist_to_0_1(closest_point) < dist_to_0_1(closest_point_to_0_1):\n",
    "#             closest_point_to_0_1 = closest_point\n",
    "#             closest_model = model_name\n",
    "#     ax.plot([0, 1], [0, 1], 'k--')\n",
    "#     ax.set_xlabel('1 - Specificity')\n",
    "#     ax.set_ylabel('Sensitivity')\n",
    "#     ax.set_title(f'ROC Curves for WT vs Mutated on {data_name}')\n",
    "#     ax.legend()\n",
    "#     # Make x and y axis equal\n",
    "#     ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "#     # Put a label on the point closest to (0,1)\n",
    "#     ax.annotate(f\"(1 - {1-closest_point_to_0_1[0]:.2f}, {closest_point_to_0_1[1]:.2f})\", closest_point_to_0_1, \n",
    "#         textcoords=\"offset points\", xytext=(10,-20), ha='left', color='black', fontsize=8, arrowprops=dict(arrowstyle='->', color='black'), rotation=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC per class (not very interesting, basically shows the same as the confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC per class comparison between these models\n",
    "data_name = \"test+pathxl\"\n",
    "metric_name = \"auc_per_class\"\n",
    "auc_per_class = {}\n",
    "std_auc_per_class = {}\n",
    "for i in range(4): # The four classes\n",
    "    auc_per_class[i]     = [model_visualizations[data_name][model_name][f\"mean_{metric_name}\"][i] for model_name in model_names]\n",
    "    std_auc_per_class[i] = [model_visualizations[data_name][model_name][f\"std_{metric_name}\"][i] for model_name in model_names]\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "# width = 1/len(confusion_matrices)\n",
    "# class_names = P53_CLASS_CODES\n",
    "\n",
    "# # Place grid lines at the cell boundaries\n",
    "# for i in range(len(class_names)):\n",
    "#     ax.axhline(i, color='k', linewidth=1)\n",
    "# ax.set_yticks(np.arange(len(class_names))+0.5)\n",
    "# ax.set_yticklabels(reversed(class_names), rotation=45)\n",
    "# ax.set_ylim(0, len(class_names))\n",
    "# # Place x ticks above the plot\n",
    "# plt.xlabel('AUC')\n",
    "# plt.ylabel('Class')\n",
    "# plt.title('AUC per class')\n",
    "\n",
    "# for i in range(len(class_names)):\n",
    "#     cell_y = width/2 + len(class_names)-1-i\n",
    "#     for k in range(len(confusion_matrices)):\n",
    "#         bar_y = cell_y + width * k\n",
    "#         # Plot rectangle as horizontal bar\n",
    "#         color = model_colors[model_names[k]]\n",
    "#         ax.barh(bar_y, auc_per_class[i][k].item(), width, left=0, color=color, alpha=1, xerr=std_auc_per_class[i][k].item(), error_kw=dict(ecolor='#444444', lw=1))\n",
    "#         # Put the number in the center of the bar\n",
    "#         num = auc_per_class[i][k].item()\n",
    "#         num_str = f\"{num:.2f}  {std_auc_per_class[i][k].item():.2f}\"\n",
    "#         ax.text(0.01, bar_y, num_str, ha='left', va='center', color=\"black\")\n",
    "\n",
    "# if model_names is not None:\n",
    "#     plt.legend(handles=[\n",
    "#         plt.Rectangle((0,0),1,1, color=model_colors[model_names[i]], label=model_name_mapping[model_names[i]]) for i in reversed(range(len(model_names)))] +\\\n",
    "#         [plt.Line2D([0], [0], color='#444444', lw=1, label='std')],\n",
    "#     # Place the legend outside the plot\n",
    "#     loc='lower left', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison on BOLERO\n",
    "#### BOLERO predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P53_BOLERO = pd.read_csv(os.path.join(BASE_DIR[\"bolero\"], 'P53_BOLERO_T.csv'))\n",
    "P53_BOLERO.sort_values('Case ID', inplace=True)\n",
    "P53_BOLERO.reset_index(drop=True, inplace=True)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12,6))\n",
    "\n",
    "n_labels = 5\n",
    "counts = np.zeros((len(P53_BOLERO), n_labels))\n",
    "# For every case\n",
    "for i, case in enumerate(P53_BOLERO.iterrows()):\n",
    "    # Get the labels\n",
    "    labels = case[1].values[1:-1] # Exclude Case ID and GS\n",
    "    # For every label\n",
    "    for j, label in enumerate(labels):\n",
    "        # Increment the count\n",
    "        counts[i, label] += 1\n",
    "# Create a DataFrame\n",
    "counts = pd.DataFrame(counts, columns=range(n_labels))\n",
    "# Sort by custom ordering (it contains case IDs, not indices of the DataFrame)\n",
    "# case_nr_custom_ordering = ['RB0024', 'RB0020', 'RB0011', 'RB0003', 'RB0042', 'RB0051', 'RB0030', 'RB0027', 'RB0053', 'RB0032', 'RB0043', 'RB0015', 'RB0019', 'RB0006', 'RB0033', 'RB0046', 'RB0017', 'RB0031', 'RB0038', 'RB0029', 'RB0022', 'RB0040', 'RB0045', 'RB0047', 'RB0028', 'RB0054', 'RB0035', 'RB0050', 'RB0044', 'RB0009', 'RB0014', 'RB0013', 'RB0036', 'RB0037', 'RB0010', 'RB0005', 'RB0052', 'RB0039', 'RB0004', 'RB0007', 'RB0034', 'RB0049', 'RB0008', 'RB0002', 'RB0023', 'RB0026', 'RB0016', 'RB0001', 'RB0055', 'RB0012', 'RB0018', 'RB0025', 'RB0041', 'RB0048', 'RB0021']\n",
    "case_nr_custom_ordering = ['RB0024', 'RB0020', 'RB0011', 'RB0003', 'RB0042', 'RB0030', 'RB0051', 'RB0027', 'RB0032', 'RB0053', 'RB0043', 'RB0015', 'RB0019', 'RB0006', 'RB0033', 'RB0046', 'RB0017', 'RB0038', 'RB0031', 'RB0029', 'RB0022', 'RB0040', 'RB0028', 'RB0047', 'RB0045', 'RB0054', 'RB0035', 'RB0050', 'RB0044', 'RB0009', 'RB0049', 'RB0013', 'RB0036', 'RB0037', 'RB0052', 'RB0014', 'RB0010', 'RB0039', 'RB0034', 'RB0004', 'RB0007', 'RB0005', 'RB0023', 'RB0001', 'RB0002', 'RB0008', 'RB0026', 'RB0016', 'RB0012', 'RB0055', 'RB0018', 'RB0041', 'RB0048', 'RB0025', 'RB0021']\n",
    "index_ordering = [P53_BOLERO['Case ID'].tolist().index(case_id) for case_id in case_nr_custom_ordering]\n",
    "counts = counts.iloc[index_ordering]\n",
    "# More satisfying versions of these colors\n",
    "# color = ['grey','#2ca02c','#ff7f0e','#1f77b4','#444444']\n",
    "color = ['grey', color_dict['g'], color_dict['o'], color_dict['b'], '#444444']\n",
    "labels = ['missing', 'WT', 'OE', 'NM', 'Unclear']\n",
    "ax = counts.plot(kind='bar', stacked=True, width=.6, color=color, alpha=.5, ax=axs[1], figsize=(12,4))\n",
    "\n",
    "# Add legend with the labels\n",
    "plt.legend(handles=[\n",
    "    plt.Rectangle((0,0),1,1, color=color[i+1], label=labels[i+1]) for i in reversed(range(n_labels-1))],\n",
    "    loc='lower left')\n",
    "\n",
    "# ax.set_xticks(np.arange(len(counts)), np.arange(1, len(counts)+1), rotation=0)\n",
    "# Skip every even x-tick\n",
    "ax.set_xticks(np.arange(len(counts))[::2], np.arange(1, len(counts)+1)[::2], rotation=0)\n",
    "\n",
    "# Draw the bar that corresponds to the GS for each case in a more saturated color\n",
    "GS = P53_BOLERO['GS'].copy()\n",
    "GS = GS.loc[counts.index]\n",
    "for i in range(len(GS)):\n",
    "    GS_label = GS.iloc[i]\n",
    "    bar_widths = counts.iloc[i].values\n",
    "    bar_widths_cumsum = bar_widths.cumsum()\n",
    "    left = bar_widths_cumsum[GS_label] - bar_widths[GS_label]\n",
    "    ax.bar(i, bar_widths[GS_label], bottom=left, color=color[GS_label])\n",
    "\n",
    "ax.set_xlabel('slide')\n",
    "ax.set_ylabel('pathologist votes')\n",
    "# ax.set_title('Distributions of ratings for P53')\n",
    "ax.set_ylim(0, max(counts.sum(axis=1)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For the best models, print the predictions for each BOLERO case\n",
    "data_name = \"bolero\"\n",
    "model_names = [\"CLAM_ResNet18_tuned_64\",\n",
    "               \"CLAM_retccl_tuned\",\n",
    "               \"CLAM_retccl_tuned_hybrid\",\n",
    "    \"CLAM\", \"CLAM_db\", \"CLAM_m\", \"CLAM_db_m\", \"fb_spacing4\", \"fb_db_spacing4\"]\n",
    "model_name_mapping = {\n",
    "        \"CLAM\":\"CLAM\",\n",
    "        \"CLAM_db\":\"CLAMdb\",\n",
    "        \"CLAM_m\":\"CLAM+DC\",\n",
    "        \"CLAM_db_m\":\"CLAMdb+DC\",\n",
    "        \"fb_db_spacing4\":\"FBdb\",\n",
    "        \"fb_spacing4\":\"FB\",\n",
    "        \"CLAM_ResNet18_tuned_64\":\"CLAM64\",\n",
    "        \"CLAM_retccl_tuned\":\"CLAMresnet_t\",\n",
    "        \"CLAM_retccl_tuned_hybrid\":\"CLAMresnet_h\",\n",
    "    }\n",
    "case_to_pred = {}\n",
    "case_to_confidence = {}\n",
    "model_accuracy = {}\n",
    "y_true_in_order = [P53_BOLERO.loc[P53_BOLERO['Case ID'] == case_id, 'GS'].values[0] for case_id in case_nr_custom_ordering]\n",
    "y_true_in_order = [y-1 if y != 4 else 4 for y in y_true_in_order]\n",
    "for model_name in model_names:\n",
    "    # Pick checkpoint that had the highest accuracy\n",
    "    best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "    best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "    model_accuracy[model_name] = model_results[data_name][model_name][best_checkpoint_name][\"accuracy\"]\n",
    "\n",
    "    results = load_results(model_name, data_name)\n",
    "    slide_names = [int(slide_name) for slide_name in get_labels(data_name).keys()]\n",
    "    status_probs = get_status_probs_for_bolero(results, slide_names, best_checkpoint_name)\n",
    "\n",
    "    case_to_pred[model_name] = {}\n",
    "    case_to_confidence[model_name] = {}\n",
    "\n",
    "    for i, case in enumerate(P53_BOLERO.iterrows()):\n",
    "        case_id = case[1][\"Case ID\"]\n",
    "        status_prob = status_probs[i]\n",
    "        case_to_pred[model_name][case_id] = np.argmax(status_prob)\n",
    "        case_to_confidence[model_name][case_id] = np.max(status_prob)\n",
    "\n",
    "    # Sort case_to_pred by case_nr_custom_ordering\n",
    "    case_to_pred[model_name] = {case_id: case_to_pred[model_name][case_id] for case_id in case_nr_custom_ordering}\n",
    "    case_to_confidence[model_name] = {case_id: case_to_confidence[model_name][case_id] for case_id in case_nr_custom_ordering}\n",
    "\n",
    "color_mapping = {0:color_dict['g'], 1:color_dict['o'], 2:color_dict['b']}\n",
    "\n",
    "# Plot the predictions for each model in a grid of n_models x 55 using squares in the color from the color mapping\n",
    "ax = axs[0]\n",
    "y_space_factor = 1.2\n",
    "for i, model_name in enumerate(model_names):\n",
    "    i *= y_space_factor\n",
    "    for j, case_id in enumerate(case_nr_custom_ordering):\n",
    "        color = color_mapping[case_to_pred[model_name][case_id]]\n",
    "        ax.add_patch(plt.Rectangle((j+.15, i+.15), .7, .7, color=color, alpha=case_to_confidence[model_name][case_id]**2))\n",
    "        # Put a black x in the middle if the pred was wrong\n",
    "        if case_to_pred[model_name][case_id] != y_true_in_order[j]:\n",
    "            ax.plot([j+.15, j+.85], [i+.15, i+.85], color='#444444', linewidth=1)\n",
    "            ax.plot([j+.15, j+.85], [i+.85, i+.15], color='#444444', linewidth=1)\n",
    "# ax.set_xticks(np.arange(len(case_nr_custom_ordering))+0.5, np.arange(len(case_nr_custom_ordering))+1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks((np.arange(len(model_names)))*y_space_factor+0.5, [model_name_mapping[model_name] for model_name in model_names], rotation=45)\n",
    "# ax.set_ylabel('model')\n",
    "# ax.set_xlabel('case')\n",
    "# ax.set_title('Predictions for BOLERO cases', y=.9)\n",
    "# plt.suptitle('(n=55)', fontsize=10, y=.85, x=0.51, color='#444444')\n",
    "# Make x and y axis equal\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_xlim(0, len(case_nr_custom_ordering))\n",
    "\n",
    "# # Make another y-axis on the right\n",
    "# ax2 = ax.twinx()\n",
    "# ax2.set_aspect('equal')\n",
    "# ax2.set_yticks(np.arange(len(model_names))+0.5,\n",
    "#     # Each models accuracy on BOLERO\n",
    "#     [f\"{model_accuracy[model_name]:.2f}\" for model_name in model_names])\n",
    "# ax2.set_ylabel('accuracy (n=53)')\n",
    "# ax2.set_ylim(0, len(model_names))\n",
    "\n",
    "# Decrease the distance between the two subplots\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "# Remove the box around the subplot\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    axs[1].spines[direction].set_visible(False)\n",
    "    ax.spines[direction].set_visible(False)\n",
    "    # ax2.spines[direction].set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about case nr 24 (idx 23 in the custom ordering)\n",
    "custom_idx = 23\n",
    "case_id = case_nr_custom_ordering[custom_idx]\n",
    "# Get idx of the case in the DataFrame\n",
    "idx = P53_BOLERO['Case ID'].tolist().index(case_id)\n",
    "print(f\"Case ID: {case_id}\")\n",
    "print(f\"Idx: {idx}\")\n",
    "print(f\"GS: {P53_BOLERO.loc[P53_BOLERO['Case ID'] == case_id, 'GS'].values[0]}\")\n",
    "# Print the predictions for each model for each biopsy in the case\n",
    "for model_name in model_names:\n",
    "    print(f\"{model_name_mapping[model_name]}: {case_to_pred[model_name][case_id]} ({case_to_confidence[model_name][case_id]:.2f})\")\n",
    "    results = load_results(model_name, 'bolero')\n",
    "\n",
    "    for checkpoint_name in results[\"status_probs\"]['0_0']:\n",
    "        c = checkpoint_name\n",
    "        display([np.round(results[\"status_probs\"][i][c].tolist(), 2) for i in results[\"status_probs\"] if i.startswith(str(idx))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information about cases nr 29, 30, 31, 33, and 39 (idx 28, 29, 30, 32, and 38 in the custom ordering)\n",
    "custom_idxs = [28, 29, 30, 32, 38]\n",
    "case_ids = [case_nr_custom_ordering[custom_idx] for custom_idx in custom_idxs]\n",
    "# Get idx of the case in the DataFrame\n",
    "idxs = [P53_BOLERO['Case ID'].tolist().index(case_id) for case_id in case_ids]\n",
    "for case_id, idx in zip(case_ids, idxs):\n",
    "    print(f\"Case ID: {case_id}\")\n",
    "    print(f\"Idx: {idx}\")\n",
    "    print(f\"GS: {P53_BOLERO.loc[P53_BOLERO['Case ID'] == case_id, 'GS'].values[0]}\")\n",
    "    # Print the predictions for each model for each biopsy in the case\n",
    "    for model_name in model_names:\n",
    "        print(f\"{model_name_mapping[model_name]}: {case_to_pred[model_name][case_id]} ({case_to_confidence[model_name][case_id]:.2f})\")\n",
    "        results = load_results(model_name, 'bolero')\n",
    "\n",
    "        for checkpoint_name in results[\"status_probs\"]['0_0']:\n",
    "            c = checkpoint_name\n",
    "            display([np.round(results[\"status_probs\"][i][c].tolist(), 2) for i in results[\"status_probs\"] if i.startswith(str(idx))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all case ids from P53_BOLERO\n",
    "print(\"Case IDs for BOLERO:\")\n",
    "\n",
    "ORDERED = P53_BOLERO.copy()\n",
    "ORDERED['n_1s'] = ORDERED.apply(lambda row: (row.values == 1).sum(), axis=1)\n",
    "ORDERED['n_2s'] = ORDERED.apply(lambda row: (row.values == 2).sum(), axis=1)\n",
    "ORDERED['n_3s'] = ORDERED.apply(lambda row: (row.values == 3).sum(), axis=1)\n",
    "ORDERED['n_4s'] = ORDERED.apply(lambda row: (row.values == 4).sum(), axis=1)\n",
    "ORDERED.sort_values('n_4s', ascending=False, inplace=True)\n",
    "ORDERED.sort_values('n_1s', ascending=False, inplace=True)\n",
    "\n",
    "WT_CASES = ORDERED[ORDERED['GS'] == 1].copy()\n",
    "WT_CASES.sort_values('n_1s', ascending=False, inplace=True)\n",
    "OE_CASES = ORDERED[ORDERED['GS'] == 2].copy()\n",
    "OE_CASES.sort_values('n_2s', ascending=True, inplace=True)\n",
    "NM_CASES = ORDERED[ORDERED['GS'] == 3].copy()\n",
    "NM_CASES.sort_values('n_3s', ascending=True, inplace=True)\n",
    "EQ_CASES = ORDERED[ORDERED['GS'] == 4].copy()\n",
    "print(EQ_CASES[\"Case ID\"])\n",
    "\n",
    "# Now concatenate these\n",
    "ORDERED = pd.concat([WT_CASES, EQ_CASES, NM_CASES, OE_CASES])\n",
    "\n",
    "case_ids = ORDERED['Case ID'].values\n",
    "print(case_ids.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confidence (but this information is already kind of in the ROC curves?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for each model what the confidence was for cases they got correct, and cases they got wrong\n",
    "# Plot this in a horizontal barchart\n",
    "plt.figure(figsize=(6,4))\n",
    "bar_width = 0.4\n",
    "for i, model_name in enumerate(model_names):\n",
    "    correct_confidence = []\n",
    "    false_confidence = []\n",
    "    for case_id in case_nr_custom_ordering:\n",
    "        if case_to_pred[model_name][case_id] == y_true_in_order[case_nr_custom_ordering.index(case_id)]:\n",
    "            correct_confidence.append(case_to_confidence[model_name][case_id])\n",
    "        else:\n",
    "            false_confidence.append(case_to_confidence[model_name][case_id])\n",
    "    # print(f\"{model_name}:\")\n",
    "    # print(f\"Correct confidence: {np.mean(correct_confidence):.2f}  {np.std(correct_confidence):.2f}\")\n",
    "    # print(f\"False confidence: {np.mean(false_confidence):.2f}  {np.std(false_confidence):.2f}\")\n",
    "\n",
    "#     plt.barh(i, np.mean(correct_confidence), xerr=np.std(correct_confidence), color=color_dict['g'], alpha=1, height=bar_width)\n",
    "#     plt.text(0.1, i, f\"{np.mean(correct_confidence):.2f}  {np.std(correct_confidence):.2f}\", ha='left', va='center', color='black')\n",
    "#     plt.barh(i+0.4, np.mean(false_confidence), xerr=np.std(false_confidence), color=color_dict['r'], alpha=1, height=bar_width)\n",
    "#     plt.text(0.05, i+0.4, f\"{np.mean(false_confidence):.2f}  {np.std(false_confidence):.2f}\", ha='left', va='center', color='black')\n",
    "# plt.xlabel('confidence')\n",
    "# plt.ylabel('model')\n",
    "# plt.title('Confidence for correct and false predictions')\n",
    "# plt.yticks(np.arange(len(model_names))+0.2, [model_name_mapping[model_name] for model_name in model_names])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comparison to BOLERO pathologists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raters = P53_BOLERO.columns[1:-1]\n",
    "\n",
    "# For every rater, calculate the agreement with the consensus (without the rater)\n",
    "agreement = {}\n",
    "for rater in raters:\n",
    "    without_equiv = P53_BOLERO[P53_BOLERO['GS'] != 4]\n",
    "    # Skip Case ID RB0021\n",
    "    without_equiv = without_equiv[without_equiv['Case ID'] != 'RB0021']\n",
    "    agreement[rater] = without_equiv.apply(lambda x: x[rater] == x['GS'], axis=1).sum() / len(without_equiv)\n",
    "\n",
    "agreement = pd.Series(agreement)\n",
    "pathologist_accuracy = agreement.mean()\n",
    "reference_pathologists = [\"BOLERO - SM\", \"BOLERO - KS\", \"BOLERO - MJ\", \"BOLERO - JO\"]\n",
    "# Nudge reference pathologist accuracy up by epsilon to sort them to the front if equal\n",
    "agreement[reference_pathologists] = agreement[reference_pathologists] + 1e-7\n",
    "\n",
    "# Insert AI models\n",
    "for model_name in model_names:\n",
    "    acc = model_accuracy[model_name]\n",
    "    agreement[model_name] = acc + 1e-6\n",
    "\n",
    "agreement = agreement.sort_values(ascending=False)\n",
    "\n",
    "# Plot bar chart of agreement\n",
    "plt.figure(figsize=(12,4))\n",
    "ax = agreement.plot(kind='bar', width=1, \n",
    "    # color=color_dict[\"b_p\"]\n",
    "    # Make bars white with black border\n",
    "    facecolor='white', edgecolor='#444444',\n",
    ")\n",
    "\n",
    "# PATHOLOGIST LINES\n",
    "# Put horizontal black dashed line at median/mean of pathologists\n",
    "ax.axhline(pathologist_accuracy, color='#444444', linestyle='dashed', linewidth=1)\n",
    "# Put text on top of line\n",
    "ax.text(len(agreement)-1, pathologist_accuracy+0.01, f\"55 pathologists mean: {pathologist_accuracy*100:.1f}%\", ha='right', va='bottom')\n",
    "# Also put dashed line at mean of reference pathologists\n",
    "ref_pat_accuracy = agreement[reference_pathologists].mean()\n",
    "ax.axhline(ref_pat_accuracy, color='#444444', linestyle='dashed', linewidth=1)\n",
    "ax.text(len(agreement)-1, ref_pat_accuracy+0.01, f\"4 reference pathologists mean: {ref_pat_accuracy*100:.1f}%\", ha='right', va='bottom')\n",
    "\n",
    "# Set y labels to be *100\n",
    "xticks = [0, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 1]\n",
    "plt.yticks(xticks, [f\"{x*100:.0f}\" for x in xticks])\n",
    "plt.xticks([])\n",
    "plt.ylabel('accuracy %', labelpad=-20)\n",
    "plt.xlabel('rater')\n",
    "# plt.title('Comparison of AI and Pathologists')\n",
    "# plt.suptitle('on BOLERO (n=53)', fontsize=10, y=0.88, x=0.51, color='#444444')\n",
    "# Show text on the bars\n",
    "for i in range(len(agreement)):\n",
    "    # Put square behind the text for contrast\n",
    "    patch_color = \"white\"\n",
    "    if agreement.index[i] in reference_pathologists:\n",
    "        patch_color = \"#aaaaaa\"\n",
    "    elif agreement.index[i] in model_names:\n",
    "        patch_color = model_colors[model_names[model_names.index(agreement.index[i])]]\n",
    "    patch_w = 0.75\n",
    "    patch_h = 1 / 20\n",
    "    # patch_color = \"black\"\n",
    "    ax.add_patch(plt.Rectangle((i-patch_w/2, agreement.iloc[i]-0.05-patch_h/2), patch_w, patch_h, color=patch_color, zorder=3))\n",
    "    ax.text(i+0.1, agreement.iloc[i]-0.02, round(agreement.iloc[i]*100), ha='center', va='top', rotation=90)\n",
    "    # plt.text(i, 0.01, round(agreement.iloc[i]*100), ha='center', va='bottom')\n",
    "# Make reference pathologist bars different color\n",
    "for ref_pat in reference_pathologists:\n",
    "    ref_pat_x = agreement.index.get_loc(ref_pat)\n",
    "    plt.gca().get_children()[ref_pat_x].set_facecolor(\"#aaaaaa\")\n",
    "\n",
    "# Make AI bars their own color\n",
    "for model_name in model_names:\n",
    "    model_x = agreement.index.get_loc(model_name)\n",
    "    plt.gca().get_children()[model_x].set_facecolor(model_colors[model_name])\n",
    "    # Put rotated 90 text on the bar with name\n",
    "    plt.text(model_x+0.1, 0.05, model_name_mapping[model_name], ha='center', va='bottom', rotation=90)\n",
    "\n",
    "plt.xlim(-.6, len(agreement)-.4)\n",
    "plt.ylim(-0.01, 1)\n",
    "\n",
    "plt.legend(handles=[\n",
    "        plt.Rectangle((0,0),1,1, facecolor=\"white\", edgecolor=\"#444444\", label='pathologist'),\n",
    "        plt.Rectangle((0,0),1,1, facecolor=\"#aaaaaa\", label='reference pathologist')] +\\\n",
    "        [plt.Rectangle((0,0),1,1, facecolor=model_colors[model_name], label=model_name_mapping[model_name]) for model_name in reversed(model_names)],\n",
    "    loc='lower left', bbox_to_anchor=(0.65, 0)\n",
    ")\n",
    "\n",
    "# Remove box around plot\n",
    "for direction in ['top', 'right', 'bottom', 'left']:\n",
    "    plt.gca().spines[direction].set_visible(False)\n",
    "\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"..\", \"images\", \"ai-vs-pathologist.png\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print name of the worst pathologist\n",
    "worst_pathologist = agreement.idxmin()\n",
    "print(f\"Worst pathologist: {worst_pathologist} with {agreement[worst_pathologist]*100:.2f}% agreement\")\n",
    "\n",
    "# Print their predictions in the case_nr_custom_ordering\n",
    "print(f\"{worst_pathologist} predictions:\")\n",
    "worst_pathologist_preds = P53_BOLERO.loc[:, worst_pathologist].values\n",
    "for i, case_id in enumerate(case_nr_custom_ordering):\n",
    "    print(f\"{case_id}: {worst_pathologist_preds[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curves + comparison to BOLERO pathologists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each pathologist calculate the specificity and sensitivity\n",
    "sensitivity = {}\n",
    "specificity = {}\n",
    "for rater in raters:\n",
    "    TP = without_equiv[(without_equiv['GS'] > 1) & (without_equiv[rater] > 1)].shape[0]\n",
    "    TN = without_equiv[(without_equiv['GS'] == 1) & (without_equiv[rater] == 1)].shape[0]\n",
    "    FP = without_equiv[(without_equiv['GS'] == 1) & (without_equiv[rater] > 1)].shape[0]\n",
    "    FN = without_equiv[(without_equiv['GS'] > 1) & (without_equiv[rater] == 1)].shape[0]\n",
    "    sensitivity[rater] = TP / (TP + FN)\n",
    "    specificity[rater] = TN / (TN + FP)\n",
    "\n",
    "# Create a row of three subplots, each displaying a model pair of (CLAM, CLAM_db), (CLAM_m, CLAM_db_m), (fb_spacing4, fb_db_spacing4)\n",
    "# Each subplot will have two ROC curves, one for each of the two models in the pair\n",
    "data_name = \"bolero\"\n",
    "model_pairs = [(\"CLAM\", \"CLAM_db\"), (\"CLAM_m\", \"CLAM_db_m\"), (\"fb_spacing4\", \"fb_db_spacing4\")]\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 6))\n",
    "for i, (model_name1, model_name2) in enumerate(model_pairs):\n",
    "    ax = axs[i]\n",
    "    closest_point_to_0_1 = (1,0)\n",
    "    closest_model = None\n",
    "    outperformed = 0\n",
    "    outperformed_by = 0\n",
    "    legend_handles = [\n",
    "        plt.Line2D([0], [0], color='#444444', marker='o', markersize=5, label='pathologist', linewidth=0),\n",
    "        plt.Line2D([0], [0], color=color_dict['b'], marker='o', markersize=7, markerfacecolor='none', label='reference pathologist', linewidth=0)]\n",
    "    for model_name in [model_name1, model_name2]:\n",
    "        color = model_colors[model_name]\n",
    "\n",
    "        best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "        best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "        fpr = model_results[data_name][model_name][best_checkpoint_name][\"fpr_m\"]\n",
    "        tpr = model_results[data_name][model_name][best_checkpoint_name][\"tpr_m\"]\n",
    "        AUC = model_results[data_name][model_name][best_checkpoint_name][\"auc_m\"]\n",
    "\n",
    "        ax.plot(fpr, tpr, \n",
    "                label=f\"(AUC = {AUC:.2f}) {model_name_mapping[model_name]}\",\n",
    "            color=color, alpha=1, linewidth=2)\n",
    "        # Find the point closest to (0,1) for each model\n",
    "        closest_point = min(zip(fpr, tpr), key=dist_to_0_1)\n",
    "        if dist_to_0_1(closest_point) < dist_to_0_1(closest_point_to_0_1):\n",
    "            closest_point_to_0_1 = closest_point\n",
    "            closest_model = model_name\n",
    "\n",
    "        legend_handles.append(plt.Line2D([0], [0], color=color, label=f\"(AUC = {AUC:.2f}) {model_name_mapping[model_name]}\", linewidth=3))\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlabel('1 - specificity')\n",
    "    axs[0].set_ylabel('sensitivity')\n",
    "    # ax.set_title(f'ROC Curves for WT vs Mutated on {data_name}')\n",
    "    ax.legend(loc='lower right')\n",
    "    # Make x and y axis equal\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    # Put a label on the point closest to (0,1)\n",
    "    xy_text = (5,-40) if i in [0,1] else (5,-20)\n",
    "    ax.annotate(f\"(1 - {1-closest_point_to_0_1[0]:.2f}, {closest_point_to_0_1[1]:.2f})\", closest_point_to_0_1, \n",
    "        textcoords=\"offset points\", xytext=xy_text, ha='left', color='black', fontsize=10, arrowprops=dict(arrowstyle='->', color='black'), rotation=0)\n",
    "\n",
    "    # Put dot for each rater\n",
    "    # Seed random number generator for reproducibility\n",
    "    np.random.seed(42)\n",
    "    for rater in raters:\n",
    "        sens = sensitivity[rater]\n",
    "        spec = specificity[rater]\n",
    "        x, y = 1-spec, sens\n",
    "        # # Add small perturbation to make sure the dots don't overlap\n",
    "        # perturbation = 0.005\n",
    "        # x = 1-spec + np.random.uniform(-perturbation, perturbation)\n",
    "        # y = sens + np.random.uniform(-perturbation, perturbation)\n",
    "        # white dot with #444444 edge\n",
    "        ax.scatter(x, y, color='#444444', edgecolor='#444444', s=5)\n",
    "    for ref_rater in reference_pathologists:\n",
    "        sens = sensitivity[ref_rater]\n",
    "        spec = specificity[ref_rater]\n",
    "        # grey dot with #444444 edge\n",
    "        ax.scatter(1-spec, sens, edgecolor=color_dict[\"b\"], s=30, facecolor='none')\n",
    "\n",
    "    plt.legend(handles=legend_handles, loc='lower right')\n",
    "\n",
    "    ax.set_xlim(-0.005, 1)\n",
    "    ax.set_ylim(0, 1.015)\n",
    "\n",
    "    # Remove top and right spines\n",
    "    for direction in ['top', 'right']:\n",
    "        ax.spines[direction].set_visible(False)\n",
    "\n",
    "    # Calculate how many pathologists are strictly outperformed by the model based on the ROC curve\n",
    "    for model_name in [model_name1, model_name2]:\n",
    "        best_checkpoint_idx = np.argmax([model_results[\"test\"][model_name][checkpoint_name][\"accuracy\"] for checkpoint_name in model_results[data_name][model_name]])\n",
    "        best_checkpoint_name = list(model_results[data_name][model_name].keys())[best_checkpoint_idx]\n",
    "        fpr = model_results[data_name][model_name][best_checkpoint_name][\"fpr_m\"]\n",
    "        tpr = model_results[data_name][model_name][best_checkpoint_name][\"tpr_m\"]\n",
    "\n",
    "        outperformed = 0\n",
    "        outperformed_by = 0\n",
    "        for rater in raters:\n",
    "            sens = sensitivity[rater]\n",
    "            spec = specificity[rater]\n",
    "            fp = 1-spec\n",
    "            # If there is a point on the ROC curve that is strictly above the point for the rater, the model outperforms the rater\n",
    "            if any([(tpr[i] >= sens and fpr[i] < fp) or (tpr[i] > sens and fpr[i] <= fp) for i in range(len(fpr))]):\n",
    "                outperformed += 1\n",
    "            # If every point on the ROC curve is strictly below the point for the rater, the model is outperformed by the rater\n",
    "            elif all([(tpr[i] <= sens and fpr[i] > fp) or (tpr[i] < sens and fpr[i] >= fp) for i in range(len(fpr))]):\n",
    "                outperformed_by += 1\n",
    "                # This is kinda silly because only a perfect rater would strictly outperform ANY ROC curve\n",
    "\n",
    "        print(f\"{model_name_mapping[model_name]} outperforms {outperformed} raters\")\n",
    "\n",
    "# plt.suptitle(\"ROC Curves (Any Mutation)\", y=.95, x=.51)\n",
    "# axs[1].set_title(f'on BOLERO data (n={len(without_equiv)})', fontsize=10, color='#444444', y=1.03)\n",
    "\n",
    "# Move the subplots closer together\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacing comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot horizontal barchart of accuracy, of only the fb models, sorted by spacing\n",
    "data_name = \"test+pathxl-100\"\n",
    "data_len = len(get_labels(data_name))\n",
    "kwargs = {\n",
    "    \"accuracy_m\": {\"title\": \"Accuracy by Spacing\", \"subtitle\": f\"on test 1+2 data (n={data_len})\", \"xlabel\": \"accuracy\"},\n",
    "    \"auc\": {\"title\": \"AUC by Spacing\", \"subtitle\": f\"on test 1+2 data (n={data_len})\", \"xlabel\": \"mean( AUC per class )\"},\n",
    "    }\n",
    "one_color = color_dict[\"p\"]\n",
    "std_color = '#666666'\n",
    "for metric_name in kwargs.keys():\n",
    "    model_names = [model_name for model_name in model_visualizations[data_name] if \"fb\" in model_name]\n",
    "    # First sort by whether it's db or not, then by spacing\n",
    "    model_names = sorted(model_names, key=lambda x: \"db\" in x)\n",
    "    model_names = sorted(model_names, key=lambda x: model_kwargs[x][\"spacing\"], reverse=True)\n",
    "    metrics     = [model_visualizations[data_name][model_name][f\"mean_{metric_name}\"] for model_name in model_names]\n",
    "    std_metrics = [model_visualizations[data_name][model_name][f\"std_{metric_name}\"] for model_name in model_names]\n",
    "\n",
    "    model_names.insert(-1, \"fb_spacing2\")\n",
    "    metrics.insert(-1, 0)\n",
    "    std_metrics.insert(-1, 0)\n",
    "\n",
    "    # Each spacing has a non-db and a db model, group these together in pairs of two, with the same color\n",
    "    plt.figure(\n",
    "        figsize=(6,6)\n",
    "        )\n",
    "    bar_width = 0.8\n",
    "    for i, (model_name, metric, std_metric) in enumerate(zip(model_names, metrics, std_metrics)):\n",
    "        if i % 2 == 0:\n",
    "            # color = colors[i//2]\n",
    "            color = one_color\n",
    "        alpha = 0.7\n",
    "        y = i - 0.1\n",
    "        if \"db\" not in model_name:\n",
    "            alpha = 0.3\n",
    "            y += 0.2\n",
    "        plt.barh(y, metric, xerr=std_metric, color=color, alpha=alpha, label=model_name, height=bar_width,\n",
    "            error_kw=dict(ecolor=std_color))\n",
    "        # Put the number in the center of the bar\n",
    "        if metric > 0:\n",
    "            # plt.text(metric/2, y, f\"{metric:.2f}  {std_metric:.2f}\", \n",
    "            #          ha='center', va='center', color=\"black\")\n",
    "            plt.text(0.01, y, f\"{metric:.2f}  {std_metric:.2f}\", \n",
    "                     ha='left', va='center', color=\"black\")\n",
    "    plt.yticks(np.arange(len(model_names)//2)*2+0.5, [model_name.split('spacing')[-1]+\"mpp\" for model_name in model_names][::2], rotation=45)\n",
    "    plt.xlabel(kwargs[metric_name][\"xlabel\"])\n",
    "    # plt.suptitle(kwargs[metric_name][\"title\"], fontsize=12, y=.96, x=.51)\n",
    "    # plt.title(kwargs[metric_name][\"subtitle\"], fontsize=10, color='#444444')\n",
    "    # plt.ylabel(\"spacing\")\n",
    "    # Put custom legend that only says db has brighter colors (with color 0 as example) and the black stripes are std\n",
    "    plt.legend(handles=[\n",
    "        plt.Rectangle((0,0),1,1, color=one_color, alpha=0.7, label=\"db\"),\n",
    "        plt.Rectangle((0,0),1,1, color=one_color, alpha=0.3, label=\"not db\"),\n",
    "        # Black line for std\n",
    "        plt.Line2D([0], [0], color='#666666', lw=2, label='std')\n",
    "    ], loc='lower right')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
